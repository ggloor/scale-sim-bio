---
title: "Explicit Scale Simulation for analysis of RNA-sequencing  with ALDEx2"
shorttitle: "Scale ALDEx2"
author:
  - name: Greg Gloor
    affiliation: Department of Biochemistry, University of Western Ontario
    email: ggloor@uwo.ca
  - name: Michelle Pistner Nixon
    affiliation: Department of Population Health Sciences, Geisinger, Danville, PA 
  - name: Justin Silverman
    affiliation: 
      College of Information Sciences and Technology and Department of Medicine, Pennsylvania   State University
      Department of Medicine, Pennsylvania State University
      Institute for Computational and Data Science, Pennsylvania State University
      Department of Statistics, Pennsylvania State University
cite-method: citeproc
citeproc: TRUE
bibliography: /Users/ggloor/Library/texmf/bibtex/bib/bibdesk_refs.bib
csl: /Users/ggloor/Documents/0_git/csl_styles/nucleic-acids-research.csl 
output:
    pdf_document:
      keep_tex: true
header-includes:
  - \usepackage{setspace}
package: ALDEx2 

abstract: 
  In high-throughput sequencing (HTS) studies,  sample-to-sample variation in sequencing depth is driven by technical factors, and not by variation in the scale (size) of the biological system. Typically a statistical normalization removes unwanted technical variation in the data or the parameters of the model to enable differential abundance analyses. We recently showed that all normalizations make implicit assumptions about the unmeasured system scale and that errors in these assumptions can dramatically increase false positive and false negative rates. We demonstrated that these errors can be mitigated by accounting for uncertainty using a *scale model*, which we integrated into the ALDEx2 R package. This article provides new insights focusing on the application to transcriptomic analysis. We provide transcriptomic case studies demonstrating how scale models, rather than traditional normalizations, can reduce false positive and false negative rates in practice while enhancing the transparency and reproducibility of analyses. These scale models replace the need for dual cutoff approaches often used to address the disconnect between practical and statistical significance. We demonstrate the utility of scale models built based on known housekeeping genes in complex metatranscriptomic datasets. Thus this work provides guidance on how to incorporate scale into transcriptomic data sets.

---

# Introduction

\doublespacing 
\singlespacing 

High-throughput sequencing (HTS) is a ubiquitous
tool used to explore many biological phenomenon such as gene expression
(single-cell sequencing, RNA-sequencing, meta-transcriptomics), microbial
community composition (16S rRNA gene sequencing, shotgun metagenomics) and
differential enzyme activity (selex, CRISPR killing). HTS proceeds by taking a
sample from the environment, making a library, multiplexing (merging) multiple
libraries together, and then applying a sample of the multiplexed library to a
flow cell. Each of these steps is a compositional sampling step as only a
fixed-size subsample of nucleic acid is carried over to subsequent steps. Thus,
with each sampling step the connection between the actual size of the sampled
DNA pool and the scale (e.g., size, microbial load, or total gene expression) of
the measured biological system is degraded or lost. In the end, the information
contained in the data relates only to relative abundances and  has an arbitrary
scale imposed by the sequencing
process [@lovell:2011;@Quinn:2019aa;@nixon2024scale]. Researchers
can use modified experimental protocols [@Jiang:2011aa;@Loven:2012aa;@Vandeputte:2017aa;@Props:2017aa] or machine learning methods [@Nishijima:2024aa] to uncover the
biological variation in scale. However,  wet-lab protocols only provide information on
the size of the data downstream of the step in the sample preparation protocol where the
intervention was made and  introduce an additional source of variation that must be accounted for [@Loven:2012aa]. The computational methods developed for microbiome analysis have low correlation with the actual scale but are useful [@Nishijima:2024aa]. In short, the disconnect between sample-to-sample variation in sequencing depth and biological variation in scale remains an open
challenge.

The analysis of  HTS data suffers from several known problems that can be
traced, in whole or in part, to  misspecification of scale. The first issue is
poor control of the false discovery rate (FDR) [@Thorsen:2016aa;@Quinn:2018aa;@Nearing:2022aa;@Ge:2021aa;@Li:2022aa], exhibited
as dataset-dependent FDR control and by the disconnect between statistical and
biological significance [@Schurch:2016aa]. In current practice,  these issues are
addressed  by a dual-filtering method, whereby both a low p-value (or
equivalently a low q-value following FDR correction [@storey2003positive]) and a
large difference between groups is used to identify interesting transcripts or
genes for follow-up analysis [@Cui:2003aa;@Schurch:2016aa]. This double-filtering approach is
graphically exemplified by the volcano plot [@Cui:2003aa], but is known to not
appropriately control the  FDR [@Zhang:2009aa;@Ebrahimpoor:2021aa]. The second
issue is poor performance when analyzing data where the mean change between
groups is non-zero [@nixon2024scale]. Such asymmetric data can arise when a gene set is expressed
in one group but not the other, or when one group contains different gene
content from the other. This type of data frequently arises in in-vitro
selection experiments (SELEX), transcriptome analysis, and microbiome analysis
[@Wu2021]. The third issue is that the actual scale of the environment is often
a major confounding variable during analysis [@nixon2024scale;@Nishijima:2024aa]. The final issue is that these problems become more pronounced as more
samples are collected; that is, more information results in a worsening of the
accuracy of the analysis [@Li:2022aa;@nixon2024scale;@Nixon2024B].

The  four problems were recently shown by Nixon et al. [@nixon2024scale]
to be a result of a mismatch between the underlying size or scale of the system
and the assumptions of the normalizations used for the analysis of HTS.
Biological variation in scale often represents an important unmeasured
confounder in HTS analyses [@Lovell:2015]. For example, cells transformed by the
cMyc oncogene have about 3 times the amount of mRNA and about twice the rRNA
content than  non-transformed cells [@Nie:2012aa], and this dramatically skews
transcriptome analysis unless spike-in approaches are used [@Loven:2012aa]. In addition, wild-type and mutant
strains of cell lines, yeast or bacteria  have different growth rates and RNA
contents under different conditions, which  affect our ability to identify truly
differentially abundant genes [@Scott:2010;@Yoshikawa:2011aa;@Lin:2018aa]. As
another example, the total bacterial load of the vaginal microbiome differs by
1-2 orders of magnitude in absolute abundance between the healthy and bacterial
vaginosis states [@Zozaya:2010], and the composition between these states is
dramatically different [@Ravel:2010;@Hummelen:2010]. Thus, a full description of
any of these systems includes both relative change (composition) and absolute
abundance (scale).  Current methods access only the compositional information
yet make implicit assumptions about the scale [@nixon2024scale,@Nixon2024B].

Recently, Nixon et al. [-@nixon2024scale] showed that the challenge of
non-biological variation in sequencing depth be viewed as a problem of
partially-identified models. They showed that \emph{all} normalizations make
some assumption about  scale but these implicit assumptions are often
inappropriate and difficult to interpret. This causes different normalizations
to provide different outputs when applied to the same dataset
[@Bullard:2010;@Dillies:2013;@Thorsen:2016aa;@Weiss:2017aa;@Nearing:2022aa].
Intuitively, normalizations in widespread use  assume that either all samples
have the same scale, e.g. proportions, rarefaction [@Hughes:2005tu], RPKM
[@Mortazavi:2008; @wagner:tpm], etc; or that a subset of features in one sample
can be chosen as a reference to which the others are scaled e.g. the TMM
[@Robinson:2010a], or LVHA [@Wu2021] or the additive log-ratio [@aitchison1982];
or that different sub-parts of each sample maintain a constant scale across
samples e.g. the RLE [@Anders:2010];  or that the geometric mean of the parts is
appropriate e.g the CLR [@Aitchison:1986] and its derivatives.

The original naive ALDEx2 [@fernandes:2013] model unwittingly made a strict
assumption about scale through the CLR normalization [@nixon2024scale]. This assumption was often close enough to the true value to be useful, but was not always the a good estimate and could be outperformed by other normalizations [@Yerke:2024aa].
Nixon et al. [@nixon2024scale] showed that better scale assumptions resulted in
more reproducible data analysis including better control of both false positive
and false negative results. We recently modified ALDEx2  to explicitly model the
scale over a range of reasonable normalization parameters, and showed
significant improvements in performance in microbiome and in-vitro selection
experiments [@Nixon2024B]. Here, we briefly  review these modifications and show how scale uncertainty can greatly improve modeling in transcriptome
and meta-transcriptome datasets to provide more robust and reproducible results.

# Implementation

Formal and expanded descriptions of the concepts that follow are given in
[@nixon2024scale;@Nixon2024B].   To be  concrete, we let \(\mathbf{Y}\) denote
the \emph{measured} \(D \times N\) matrix of sequence counts with elements
\(\mathbf{Y}_{dn}\) indicating the number of measured DNA molecules mapping to
feature \(d\) (e.g., a taxon, transcript or gene) in sample \(n\).  Likewise, we
can denote \(\mathbf{W}\) as the \emph{true}  amount of class \(d\) in the
biological system from which sample \(n\) was obtained. We can think of
\(\mathbf{W}\) as consisting of two parts, the scale \(\mathbf{W}^{\perp}\)
(e.g., totals) and the composition \(\mathbf{W}^{\parallel}\) (i.e.,
proportions). That is, \(\mathbf{W}^{\perp}\) is a \(N\)-vector with elements
\(\mathbf{W}^{\perp}_{n}=\sum_{d}\mathbf{W}_{dn}\) while
\(\mathbf{W}^{\parallel}\) is a \(D \times N\) matrix with elements
\(\mathbf{W}^{\parallel}_{dn}=\mathbf{W}_{dn}/\mathbf{W}^{\perp}_{n}\). Note
that with these definitions \(\mathbf{W}\) can be written as the element-wise
combination of scale and composition:
\(\mathbf{W}_{dn}=\mathbf{W}^{\parallel}_{dn}\mathbf{W}^{\perp}_{n}\), or as the
logarithm \(\log \mathbf{W}_{dn}= \log \mathbf{W}^{\parallel}_{dn} + \log
\mathbf{W}^{\perp}_{n}\).

Many of  the normalizations in widespread use in tools such as DESeq2
[@Love:2014aa], edgeR [@Robinson:2010a], metagenomeSeq [@Paulson:2013aa] ALDEx2
[@fernandes:2014]  can be stated as ratios of the form \(\hat{{\mathbf{W}}}_{dn}
\approx \mathbf{Y}_{dn}/f(\mathbf{Y})\), where the denominator is determined by
some function of the observation. We use the \(\hat{{\mathbf{W}}}\) (\(\ \hat{}\
\)) notation to indicate that the output is an estimate of the true value.  The
technical variation in sequencing depth
(\(\mathbf{Y}^{\perp}_{n}=\sum_{d}\mathbf{Y}_{dn}\)) implies that observed data
\(\mathbf{Y}\) provides us with information about the system composition
\(\mathbf{W}^{\parallel}\) but little to no information in the system scale
\(\mathbf{W}^{\perp}\) (Lovell et al. 2011).

## Adding Scale Uncertainty in ALDEx2

The ALDEx2 R package [@fernandes:2013;@fernandes:2014] is a general purpose
toolbox to  model the uncertainty of HTS data and to use that model to estimate
the underlying LFC (log-fold change) significance.  At a high-level, ALDEx2  has
three connected components to estimate the uncertainty inherent in HTS datasets.
First, the tool accounts for the uncertainty of the sequencing counts using
Dirichlet multinomial sampling to build a probabilistic model of the data; i.e.,
\(\mathbf{\hat{W}}^{\parallel} \approx \mathrm{Dir}(\mathbf{Y})\).  Secondly,
ALDEx2 uses the centred log-ratio transformation to scale the data
[@fernandes:2013]. However, this step was modified recently to account for scale
uncertainty and misspecification [@nixon2024scale,@Nixon2024B] via a scale
model, explained  with more details in [@nixon2024scale;@Nixon2024B] and
summarized in the next paragraph. Finally, a standard null-hypothesis test and a
non-parametric estimate of mean standardized difference are used to report on
the finite sample variation. These sources of uncertainty and variation are
combined via reporting the expected values from a Monte-Carlo simulation
framework. For simplicity, we use the term 'difference' to refer to the absolute
difference between groups, and 'dispersion' to refer to the  within-condition
difference or pooled variance as defined in [@fernandes:2013]. These are
calculated on a \(\log_2\) scale. For more details on ALDEx2 see
[@fernandes:2013;@fernandes:2014;@nixon2024scale;@Nixon2024B].

Scale models can be incorporated into ALDEx2, turning the ALDEx2 model into a
specialized type of statistical model which Nixon et al. [@nixon2024scale] term
a \textit{Scale Simulation Random   Variable} (SSRV). To do this, Nixon et al.
[@nixon2024scale] generalized the concept of normalizations by introducing  the
concept of a \textit{scale model} to account for potential error in the centred
log-ratio normalization step.  They did this by including a model for
\(\mathbf{\hat{W}}^{\perp}_{n}\). The CLR normalization used by ALDEx2 makes the
assumption \(\mathbf{\hat{W}}^{\perp}_{n}=1/G_{n}\), where \(G_n\) is the
geometric mean of sample n, which while being a random variable, is essentially
constant across each Monte-Carlo replicate, but that differs between samples.
With this modification, ALDEx2 can be generalized by considering probability
models for the scale \(\mathbf{\hat{W}}^{\perp}_{n}\) that have mean
\(1/G_{n}\). For example, the following scale model generalizes the CLR:

\[\log \mathbf{\hat{W}}^{\perp}_{n} = -\log G_{n} + \Lambda x_{n} \qquad \Lambda
\sim N(\mu, \gamma^{2})\].

This formulation is quite flexible [@nixon2024scale;@Nixon2024B]. In the simple
or 'default' configuration,   \(\mu = 0\) and  \(\gamma\) is a tunable parameter
drawn from a log-Normal distribution[@nixon2024scale]. Adding scale uncertainty with the \(\gamma\) paramenter  controls only the degree of uncertainty of the CLR assumption for the
\(x_{n}\) binary condition indicator (e.g., \(x_{n}=1\) denotes case and
\(x_{n}=0\) denotes control). In the advanced or 'informed' configuration,
\(\mu\) takes different values for each group and  controls the location of
the LFC assumption; combining \(\mu\) with a \(\gamma\) estimate allows for uncertainty in both the location and the scale. An example of both the default and informed approaches is
given for a microbiome dataset in [@Nixon2024B] showing increased sensitivity and specificity. Here we show that these
approaches also work well in transcriptome and metatranscriptome datasets. These
modifications are  instantiated in ALDEx2 which is  the first software package
designed for SSRV-based inference.

# Results

## Adding scale uncertainty replaces the need for dual significance cutoffs.

GierliÅ„ski et al. [@Gierlinski:2015aa]  generated a  highly replicated yeast
transcriptome dataset to compare gene expression between a wild-type strain and
a snf2 gene knockout, \(\Delta\)snf2. This dataset was  used to test several
RNA-seq tools for their power  to detect the set of differentially abundant
transcripts identified in the full dataset when the data was subset
[@Schurch:2016aa]. In this  study each tool had its own 'gold standard' set of
transcripts with different tools identifying between between 65% to >80% of all
transcripts as being significantly different. Since the majority of transcripts
were significantly different, the authors suggested that it was more appropriate
to apply a  dual cutoff composed of both a Benjamini-Hochberg [@benjamini:1995]
corrected p-value (q-value) plus a difference cutoff  to limit the number of
identified transcripts to a much smaller fraction of the total.  Nixon et al,
@Nixon2024B showed that adding even a small amount of  scale uncertainty with
ALDEx2  dramatically reduced the number of significant transcripts identified,
removing the need for the dual cutoff approach in this dataset and others. Below
we include an intuitive explanation of why and how incorporating scale uncertainty
achieved this outcome using a setting of \(\gamma=0.5\). The approach is in line with
the recommendations of [@Nixon2024B] and  gives results comparable to those
proposed in [@Schurch:2016aa].


```{r yst-res, echo=F, warning=F, message=F,comment=F,}
#ALDEx2
library(ALDEx2) 
load(file="analysis/yst.all.Rda")
load(file="analysis/yst.s.all.Rda") 
load(file="analysis/yst.1.all.Rda")
# DESeq2
load('analysis/res.Rda')


# with gamma = 0.5
load('analysis/x.s.all.Rda') 
sig.des <- which(res@listData$padj < 0.05) 
sig.ald <- which(yst.all$we.eBH < 0.05) 
sig.s.ald <- which(yst.s.all$we.eBH < 0.05)

sig.all <- yst.all$we.eBH < 0.05 
sig.t <- yst.all$we.eBH < 0.05 &
abs(yst.all$diff.btw) > 1.4 
sig.s <- yst.s.all$we.eBH < 0.05 
sig.1 <- yst.1.all$we.eBH < 0.05 
```

We start with the assumption that not all statistically significant differences
are biologically relevant [@efron2008FDR], and that such a large number of
significantly different transcripts breaks the  necessary assumption for DA/DE
expression that most parts be invariant [@Dillies:2013]. As noted,
transcriptomics commonly uses a dual cutoff approach that is graphically
exemplified by volcano plots [@Cui:2003aa;@Schurch:2016aa]. Using either DESeq2
or ALDEx2, a majority of transcripts are statistically significantly different
between groups  with a  q-value cutoff of \( \le 0.05\); i.e. `r length(sig.des)` (`r round(length(sig.des)/nrow(yst.all), 2)*100`%, DESeq2) or
`r length(sig.ald)` (`r round(length(sig.ald)/nrow(yst.all), 2)*100`%, ALDEx2)
of the `r nrow(yst.all)` transcripts. These values are in line with those
observed by [@Schurch:2016aa]. Such  large numbers of statistically significant
transcripts seems biologically unrealistic. That `r length(setdiff(sig.ald,
sig.des))` transcripts are identified by ALDEx2 and not DESeq2, while DESeq2
identifies `r length(setdiff(sig.des, sig.ald))` transcripts that ALDEx2 does
not, suggests that the choice of normalization plays a role in which results are
returned as significant and that some, if not the majority, are driven by
technical differences in the analysis [@Dillies:2013;@Li:2022aa].


```{r plot1, echo=FALSE, warning=FALSE, fig.dim=c(5,6), fig.cap="Volcano and effect plots for unscaled and scaled transcriptome analysis. ALDEx2 was used to conduct a differential expression (DE) analysis on the yeast transcriptome dataset. The results were plotted to show the relationship between difference and dispersion using effect plots or difference and the q-values using volcano plots. Panels A,C are for the naive analyses, and Panels B,D are for the  default analyses that include scale uncertainty. Each point represents the values for one transcript, with the color indicating if that transcript was significant in the both analyses (red) or in the naive analysis only (orange). Points in grey are not statistically signficantly different under any condition. The horizontal dashed lines represent a \\(\\log_2\\)difference of \\(\\pm 1.4\\)."}


par(mfrow=c(2,2))
# volcano plot(res@listData$log2FoldChange,-1*log10(res@listData$padj + 1e-300),
# col=rgb(0,0,0,0.1), xlab='log2 Difference', ylab='-1 log10(p.adjust)')
# title('D: DESeq2 volcano', adj=0, line= 0.8)
# points(res@listData$log2FoldChange[sig.des],-1*log10(res@listData$padj[sig.des
# ] + 1e-300), col=rgb(1,.66,0,0.3), pch=19, cex=0.5)
# points(res@listData$log2FoldChange[sig.s.ald],-1*log10(res@listData$padj[sig.s
# .ald] + 1e-300), col=rgb(1,0,0,1), pch=19, cex=0.5) abline(v=c(-1.4,1.4),
# lty=2,lwd=2, col='grey')

l10q <- expression(-log[10](q)) 
l2d <- expression(log[2]~ Difference)

plot(yst.all$diff.btw, -1*log10(yst.all$we.eBH +1e-70), col=rgb(0,0,0,0.1), xlab=l2d, ylab=l10q) 
title(expression(paste("A: Volcano: ", gamma, " = 0")), adj=0, line= 0.8) 
points(yst.all$diff.btw[sig.ald],
-1*log10(yst.all$we.eBH[sig.ald]  +1e-70), col=rgb(1,.66,0,.3), cex=0.5, pch=19)
points(yst.all$diff.btw[sig.s.ald], -1*log10(yst.all$we.eBH[sig.s.ald]  +1e-70),
col=rgb(1,0,0,1), cex=0.5, pch=19) 
abline(v=c(-1.4,1.4), lty=2,lwd=2, col='grey')


plot(yst.s.all$diff.btw, -1*log10(yst.s.all$we.eBH +1e-15), col=rgb(0,0,0,0.1),
xlab=l2d, ylab=l10q) 
title(expression(paste("B: Volcano: ", gamma, " = 0.5")), adj=0, line= 0.8) 
points(yst.s.all$diff.btw[sig.s.ald], -1*log10(yst.s.all$we.eBH[sig.s.ald]+1e-15), col=rgb(1,0,0,1), cex=0.5, pch=19)
abline(v=c(-1.4,1.4), lty=2,lwd=2, col='grey')

#plot(res@listData$lfcSE*sqrt(ncol(yst)), res@listData$log2FoldChange,
#xlim=c(0,5), col=rgb(0,0,0,0.1), xlab='LFC SD', ylab='log2 Difference')
#title('A: DESeq2 effect', adj=0, line= 0.8)
#points(res@listData$lfcSE[sig.des]*sqrt(ncol(yst)),
#res@listData$log2FoldChange[sig.des], col=rgb(1,.66,0,0.5), pch=19, cex=0.5)
#points(res@listData$lfcSE[sig.s.ald]*sqrt(ncol(yst)),
#res@listData$log2FoldChange[sig.s.ald], col=rgb(1,0,0,0.5), pch=19, cex=0.5)
#abline(h=c(-1.4,1.4), lty=2,lwd=2, col='grey')

plot(yst.all$diff.win, yst.all$diff.btw, col=rgb(0,0,0,0.1), xlab='Dispersion',
ylab=l2d) 
title(expression(paste("C: Effect: ", gamma, " = 0")), adj=0, line=
0.8) 
points(yst.all$diff.win[sig.ald], yst.all$diff.btw[sig.ald],
col=rgb(1,.66,0,0.3), cex=0.5, pch=19) 
abline(h=c(-1.4,1.4), lty=2,lwd=2,
col='grey') 
points(yst.all$diff.win[sig.s.ald], yst.all$diff.btw[sig.s.ald],
col=rgb(1,0,0,0.6), cex=0.5, pch=19) 
abline(h=c(-1.4,1.4), lty=2,lwd=2,
col='grey')

plot(yst.s.all$diff.win, yst.s.all$diff.btw, col=rgb(0,0,0,0.1),
xlab='Dispersion', ylab=l2d, xlim=c(0.1,5)) 
title(expression(paste("D: Effect:
", gamma, " = 0.5")), adj=0, line= 0.8) 
points(yst.s.all$diff.win[sig.s.ald],
yst.s.all$diff.btw[sig.s.ald], col=rgb(1,0,0,0.5), cex=0.5, pch=19)
abline(h=c(-1.4,1.4), lty=2,lwd=2, col='grey')


```

The Volcano plots in Figure 1 A and B show  that adding scale uncertainty
increases the minimum q-value and increases the concordance between the q-value
and the difference between groups (compare panels A and B). The effect plots
[@gloor:effect] in Figure 1C  shows that the majority of significant transcripts
(red, orange) have negligible differences between groups and  very low
dispersion. We suggest that this low dispersion is driven by the experimental
design which is actually a technical wet lab replication  rather than a true
biological replication design [@Gierlinski:2015aa]. Scale uncertainty can be
incorporated using the \texttt{gamma} parameter that controls the amount of
noise added to the CLR mean assumption when we call either \texttt{aldex()}, or
\texttt{aldex.clr()}. Figure 1 B,D shows that  setting  \( \gamma=0.5 \) results
in `r length(sig.s.ald)` which is far fewer significant transcripts than in the
naive analysis  and we observe that  the minimum dispersion increases from  `r round(min(yst.all$diff.win), 2)` (\(\gamma = 0\) ) to  `r round(min(yst.s.all$diff.win), 2)` (\(\gamma=0.5\)).



```{r disp, eval=T, echo=F, warning=F, message=F,comment=F, fig.cap="Adding scale uncertainty changes the dispersion distribution. Panel A shows a plot of the expected value for relative abundance vs the expected value for the pooled dispersion as output by \\texttt{aldex.effect}. The dashed  horizontal lines show the median value for the features with a rAbundance between -0.5 and 0.5, and the light colored lines are lowess lines of fit through the center of mass of the data. Panel B plots the dispersion difference between  \\(\\gamma = 1\\) and \\(\\gamma = 0\\); note the non-linear relationship that highlights  the rotation that is evident in Panel A. The colored lines indicate the lowess line of fit through the centre of mass of the plot for the various  populations of points. The grey line is the total population and shows the difference \\(\\Delta\\), the red line is the population of significant transcripts (\\*) with \\(\\gamma = 0\\), the orange line is the population of significant transcripts with a difference threshold (T) of about \\(\\pm 2^{1.4}\\)-fold change, the blue line is the population of significant transcripts with \\(\\gamma = 0.5\\), and the cyan line is the significant population with \\(\\gamma = 1\\). \\(\\Delta\\): Difference, *: significant, T: thresholded. "}


cuts <- yst.all$rab.all > -0.5 & yst.all$rab.all < 5 
par(mfrow=c(1,2)) 
mn.mid <- median(yst.all$diff.win[cuts]) 
mn.mid.s <- median(yst.s.all$diff.win[cuts])
mn.mid.1 <- median(yst.1.all$diff.win[cuts])

no.scale <- data.frame(yst.all$rab.all, yst.all$diff.win) 
half.scale <- data.frame(yst.s.all$rab.all, yst.s.all$diff.win) 
full.scale <- data.frame(yst.1.all$rab.all, yst.1.all$diff.win)

# we change dispersion a lot for the bulk of the features
plot(yst.all$rab.all, yst.all$diff.win, pch=19, cex=0.5, col=rgb(0,0,0,0.3),
xlab='rAbundance', ylab='Dispersion') 
lines(lowess(no.scale, f=0.1), col='grey',
lwd=3) 
points(yst.s.all$rab.all, yst.s.all$diff.win, pch=19, cex=0.5,
col=rgb(1,0,0,0.3)) 
lines(lowess(half.scale, f=0.1), col='pink', lwd=3)
points(yst.1.all$rab.all, yst.1.all$diff.win, pch=19, cex=0.5,
col=rgb(0,0,1,0.3)) 
lines(lowess(full.scale, f=0.1), col='royalblue1', lwd=3)
abline(h=mn.mid, lty=2) 
abline(h=mn.mid.s, lty=2, col='red') 
abline(h=mn.mid.1,
lty=2, col='blue') 
title(main='A', line=-1.2, adj=0.9)

g0 <- expression(paste(gamma, " = 0")) 
g5 <- expression(paste(gamma, " = 0.5"))
g1 <- expression(paste(gamma, " = 1")) 
legend(1,4, legend=c(g0, g5, g1), pch=19, col=c("black", "red", "blue"))

# plot(yst.all$diff.win, yst.1.all$diff.win, pch=19, cex=0.5,
# col=rgb(0,0,1,0.3), ylim=c(0,4.5), xlab='g=0 Dispersion', ylab='g=0.5 | g=1
# Dispersion') points(yst.all$diff.win, yst.s.all$diff.win, pch=19, cex=0.5,
# col=rgb(1,0,0,0.3)) abline(0,1, lty=2) title(main='B', line=-1.2, adj=0.5)
# legend(0,4, legend=c( "0.5", "1"), pch=19, col=c("red", "blue"))

diff.df <- data.frame(yst.all$rab.all, yst.1.all$diff.win- yst.all$diff.win)
sig.df <- data.frame(yst.all$rab.all[sig.all], yst.1.all$diff.win[sig.all]-
yst.all$diff.win[sig.all]) 
sig.t.df <- data.frame(yst.all$rab.al[sig.t],
yst.1.all$diff.win[sig.t] - yst.all$diff.win[sig.t]) 
sig.s.df <-
data.frame(yst.all$rab.all[sig.s], yst.1.all$diff.win[sig.s] -
yst.all$diff.win[sig.s]) 
sig.1.df <- data.frame(yst.all$rab.all[sig.1],
yst.1.all$diff.win[sig.1]- yst.all$diff.win[sig.1])

ylab1 <- expression(paste("Dispersion: ", gamma, "1 - ", gamma, "0"))

plot(yst.all$rab.all, yst.1.all$diff.win- yst.all$diff.win, xlab='rAbundance',
ylab=ylab1, pch=19, cex=0.5, xlim=c(-11,10), ylim=c(.1,1.5)) 
title(main='B',
line=-1.2, adj=0.9) 
lines(lowess(diff.df, f=0.1), col='grey60', lwd=3)
lines(lowess(sig.df, f=0.1), col='red', lwd=3) 
lines(lowess(sig.t.df, f=0.5),
col='orange', lwd=3) 
lines(lowess(sig.s.df, f=0.5), col='royalblue1', lwd=3)
lines(lowess(sig.1.df, f=0.7), col='cyan', lwd=3)

d0 <- expression(paste(Delta)) 
dstar <- expression(paste(Delta,"*")) 
dstarT <- expression(paste(Delta,"*T")) 
dg5 <- expression(paste(Delta,"*", gamma, "0.5"))
dg1 <- expression(paste(Delta,"*", gamma, "1"))

legend(-12,1.6, legend=c( d0, dstar, dstarT,dg5, dg1), pch="-", col=c("grey",
"red", "orange", "royalblue1", "cyan"))

```

It is common practice to use a dual-cutoff by choosing transcripts based on a
thresholds for both q-values and fold-changes  [@Cui:2003aa;@Schurch:2016aa].
Note that there is considerable variation in recommended cutoff
values[@Schurch:2016aa]. Here, applying a dual-cutoff using a heuristic of at
least a \(2^{1.4}\) fold change  reduces the number of significant outputs to
193 for DESeq2 and to 186 for ALDEx2. This cutoff was chosen for convenience and
is in-line with the recommendations of [@Schurch:2016aa] with the fold change limits
 shown by the dashed grey lines in Figure 1. The  \(2^{1.4}\) fold change
cutoff identifies a similar number of transcripts as does  ALDEx2 using \(
\gamma = 0.5 \) which identifies `r length(sig.s.ald)`.  Supplementary Figures 1
and 2 shows how to use the \texttt{aldex.senAnalysis()} function to identify
those transcripts that are very sensitive to scale uncertainty. In this
supplementary figure we see that even adding a very small amount of scale \(
\gamma = 0.1 \) reduces the number of significant transcripts by more than half.
This allows us to ignore those low-dispersion transcripts that were  significant only
because of an absence of scale uncertainty. In practice, we suggest that a
\texttt{gamma} parameter between 0.5 and 1 is realistic for most experimental
designs [@Nixon2024B].

The effect on dispersion with increasing  amounts of scale uncertainty are shown
in Figure 2A, where we can see that the  dispersion increases as uncertainty is
added. Note that the dispersion in the unscaled analysis in Figure 2A reaches a
minimum near the mid-point of the distribution, and also does so when the
analysis is conducted with DESeq2 (Supplementary Figure 3). This shows more
clearly that dispersion of many transcripts is  almost negligible in the absence
of scale uncertainty. This plot makes the counter-intuitive suggestion that the
variance in expression of the majority of genes with moderate expression is more
predictable  than highly-expressed genes or of housekeeping genes
[@Rocha:2020aa].  This is at odds with the known biology of cells where single
cell counting of highly-expressed transcripts shows that they have little
intrinsic variation [@Taniguchi:2010aa;@Scott:2010].


Adding scale uncertainty by setting \(\gamma=0.5\) , or \(\gamma = 1.0\),
increases the minimum dispersion as shown in Figure 2A by the red and blue data
points, and by the colored lines of fit through the centre of mass of the data.
Less obvious is that the additional dispersion is  not applied equally to all
points. Figure 2B shows a plot of the difference between the \(\gamma= 0\) and
\(\gamma= 1\) data  and here we can see that  scale uncertainty is
preferentially increasing the dispersion of the mid-expressed transcripts that
formerly had negligible dispersion; examine the grey line of best fit (overlaid
by the red line) for the trend. Panel B also shows the trend of the
expression-dispersion relationship for transcripts that are classed as
statistically significant. The red line shows the trendline with no added scale
uncertainty, and this trendline exactly overlays with the grey trendline for the
bulk of transcripts. The orange trendline indicates those transcripts that are
both statistically significant and that have a thresholded expression level of
\( \pm 1.4\), and the dark blue and cyan lines show the statistically
significant trendline for \(\gamma=0.5\) , or \(1.0\).

Thus, taking Figures 1 and 2 together adding scale uncertainty has the desirable
effect of changing the distribution of transcripts identified as significantly
different between groups. Those parts that were statistically significantly
different *only because of low dispersion* are preferentially excluded from
statistical significance while those parts that were significantly different
because of a high difference between groups remain.


<!-- Interestingly, the yeast transcriptome experiment has an unacknowledged
difference in scale between conditions; yeast deficient for snf1 are smaller,
grow more slowly and are sensitive to a variety of common agents that cause cell
stress [@Yoshikawa:2011aa]. The CLR normalization assumes that the scale of the
wild-type is exactly 98.4\% of the snf1 strain, and so is \( \theta = 0.016\).
Given the per-generation time difference of about 15\%, this seems implausible.
Setting \(\gamma=0.5\) can be interpreted as acknowledging uncertainty in the
difference in the underlaying scale between groups. Nixon et al. -@Nixon2024B
showed that the 95\% bounds on the scale uncertainty for a given \(\gamma\)
value, say 0.5, can be calculated as \(2^{-(2 \gamma + \theta)}, 2^{(2 \gamma +
\theta)}\), or \(2^{-(2*0.5 + 0.016)}, 2^{(2*0.5 + 0.016)} = 0.494,2.022\).
Thus, by adding scale uncertainty, we are acknowledging that the data are
uncertain and that any results are robust to that uncertainty, within the bounds
of our model. -->

## Housekeeping genes and functions to guide scale model choices.

Dos Santos et al. [@dosSantos:2024]  used a vaginal metatranscriptome dataset  to
compare the gene expression in bacteria collected from healthy (H) and bacterial
vaginosis (BV) affected women. In this environment, both the relative abundance
of species between groups and the gene expression level within a species is
different [@macklaim:2013]. Additionally, prior research suggests that the total
number of bacteria is about 10 times more in the BV than in the H condition
[@Zozaya:2010]. Thus, these are extremely challenging datasets in which to
determine differential abundance as there are both compositional and scale
changes between conditions. The usual method to analyze vaginal
metratranscriptome data is to do so on an organism-by-organism basis
[@macklaim:2013; @Denge00262-18; @Fettweis:2019aa] because the scale confounding
of the environment is less pronounced. One attempt at system-wide analysis
returned several housekeeping functions as differentially expressed between
groups [@Denge00262-18]; a result likely due to a disconnect between the
assumptions of the normalization used and the actual scale of the environment
[@Wu2021].

In this example, we show how to specify and interpret an informed scale model
that can explicitly account for some of these modeling difficulties
[@Nixon2024B] even in a difficult dataset. An informed scale model can control
for both the mean difference of scale between groups (e.g., directly incorporate
information on the differences in total number of bacteria between the BV and H
conditions) as well as the uncertainty of that difference. To specify a
user-defined scale model, we can pass a matrix of scale values instead of an
estimate of just the scale uncertainty  to \texttt{aldex.clr()}. This matrix
should have the same number of rows as the of Monte-Carlo Dirichlet samples, and
the same number of columns as the number of samples. While this matrix can be
computed from scratch by the analyst, there is an
\texttt{aldex.makeScaleModel()} function that can be used to simplify this step
in most cases. This encodes the scale model as \(\Lambda \sim N(log_2 \mu_n,
\gamma^{2})\), where \(\mu_n\) represents the scale value for each sample or
group and gamma is the uncertainty as before. The scale estimate can be a
measured value (cell count, nucleic acid input, etc) or an estimate.  Nixon et
al. [@nixon2024scale;@Nixon2024B] showed that only the ratio of the means are important when
providing values for \( \mu_n\); i.e., the ratio between the \( \log_2 \mu_i\)
and \(\log_2 \mu_j\) values. See the supplement to  Nixon et al. [@Nixon2024B]
for more information.

Figure 3A shows an effect plot of the data where reads are grouped by homologous
function regardless of the organism of origin. Each point represents one of 
3728 KEGG  functions [@Okuda:2008]. There are many more functions represented in
the BV group (bottom) than in the healthy group (top). This is because the
\textit{Lactobacilli} that dominate a healthy vaginal microbiome have reduced
genome content relative to the anaerobic organisms that dominate in BV, because
there is a greater diversity of organisms in BV than in H samples, and because
the BV condition has about  an order of magnitude more bacteria than does the
H condition.

The naive scale model appears to be reflecting the bacterial load as observed by calculating the mean scale value for each group. Using a negligible scale value; i.e., \(\gamma=1e-3\) exposes the naive scale estimate for
samples  in the \texttt{@scaleSamps} slot from the
\texttt{aldex.clr} output. the naive scale estimate for the
`r load('analysis/xg.clr.Rda')`
healthy group is `r round(mean(rowMeans(xg@scaleSamps[c(1:8,37:44),])),2)` and
for the BV group is  `r round(mean(rowMeans(xg@scaleSamps[c(9:36),])),2)` for a
difference of `r round(mean(rowMeans(xg@scaleSamps[c(1:8,37:44),])) - mean(rowMeans(xg@scaleSamps[c(9:36),])),2)`. This is interpreted as the scale of
the H group of samples being 
`r round(2^(mean(rowMeans(xg@scaleSamps[c(1:8,37:44),])) - mean(rowMeans(xg@scaleSamps[c(9:36),]))),2)` 
greater than the BV group. This
precise but incorrect estimate places the location of the housekeeping genes off
the midline of no difference.

Applying the default scale model of \(\gamma=0.5\) increases the dispersion
slightly but does not move the   housekeeping functions toward the midline. This
is as expected; the mean of the default scale model is based on the CLR
normalization so no shift in location would be expected over the original ALDEx2
model. Nevertheless, about 30\% of the housekeeping functions are no longer
statistically significantly different. Note that this change is simple to
conduct, has no additional computational complexity and requires only a slight
modification for the analyst.

```{r ribo, echo=F}
# list of ribosomal associated functions in the datset TIL about
# complete.cases()
ribo.v <-c("K02863","K02864","K02867","K02871","K02874","K02876","K02878","K02879","
K02881","K02884","K02886","K02887","K02888","K02890","K02892","K02895","K02897",
"K02899","K02902","K02904","K02906","K02907","K02909","K02911","K02913","K02914"
,"K02916","K02926","K02931","K02933","K02935","K02939","K02945","K02946","K02948
","K02950","K02952","K02954","K02956","K02959","K02961","K02963","K02965","
K02967","K02968","K02970","K02982","K02986","K02988","K02990","K02992","K02994",
"K02996") 
```

There are 101  functions with low dispersion that appear to be shared by both
groups (boxed area in Figure 3A, and colored in cyan). Inspection shows that
these   largely correspond to core metabolic functions such as transcription,
translation, ribosomal functions, glycolysis, replication, chaperones, etc
(Supplementary file housekeeping.txt).  The transcripts of many of these are
commonly used as invariant reference sequences in wet lab experiments
[@Rocha:2020aa] and so are not be expected   to contribute to differences in
ecosystem behaviour. The average location of these should be centred on 0
difference to represent an internal reference set.  However, without an informed
scale model, the mean  of these housekeeping functions is  approximately located
at +2.3.



```{r meta, echo=F, warning=F, message=F,comment=F, result=F, fig.cap="Analysis of vaginal transcriptome data aggregated at the Kegg Orthology (KO) functional level. Panel A shows an effect plot for the default analysis where the functions that are elevated in  the  healthy individuals have positive values and functions that are elevated in BV have negative values. Highlighed in the box are KOs that are almost exlusively housekeeping functions; these and are colored cyan. These housekeeping functions should be located on the midline of no difference.  Panel B shows the same data scaled with \\(\\gamma = 0.5\\), which increase the minimum dispersion as before.  Panel C shows the same data scaled with \\(\\gamma = 0.5\\) and a 0.14 fold difference in dispersion applied to the BV samples relative to the H samples. In these plots statistically significant (q-value < 0.01) functions in the informed model are  in red, false positive functions are in blue,  non-significant functions in black and false negative functions are in orange. " }

# all code for ALDEx2 and DESeq is being moved to the code/directory all
# variables recreated using the makefile from code/meta_aldex.R

load('analysis/xt.Rda') 
load('analysis/xg.Rda')
load('analysis/xt.m.Rda') 
load('analysis/xt.lv.Rda')
load('analysis/xg.clr.Rda') 

hk <- rownames(xt.all)[xt.all$diff.win < 2.5 & xt.all$diff.btw > 1 &
xt.all$diff.btw < 3] 
hk.off <- xt.all$diff.win < 2.5 & xt.all$diff.btw > 1 &
xt.all$diff.btw < 3

fn <- rownames(xt.m.all)[xt.m.all$we.eBH < 0.01 & xt.m.all$diff.btw < 1 & xt.all$we.eBH > 0.01] 
fp <- rownames(xt.all)[xt.all$we.eBH < 0.01 &
xt.m.all$we.eBH > 0.01 & !xt.m.all$we.eBH < 0.01]

#par(mfrow=c(1,1)) 
#plot(density(xt.m.e$diff.btw[hk.off])) 
#abline(v=0, col='red')

par(mfrow=c(1,3)) 
aldex.plot(xt.all, cutoff.pval=0.01, xlim=c(0.3,9),xlab="Dispersion", ylab=l2d) 
points(xt.all[fp,'diff.win'],xt.all[fp,'diff.btw'], col='blue', cex=0.4, pch=19)

points(xt.all$diff.win[hk.off], xt.all$diff.btw[hk.off], pch=19, cex=0.4,
col=rgb(0,1,1,0.5)) 
points(xt.all[fn,'diff.win'], xt.all[fn,'diff.btw'],
col='orange', cex=0.4, pch=19) 
rect(0.5,1,2.5,3, col=rgb(0,0,0,0.1), lty=2, lwd=3) 
title('A: ALDEx2 unscaled', adj=0, line= 0.8) 
legend(0,12, legend=c("HK",
"TP", "FP", "FN"), pch=19, cex=0.5, col=c("cyan", "red", "orange", "blue"))

aldex.plot(xg.all, xlim=c(0.3,9), cutoff.pval=0.01, xlab="Dispersion", ylab=l2d)
points(xg.all[fp,'diff.win'], xg.all[fp,'diff.btw'], col='blue', cex=0.4,
pch=19) 
points(xg.all$diff.win[hk.off], xg.all$diff.btw[hk.off], pch=19,
cex=0.4, col=rgb(0,1,1,0.5)) 
points(xg.all[fn,'diff.win'],
xg.all[fn,'diff.btw'], col='orange', cex=0.4, pch=19) 
title('B: ALDEx2 gamma',
adj=0, line= 0.8)


aldex.plot(xt.m.all, xlim=c(0.3,9), cutoff.pval=0.01, xlab="Dispersion",
ylab=l2d) 
points(xt.m.all[fp,'diff.win'], xt.m.all[fp,'diff.btw'], col='blue',
cex=0.4, pch=19) 
points(xt.m.all$diff.win[hk.off], xt.m.all$diff.btw[hk.off],
pch=19, cex=0.4, col=rgb(0,1,1,0.5)) 
points(xt.m.all[fn,'diff.win'],
xt.m.all[fn,'diff.btw'], col='orange', cex=0.4, pch=19) 
title('C: ALDEx2 both',
adj=0, line= 0.8)


```


We desire a scale model that approximately centres the housekeeping functions, because we expect housekeeping functions tp be nearly invariant; thus an
appropriate scale in this dataset for functional analysis is likely closer to 0 than the naive estimate.
One way to choose an appropriate value for \(\mu_n\) is to use the
\texttt{aldex.clr} function on only the presumed invariant functions setting
\(\gamma > 0\), and then accessing the \texttt{@scaleSamps} slot as before.
Doing so suggests that the difference in scale should be about 14%.   A second
approach would be to identify the functions used as the denominator with the
\texttt{denom="lvha"} option [@Wu2021] for the \texttt{aldex.clr} function, and
then to use these values as before. This approach suggests a 5% difference in
scale, and is potentially less subject to user interpretation.

`r load('analysis/xt.m.Rda')`

For the purposes of this example, if we assume a 14% difference in scale, we can
set \(\mu_i = 1\) and \(\mu_j = 1.14\) using the \texttt{makeScaleMatrix}
function. This function uses a logNormal distribution to build a scale matrix
given a user-specified mean difference between groups and uncertainty level.
Applying a per-group relative differential scale of 0.14 moves the housekeeping
functions close to the midline of no difference (Figure 3C, assuming 14% mean difference = 
`r round(mean(xt.m.all[hk.off,"diff.btw"]),2)`, assuming a 5% mean difference = 
`r round(mean(xt.lv.all[hk.off,"diff.btw"]),2)`), and applying a gamma of 0.5
provides the same dispersion as in panel B of Figure 3. Note that now a
significant number of functions are differentially up in BV that were formerly
classed as not different without the full scale model (orange), or when only a
default scale was applied. Inspection of the functions shows that these are
largely missing from the \emph{Lactobacillus} species and so should actually be
captured as differentially abundant in the BV group. Supplementary Figure 4
shows that the using either the 5% or the 14%  scale difference  give
imperceptibly different results suggesting that an informed scale model does not
have to precisely estimate the scale difference to be useful. Nixon et al,
@Nixon2024B also found that multiple reasonable estimates for the \(\mu_n\) part
of the informed scale model were similarly useful in microbiome data.

Thus, applying an informed scale allows us to distinguish between both false
positives (housekeeping functions in cyan, and others in blue) and false
negatives (orange functions) even in a very difficult to analyze dataset. The
remarkable improvements in biological interpretation afforded by an informed
scale model, and the transferrability of it between sample cohorts of the same
condition is outlined elsewhere [@dosSantos:2024]. We suggest that the default
scale model  is sufficient when the data are approximately  centred. However,
an informed model is more appropriate with datasets are not well centred or
when the investigator has prior information about the underlying biology.


# Discussion

Biological systems are both predictably variable and stochastic
[@Taniguchi:2010aa] and systems biology experiments show that there are
transcripts with approximately constant concentrations in the cell and those
with large variability under different growth conditions [@Scott:2010].  Current
measurement methods that rely on high throughput sequencing fail to capture all
of the variation, particularly variation due to scale
[@nixon2024scale;@Nixon2024B].  In the absence of external information
[@Loven:2012aa;@Vandeputte:2017aa;@yeast-absolute] sequencing depth
normalisation methods cannot recapture the scale information
[@Loven:2012aa;@Lovell:2015], and can only normalize for the technical variation
due to sequencing depth. Here we demonstrated that even approximate estimates of
the true system scale and the uncertainty of measuring it can aid in the interpretation of
RNA-sequencing experiments.

Nixon et al. [@nixon2024scale] introduced the idea of explicitly modeling the
scale of a HTS dataset, and showed how to incorporate these models in the
analysis of microbiome and other datasets [@Nixon2024B]. They  demonstrated that
many tools commonly used to analyze HTS datasets had substantial Type 1 and Type
2 error rates, in line with recent findings by others
[@Quinn:2018aa;@Ge:2021aa;@Li:2022aa]. A version of ALDEx2 with the ability to
include scale uncertainty was shown to be able to correct for the high Type 1
error rate for that tool, albeit with some loss of sensitivity. Finally, they
showed that incorporating an informed scale model incorporating both location
and scale uncertainty estimates could both control for Type 1 and Type 2 error
rates [@Nixon2024B].

Building and using a scale model thus has substantial benefits relative to the
dual cutoff approach that is advocated for many gene expression experiments
[@Cui:2003aa;@Schurch:2016aa]. In particular, the dual cutoff approach has long
been known to not control for Type 1 errors [@Zhang:2009aa;@Ebrahimpoor:2021aa],
and the frequent lack of concordance between tools when benchmarked on
transcriptomes[@Bullard:2010;@Soneson:2013;@Schurch:2016aa;@Quinn:2018aa;@Ge:2021aa;@Li:2022aa] and microbiomes [@McMurdie:2014a;@Thorsen:2016aa;
@Weiss:2017aa; @hawinkel2017;@Nearing:2022aa;@Yerke:2024aa] suggests poor
control of Type 2 errors as well [@Quinn:2018aa;@Li:2022aa]. Thus, incorporating
a scale model during the analysis of HTS data promises the best of both worlds.
A default scale model can control for Type 1 errors with minimal prior knowledge
of the environment and this can be done with essentially no additional
computational overhead. Furthermore, this work and previous [@Nixon2024B] show
that even minimal information about the underlying environment can be used to
build a relatively robust informed scale model that controls for both Type 1 and
2 error rates.

In the analysis of HTS data it is often observed that larger datasets converge
on the majority of parts being significantly different
[@Schurch:2016aa;@Li:2022aa;@nixon2024scale]. Li et al. [@Li:2022aa] conducted a
permutation-based benchmarking study and found that widely used tools performed
worse than simple Wilcoxon rank-sum tests in controlling the FDR when sample
sizes became large. They suggested that the presence of outliers were one of the
factors driving this observation. Brooks et al. [@Brooks:2024aa] suggested that
inappropriate choice of benchmarking methods are also a major contributing
factor and that objective standards of truth are important. From the perspective
of our work the disagreement between tools can be explained by the observation
that different analytic approaches  produce  different parameter estimates for
location or scale or for both. Thus, more data produces worse estimates because the
additional data simply increases the precision of a flawed estimate
[@gustafson2015bayesian;@nixon2024scale].

Scale simulation  is now built into ALDEx2 [@Nixon2024B] and here we suggest
that  there are two main root causes to common HTS data pathologies. The first
contributing factor is the observed very low dispersion estimate for many
features that is a by-product of some experimental designs and of normalization.
In the Schurch et al. [@Schurch:2016aa dataset], the data were  from single
colonies derived from a single culture. Thus, it is more accurate to describe
the 96 samples as wet-lab technical replicates rather than independent samples.
This type of replication approach is standard in the molecular literature, and
would be expected to result in the very low dispersion that is observed.
Applying the default scale model with \( \gamma=0.5 \) a large number of
transcripts have  their dispersion increased (Figure 1D), with the effect being
largest for those with the lowest initial dispersion (Figure 2). Adding scale
uncertainty results in modest number of transcripts, `r length(sig.s.ald)`,
being called significantly different as shown in the volcano plot in Figure 1B
(red points). In addition, there is now a strong concordance between the
difference and q-values. In hindsight, it is not obvious  why the unscaled
volcano plot shows such poor correspondence. We suggest that this is explained
by random fluctuations of the many very low variance estimates and this is
supported by the  plots shown in Figure 2.


The second contributing factor is unacknowledged asymmetry in many datasets
[@Wu2021]; i.e., different gene content or a directional change in the majority
of features. In the case of asymmetry, the use of a user-specified scale model
can be very useful for otherwise difficult-to-analyze datasets such as
meta-transcriptomes and in-vitro selection datasets where the majority of
features can change. We showed one such example for the metatranscriptome
dataset in Figure 3. Here the dataset was highly asymmetrical. Incorporating
differential scale on a per-group basis moves the mass of the housekeeping
functions towards the midline of no difference and so affects both Type I and
Type II error rates. We showed two ways of estimating the  scale difference
between groups and found that any reasonable estimate is an improvement over the
naive approach and also over the default scale model. This is in line with the
observations by Nixon et al [@Nixon2024B] in a 16S rRNA gene sequencing dataset.
It is also of note that in the case of true biological replicates (different
individuals) that adding a modest amount of scale \(\gamma=0.5\) had little
effect on the the difference between groups and on the dispersion. Thus, in this
dataset the scale mis-specification was affecting mainly the location of the
difference between groups.  While we acknowledge that some prior information on
which housekeeping transcripts should not be classed as differentially abundant
is needed, we suggest that this information is widely available and is already
used when performing the gold-standard quantitative PCR test of differential
abundance [@Thellin:1999aa;@SEQC/MAQC-III-Consortium:2014aa].

Beyond concerns of fidelity and rigor, scale models also enhance the
reproducibility and transparency of HTS analyses. The addition of scale
uncertainty essentially tests the model over a range of normalizations
[@nixon2024scale] and so can replace the consensus approach that has been
proposed by some groups [@Nearing:2022aa;@Song:2023aa] with no additional
computational overhead. Thus, an advantage of incorporating scale is that
analyses can be made much more robust such that actual or potential differences
in scale can be tested and accounted for explicitly. While it is beyond the
scope of the present article, we note that there are many ways of building scale
models that enhance the interpretability of the parameters and assumptions and a
detailed description of these points is describe elsewhere [@nixon2024scale,].

In summary, we supply a toolkit that makes incorporating scale uncertainty and
location information simple to incorporate for transcriptomes or indeed any type
of HTS dataset. While the underlying scale of the system is generally
inaccessible, the effect of scale on the analysis outcomes can be modelled and
can help explain some of the underlying biology, and help to expose known issues
with the analysis of HTS data. Adding scale information to the analysis allows
for more robust inference because the features that are sensitive to scale can
be identified and their impact on conclusions weighted accordingly.
Additionally, the use of informed  scale models permits difficult to analyze
datasets to be examined in a robust and principled manner even when the majority
of features are asymmetrically distributed or expressed (or both) in the groups
[@dosSantos:2024]. Thus, using and reporting scale uncertainty should become a
standard practice in the analysis of HTS datasets.

\singlespacing

# References

