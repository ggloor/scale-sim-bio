---
title: "Explicit Scale Simulation for analysis of RNA-sequencing count data with ALDEx2"
shorttitle: "Scale ALDEx2"
author:
  - name: Greg Gloor 
    affiliation: Department of Biochemistry, University of Western Ontario 
    email: ggloor@uwo.ca 
  - name: Michelle Pistner Nixon 
    affiliation: Department of Population Health Sciences, Geisinger, Danville, PA 
  - name: Justin Silverman 
    affiliation: College of Information Sciences and Technology and Department of Medicine, Pennsylvania State University, Department of Medicine, Pennsylvania State University, Institute for Computational and Data Science, Pennsylvania State University, Department of Statistics, Pennsylvania State University
 

cite-method: citeproc 
citeproc: TRUE 
bibliography: /Users/ggloor/Library/texmf/bibtex/bib/bibdesk_refs.bib 
csl: /Users/ggloor/Documents/0_git/csl_styles/nucleic-acids-research.csl 
output:
    pdf_document:
      keep_tex: true
header-includes:
  - \usepackage{setspace}
  - \usepackage[left]{lineno}

package: ALDEx2

Abstract: In high-throughput sequencing (HTS) studies, sample-to-sample variation in sequencing depth is driven by technical factors, and not by variation in the scale (size) of the biological system. Typically a statistical normalization removes unwanted technical variation in the data or the parameters of the model to enable differential abundance analyses. We recently showed that all normalizations make implicit assumptions about the unmeasured system scale and that errors in these assumptions can dramatically increase false positive and false negative rates. We demonstrated that these errors can be mitigated by accounting for uncertainty using a *scale model*, which we integrated into the ALDEx2 R package. This article provides new insights focusing on the application to transcriptomic analysis. We provide transcriptomic case studies demonstrating how scale models, rather than traditional normalizations, can reduce false positive and false negative rates in practice while enhancing the transparency and reproducibility of analyses. These scale models replace the need for dual cutoff approaches often used to address the disconnect between practical and statistical significance. We demonstrate the utility of scale models built based on known housekeeping genes in complex metatranscriptomic datasets. Thus this work provides guidance on how to incorporate scale into transcriptomic data sets.

---

# Introduction

\doublespacing \singlespacing 
\linenumbers

High-throughput sequencing (HTS) is a ubiquitous tool used to explore many biological phenomenon such as gene expression (single-cell sequencing, RNA-sequencing, meta-transcriptomics), microbial community composition (16S rRNA gene sequencing, shotgun metagenomics) and differential enzyme activity (selex, CRISPR killing). HTS proceeds by taking a sample from the environment, making a library, multiplexing (merging) multiple libraries together, and then applying a sample of the multiplexed library to a flow cell. Each of these steps is a compositional sampling step as only a fixed-size subsample of nucleic acid is carried over to subsequent steps. Thus, with each sampling step the connection between the actual number of molecules in the sampled DNA pool and the environmental scale (e.g., total number of molecules, microbial load, or total gene expression) of the measured biological system is degraded or lost. In the end, the information contained in the data relates only to relative abundances and  has an arbitrary scale imposed by the sequencing process [@lovell:2011;@Quinn:2019aa;@nixon2024scale]. 

The analysis of  HTS data suffers from several known problems that can be traced, in whole or in part, to misspecification of scale in the output data. The first issue is poor control of the false discovery rate (FDR) [@Thorsen:2016aa;@Quinn:2018aa;@Nearing:2022aa;@Ge:2021aa;@Li:2022aa], exhibited as dataset-dependent FDR control which is observed as a disconnect between statistical and biological significance [@Schurch:2016aa;@Li:2022aa]. In current practice, this issue is addressed  by a dual-filtering method, whereby both a low p-value (or equivalently a low q-value following FDR correction [@storey2003positive]) and a large difference between groups is used to identify  transcripts or genes of interest for follow-up analysis [@Cui:2003aa;@Schurch:2016aa]. This double-filtering approach is graphically exemplified by the volcano plot [@Cui:2003aa], but is known to not appropriately control the  FDR [@Zhang:2009aa;@Ebrahimpoor:2021aa]. Since there is no standard way of determining what fold-change cutoff should be used researchers have unlimited degrees of  freedom which is known to lead to unreliable inference [@simmons2011false]. In particular Li et al. [@Li:2022aa] used patient or clinical-derived transcriptome datasets and found that many methods suffer from an extremely high false positive rate. The second issue is poor performance when analyzing data where the mean change between groups is non-zero [@nixon2024scale]. Such asymmetric data can arise when a gene set is expressed in one group but not the other, or when one group contains different gene content from the other. This type of data frequently arises in in-vitro selection experiments (SELEX), transcriptome analysis, and microbiome analysis [@Wu2021]. The third issue is that the actual scale of the environment is often a major confounding variable during analysis [@nixon2024scale;@Nishijima:2024aa]. This has long been known in transcriptome analysis and was a major driver for the development of normalization factors [@Bullard:2010] and the use of molecular spike-ins  to provide a reference set of known counts [@Jiang:2011aa;@Loven:2012aa;@Vandeputte:2017aa;@Props:2017aa] to estimate scale. While often useful, spike-in methods only provide information downstream of the step in the sample preparation protocol where the intervention was made and  introduce an additional source of variation that must be accounted for [@Loven:2012aa]. In the microbiome field, a recent landmark paper showed that biological scale was a major unacknowledged confounder in many human  analyses [@Nishijima:2024aa]. These authors built a machine learning model   to uncover the biological variation in scale and including this information  was useful despite exhibiting only a  modest correlation (estimated to be 0.6)  with the  scale of data external to the training set [@Nishijima:2024aa]. The final issue is that all the above problems become more pronounced as more samples are collected; that is, more information results in optimizing for a precise but inaccurate analysis [@Li:2022aa;@nixon2024scale;@Nixon2024B].

The  four problems were recently shown by Nixon et al. [@nixon2024scale]  to be a result of a mismatch between the underlying size or scale of the system and the assumptions of the normalizations used for the analysis of HTS. Biological variation in scale often represents an important unmeasured confounder in HTS analyses [@Loven:2012aa;@Lovell:2015;@Wu2021;@Nishijima:2024aa]. For example, cells transformed by the cMyc oncogene have about 3 times the amount of mRNA and about twice the rRNA content than  non-transformed cells [@Nie:2012aa], and this dramatically skews transcriptome analysis unless spike-in approaches are used [@Loven:2012aa]. In addition, wild-type and mutant strains of cell lines, yeast or bacteria  have different growth rates and RNA contents under different conditions, which  affect our ability to identify truly differentially abundant genes [@Scott:2010;@Yoshikawa:2011aa;@Lin:2018aa]. As another example, the total bacterial load of the vaginal microbiome differs by 1-2 orders of magnitude in absolute abundance between the healthy and bacterial vaginosis states [@Zozaya:2010], and the cell and RNA composition between these states is dramatically different [@Ravel:2010;@Hummelen:2010]. Thus, a full description of any of these systems includes both relative change (composition) and absolute abundance (scale).  Current methods access only the compositional information yet make implicit assumptions about the scale [@nixon2024scale,@Nixon2024B].

Nixon et al. [-@nixon2024scale] showed that the challenge of non-biological variation in sequencing depth could be explained as a problem of partially-identified models. They showed that \emph{all} normalizations make some assumption about  scale but these implicit assumptions are often inappropriate and difficult to interpret. As a result, different normalizations  provide different outputs when applied to the same dataset [@Bullard:2010;@Dillies:2013;@Thorsen:2016aa;@Weiss:2017aa;@Nearing:2022aa]. Intuitively, normalizations in widespread use  assume that either all samples have the same scale, e.g. proportions, rarefaction [@Hughes:2005tu], RPKM [@Mortazavi:2008; @wagner:tpm], etc; or that a subset of features in one sample can be chosen as a reference to which the others are scaled e.g. the TMM [@Robinson:2010a], the LVHA [@Wu2021]  qPCR [@Vandesompele:2002aa],  the additive log-ratio [@aitchison1982]; or that different sub-parts of each sample maintain a constant scale across samples e.g. the RLE [@Anders:2010];  or that the geometric mean of the parts is appropriate e.g the CLR [@Aitchison:1986] and its derivatives. Notably when the assumption of similar scale across samples is not violated, or violated only weakly,  any method will provide a reasonable analysis. The problem arises  when this assumption is violated, in which case all tools fail without warning.

```{r, out.width="0.6\\linewidth", , echo=FALSE, include=TRUE, fig.align="center", fig.cap=c("Mismatches between estimated scale and true scale lead to poor estimation of high throughput sequencing data. All normalizations used for differential abundance analysis make some strict assumption about the scale of the environment as shown in line 1. In this example all normalizations produce a biased estimate of the environmental scale, but estimate C is the closest to the truth.  Adding uncertainty to normalization C as represented by the distribution in the line 2 leads to less bias and now includes the actual environmental scale in the assumption. As shown in line 3 in some cases it may be useful to adjust the centre of the scale uncertainty estimate if the initial normalizations give very poor estimates of the underlying environment.")}
knitr::include_graphics("./go3_files/figure-latex/scale-explained.pdf")
```

The original scale-naive ALDEx2 [@fernandes:2013] model unwittingly made a strict assumption about scale through the CLR normalization and we found that the CLR was very sensitive to violations of the assumption of scale identity between groups [@Wu2021]. Moreover, when the assumption of identity was not true the CLR used by ALDEx2 could be outperformed by other normalizations in simulation studies [@Yerke:2024aa]. As illustrated graphically in Figure 1, Nixon et al. [@nixon2024scale] showed through simulation that introducing uncertainty in the scale assumptions, and in extreme cases altering the location of the scale assumption, resulted in more reproducible data analysis including better control of both false positive and false negative results. We  modified ALDEx2  to explicitly model uncertainty in scale over a range of reasonable normalization parameters, and showed significant improvements in performance in microbiome and in-vitro selection experiments [@Nixon2024B] and in a vaginal metatranscriptome analysis [@Dos-Santos:2024aa]. Here, we briefly  review these modifications and show how adding scale uncertainty can greatly improve modeling in transcriptome and meta-transcriptome datasets to provide substantially more robust and reproducible results.

# Methods

Formal and expanded descriptions of the concepts that follow are given in [@nixon2024scale;@Nixon2024B].   To be  concrete, we let \(\mathbf{Y}\) denote the \emph{measured} \(D \times N\) matrix of sequence counts with elements \(\mathbf{Y}_{dn}\) indicating the number of measured DNA molecules mapping to feature \(d\) (e.g., a taxon, transcript or gene) in sample \(n\).  Likewise, we can denote \(\mathbf{W}_{dn}\) as the \emph{true}  amount of class \(d\) in the biological system from which sample \(n\) was obtained. We can think of \(\mathbf{W}\) as consisting of two parts, the scale \(\mathbf{W}^{Tot}\) (e.g., totals) and the composition \(\mathbf{W}^{Comp}\) (i.e., proportions). That is, \(\mathbf{W}^{Tot}\) is a \(N\)-vector with elements \(\mathbf{W}^{Tot}_{n}=\sum_{d}\mathbf{W}_{dn}\) while \(\mathbf{W}^{Comp}\) is a \(D \times N\) matrix with elements \(\mathbf{W}^{Comp}_{dn}=\mathbf{W}_{dn}/\mathbf{W}^{Tot}_{n}\). Note that with these definitions \(\mathbf{W}\) can be written as the element-wise combination of scale and composition: \(\mathbf{W}_{dn}=\mathbf{W}^{Comp}_{dn}\mathbf{W}^{Tot}_{n}\), or as the logarithm \(\log \mathbf{W}_{dn}= \log \mathbf{W}^{Comp}_{dn} + \log \mathbf{W}^{Tot}_{n}\).

Many of  the normalizations used in tools such as DESeq2 [@Love:2014aa], edgeR [@Robinson:2010a], metagenomeSeq [@Paulson:2013aa] ALDEx2 [@fernandes:2014]  can be stated as ratios of the form \(\hat{{\mathbf{W}}}_{dn} \approx \mathbf{Y}_{dn}/f(\mathbf{Y})\), where the denominator is determined by some function of the observation. We use the hat  notation (\(\ \hat{}\ \)) to indicate that the output is an estimate of the true value.  The technical variation in sequencing depth, which is often called "library size" has no relationship with the actual number of molecules in the sampled environment [@lovell:2011]. In other words (\(\mathbf{Y}^{Tot}_{n}=\sum_{d}\mathbf{Y}_{dn}\))  the observed data \(\mathbf{Y}\) provides us with information about the system composition \(\mathbf{W}^{Comp}\) but little to no information in the system scale \(\mathbf{W}^{Tot}\) (Lovell et al. 2011).

## Adding Scale Uncertainty in ALDEx2

The ALDEx2 R package [@fernandes:2013;@fernandes:2014] is a general purpose toolbox to  model the uncertainty of HTS data and to use that model to estimate the significance of the underlying LFC (log-fold change).  At a high-level, ALDEx2  has three connected components to estimate the uncertainty inherent in HTS datasets. First, the tool accounts for the uncertainty of the sequencing counts using Dirichlet multinomial sampling to build a probabilistic model of the data; i.e., \(\mathbf{\hat{W}}^{Comp} \approx \mathrm{Dir}(\mathbf{Y})\).  Secondly, ALDEx2 uses the centred log-ratio transformation to scale the data [@fernandes:2013]. It was this step that was modified  to account for scale uncertainty and misspecification [@nixon2024scale,@Nixon2024B] explained  with more details in [@nixon2024scale;@Nixon2024B] and summarized in the next paragraph. Finally, a standard null-hypothesis test and a non-parametric estimate of mean standardized difference are used to report on the finite sample variation. These sources of uncertainty and variation are combined via reporting the expected values from a Monte-Carlo simulation framework. For simplicity, we use the term 'difference' to refer to the absolute difference between groups, and 'dispersion' to refer to the  within-condition difference or pooled variance as defined in [@fernandes:2013]. These are calculated on a \(\log_2\) scale. For more details on ALDEx2 see [@fernandes:2013;@fernandes:2014;@nixon2024scale;@Nixon2024B].

Scale models were incorporated into ALDEx2, turning the ALDEx2 model into a specialized type of statistical model which Nixon et al. [@nixon2024scale] term a \textit{Scale Simulation Random   Variable} (SSRV). To do this, Nixon et al. [@nixon2024scale] generalized the concept of normalizations by introducing  the concept of a \textit{scale model} to account for potential error in the centred log-ratio normalization step.  They did this by including a model for \(\mathbf{\hat{W}}^{Tot}_{n}\). The CLR normalization used by ALDEx2 makes the assumption \(\mathbf{\hat{W}}^{Tot}_{n}=1/G_{n}\), where \(G_n\) is the geometric mean of the counts (or the corresponding proportions)of each part in sample n, which while being a random variable, is essentially constant across each Monte-Carlo replicate, but that differs between samples. With this modification, ALDEx2 can be generalized by considering probability models for the scale \(\mathbf{\hat{W}}^{Tot}_{n}\) that have mean \(1/G_{n}\). For example, the following scale model generalizes the CLR:

\[\log \mathbf{\hat{W}}^{Tot}_{n} = -\log G_{n} + \Lambda x_{n} \qquad \Lambda \sim N(\mu, \gamma^{2}).\]

This formulation is quite flexible [@nixon2024scale;@Nixon2024B]. In the simple or 'default' configuration,   \(\mu = 0\) and  \(\gamma\) is a tunable parameter drawn from a log-Normal distribution [@nixon2024scale]. Adding scale uncertainty with the \(\gamma\) paramenter (as shown in Figure 1:1) controls only the degree of uncertainty of the CLR assumption for the \(x_{n}\) binary condition indicator (e.g., \(x_{n}=1\) denotes case and \(x_{n}=0\) denotes control). In the advanced or 'informed' configuration, \(\mu\) takes different values for each group and  controls the location of the LFC assumption (as shown in Figure 1:2); combining \(\mu\) with a \(\gamma\) estimate allows for uncertainty in both the location and the scale. In the manuscript where the idea of scale was original derived, Nixon et al. [@nixon2024scale] conducted extensive simulations showing that both the default and informed approaches exhibit   increased sensitivity and specificity [@Nixon2024B]. In this report we show that these approaches also work well to control the FDR in transcriptome and metatranscriptome datasets. We also provide some additional insights into how this is achieved. These modifications are  instantiated in ALDEx2 which is  the first software package designed for SSRV-based inference.

# Results

## Adding scale uncertainty replaces the need for dual significance cutoffs.

Gierliński et al. [@Gierlinski:2015aa]  generated a  highly replicated yeast transcriptome dataset to compare gene expression between a wild-type strain and a snf2 gene knockout, \(\Delta\)snf2. This dataset of 86 samples (44 and 42 per group) is an example of technical growth replicate experiments common in the literature. The dataset was  used to test several RNA-seq tools for their power  to detect the set of differentially abundant transcripts identified in the full dataset when the data was subset [@Schurch:2016aa]. In this original study each tool had its own 'gold standard' set of transcripts with different tools identifying between between 65% to >80% of all transcripts as being significantly different. Since the majority of transcripts were significantly different, the authors suggested that it was more appropriate to apply a  dual cutoff composed of both a Benjamini-Hochberg [@benjamini:1995] corrected p-value (q-value) plus a difference cutoff  to limit the number of identified transcripts to a much smaller fraction of the total. In other datasets, Nixon et al, @Nixon2024B showed that adding even a small amount of  scale uncertainty with ALDEx2  dramatically reduced the number of significant transcripts identified, removing the need for the dual cutoff approach in this dataset and others.

```{r yst-res, echo=F, warning=F, message=F,comment=F,} 
#ALDEx2 
library(ALDEx2, warn.conflicts=F) 
load(file="analysis/yst.all.Rda") 
load(file="analysis/yst.s.all.Rda") 
load(file="analysis/yst.1.all.Rda") # DESeq2 
load('analysis/res.Rda')

# with gamma = 0.5 
load('analysis/x.s.all.Rda') 
sig.des <- which(res@listData$padj < 0.05) 
sig.ald <- which(yst.all$we.eBH < 0.05) 
sig.s.ald <- which(yst.s.all$we.eBH < 0.05)

sig.all <- yst.all$we.eBH < 0.05 
sig.t <- yst.all$we.eBH < 0.05 & abs(yst.all$diff.btw) > 1.4 
sig.s <- yst.s.all$we.eBH < 0.05 
sig.1 <- yst.1.all$we.eBH < 0.05 
```

```{r yst-plot, echo=FALSE, warning=FALSE, fig.align="center", out.width="0.8\\linewidth", fig.dim=c(6,7), fig.cap="Volcano and effect plots for unscaled and scaled transcriptome analysis. ALDEx2 was used to conduct a differential expression (DE) analysis on the yeast transcriptome dataset. The results were plotted to show the relationship between difference and dispersion using effect plots or difference and the q-values using volcano plots. Panels A,C are for the naive analyses, and Panels B,D are for the  default analyses that include scale uncertainty. Each point represents the values for one transcript, with the color indicating if that transcript was significant in the both analyses (red) or in the naive analysis only (orange). Points in grey are not statistically signficantly different under any condition. The horizontal dashed lines represent a \\(\\log_2\\)difference of \\(\\pm 1.4\\)."}

load(file="analysis/yst.all.Rda") 
load(file="analysis/yst.s.all.Rda") 
load(file="analysis/yst.1.all.Rda") # DESeq2 
load('analysis/res.Rda')


# with gamma = 0.5 
load('analysis/x.s.all.Rda') 
sig.des <- which(res@listData$padj < 0.05) 
sig.ald <- which(yst.all$we.eBH < 0.05) 
sig.s.ald <- which(yst.s.all$we.eBH < 0.05)

sig.all <- yst.all$we.eBH < 0.05 
sig.t <- yst.all$we.eBH < 0.05 & abs(yst.all$diff.btw) > 1.4 
sig.s <- yst.s.all$we.eBH < 0.05 
sig.1 <- yst.1.all$we.eBH < 0.05 


par(mfrow=c(2,2)) 
# volcano 
# plot(res@listData$log2FoldChange,-1*log10(res@listData$padj + 1e-300), 
#  col=rgb(0,0,0,0.1), xlab='log2 Difference', ylab='-1 log10(p.adjust)')
#   
# title('D: DESeq2 volcano', adj=0, line= 0.8) 
# 
# points(res@listData$log2FoldChange[sig.des],-1*log10(res@listData$padj[sig.des] + 1e-300), 
#   col=rgb(1,.66,0,0.3), pch=19, cex=0.5) 
# points(res@listData$log2FoldChange[sig.s.ald],-1*log10(res@listData$padj[sig.s.ald] + 1e-300),     
#   col=rgb(1,0,0,1), pch=19, cex=0.5) 
#  
# abline(v=c(-1.4,1.4), lty=2,lwd=2, col='grey')

l10q <- expression(-log[10](q)) 
l2d <- expression(log[2]~ Difference)

plot(yst.all$diff.btw, -1*log10(yst.all$we.eBH +1e-70), col=rgb(0,0,0,0.1), xlab=l2d, ylab=l10q) 
title(expression(paste("A: Volcano: ", gamma, " = 0")), adj=0, line= 0.8) 
points(yst.all$diff.btw[sig.ald], -1*log10(yst.all$we.eBH[sig.ald]  +1e-70), col=rgb(1,.66,0,.3), cex=0.5, pch=19) 
points(yst.all$diff.btw[sig.s.ald], -1*log10(yst.all$we.eBH[sig.s.ald]  +1e-70), col=rgb(1,0,0,1), cex=0.5, pch=19) 
abline(v=c(-1.4,1.4), lty=2,lwd=2, col='grey')


plot(yst.s.all$diff.btw, -1*log10(yst.s.all$we.eBH +1e-15), col=rgb(0,0,0,0.1), xlab=l2d, ylab=l10q) 
title(expression(paste("B: Volcano: ", gamma, " = 0.5")), adj=0, line= 0.8) 
points(yst.s.all$diff.btw[sig.s.ald], -1*log10(yst.s.all$we.eBH[sig.s.ald]+1e-15), col=rgb(1,0,0,1), cex=0.5, pch=19) 
abline(v=c(-1.4,1.4), lty=2,lwd=2, col='grey')

#plot(res@listData$lfcSE*sqrt(ncol(yst)), res@listData$log2FoldChange, #xlim=c(0,5), col=rgb(0,0,0,0.1), xlab='LFC SD', ylab='log2 Difference') 
#title('A: DESeq2 effect', adj=0, line= 0.8) 
#points(res@listData$lfcSE[sig.des]*sqrt(ncol(yst)), 
#res@listData$log2FoldChange[sig.des], col=rgb(1,.66,0,0.5), pch=19, cex=0.5) #points(res@listData$lfcSE[sig.s.ald]*sqrt(ncol(yst)), 
#res@listData$log2FoldChange[sig.s.ald], col=rgb(1,0,0,0.5), pch=19, cex=0.5) #abline(h=c(-1.4,1.4), lty=2,lwd=2, col='grey')

plot(yst.all$diff.win, yst.all$diff.btw, col=rgb(0,0,0,0.1), xlab='Dispersion', ylab=l2d) 
title(expression(paste("C: Effect: ", gamma, " = 0")), adj=0, line= 0.8) 
points(yst.all$diff.win[sig.ald], yst.all$diff.btw[sig.ald], col=rgb(1,.66,0,0.3), cex=0.5, pch=19) 
abline(h=c(-1.4,1.4), lty=2,lwd=2, col='grey') 
points(yst.all$diff.win[sig.s.ald], yst.all$diff.btw[sig.s.ald], col=rgb(1,0,0,0.6), cex=0.5, pch=19) 
abline(h=c(-1.4,1.4), lty=2,lwd=2, col='grey')

plot(yst.s.all$diff.win, yst.s.all$diff.btw, col=rgb(0,0,0,0.1), xlab='Dispersion', ylab=l2d, xlim=c(0.1,5)) 
title(expression(paste("D: Effect: ", gamma, " = 0.5")), adj=0, line= 0.8) 
points(yst.s.all$diff.win[sig.s.ald], yst.s.all$diff.btw[sig.s.ald], col=rgb(1,0,0,0.5), cex=0.5, pch=19) 
abline(h=c(-1.4,1.4), lty=2,lwd=2, col='grey')

```

We start with the assumption that not all statistically significant differences are biologically relevant [@efron2008FDR], and that a result where the majority of transcripts are significant breaks the  necessary  assumption for DA/DE expression that most parts be invariant [@Dillies:2013]. Transcriptomic analysis commonly uses a dual cutoff approach graphically exemplified by volcano plots [@Cui:2003aa;@Schurch:2016aa]. Using either DESeq2 or ALDEx2, a majority of transcripts are statistically significantly different between groups  with a  q-value cutoff of \( \le 0.05\); i.e. `r length(sig.des)` (`r round(length(sig.des)/nrow(yst.all), 2)*100`%, DESeq2) or `r length(sig.ald)` (`r round(length(sig.ald)/nrow(yst.all), 2)*100`%, ALDEx2) of the `r nrow(yst.all)` transcripts. These values are in line with those observed by [@Schurch:2016aa]. Such  large numbers of statistically significant transcripts seems biologically unrealistic. That `r length(setdiff(sig.ald, sig.des))` transcripts are identified by ALDEx2 and not DESeq2, while DESeq2 identifies `r length(setdiff(sig.des, sig.ald))` transcripts that ALDEx2 does not, suggests that the choice of normalization plays a role in which results are returned as significant and that some, if not the majority, are driven by technical differences in the analysis [@Dillies:2013;@Li:2022aa] or are false positives. 

The Volcano plots in Figure  2 A and B show  that adding scale uncertainty increases the minimum q-value and increases the concordance between the q-value and the difference between groups (compare panels A and B). The effect plots [@gloor:effect] in Figure 2C  shows that the majority of significant transcripts (red, orange) have negligible differences between groups and  very low dispersion. We suggest that this low dispersion is driven by the experimental design which is actually a technical wet lab replication  rather than a true biological replication design [@Gierlinski:2015aa]. Scale uncertainty can be incorporated using the \texttt{gamma} parameter that controls the amount of noise added to the CLR mean assumption when we call either \texttt{aldex()}, or \texttt{aldex.clr()}. Figure 2 B,D shows that  setting  \( \gamma=0.5 \) now results in `r length(sig.s.ald)` which is far fewer statistically significant transcripts than in the naive analysis  and we observe that  the minimum dispersion increases from  `r round(min(yst.all$diff.win), 2)` (\(\gamma = 0\) ) to  `r round(min(yst.s.all$diff.win), 2)` (\(\gamma=0.5\)).

It is common practice to use a dual-cutoff by choosing transcripts based on a thresholds for both q-values and fold-changes [@Schurch:2016aa], and these were first proposed for microarray experiments through volcano plots [@Cui:2003aa]. Note that there is considerable variation in recommended cutoff values [@Schurch:2016aa], and that this controversy has persisted ever since fold-change was suggested [@witten2007comparison]. Unfortunately, universal cutoff fold-change values cannot be identified in part because different tools have intrinsically different variance in their log_2-fold change ranges [@Liu:2022aa]. This has led to the widespread practice of applying a post-hoc fold-change cutoff to reduce the number of positive identifications to a manageable proportion of the whole dataset. Here, we applied the dual-cutoff method using a fold-change of at least a \(2^{1.4}\) fold change that reduces the number of significant outputs to 193 for DESeq2 and to 186 for ALDEx2. This cutoff was chosen for convenience and is mid-way between the high and low fold-change recommendations of [@Schurch:2016aa]. <!-- there is no clear guidance provided about the appropriate cutoffs in the literature. --> These limits are shown by the dashed grey lines in Figure 2 and we can see that a \(2^{1.4}\) fold change \((\sim 2.6\) fold) cutoff identifies a similar number of transcripts as does  ALDEx2 using \( \gamma = 0.5 \) which identifies `r length(sig.s.ald)` transcripts.

Supplementary Figures 1 shows an example of the \texttt{aldex.senAnalysis()} function to identify those transcripts that are very sensitive to scale uncertainty in this dataset. Here we see that adding a very small amount of scale \(\gamma = 0.1 \) reduces the number of significant transcripts by more than half in the yeast dataset. This allows the analyst to ignore those low-dispersion transcripts that were  significant only because of an absence of scale uncertainty. 

We next examined how adding scale would alter the analysis in a real dataset to which synthetically generated true positive counts had been added. We show results from the  anti-PD-1 therapy RNA-seq dataset [@Riaz:2017aa] which examined  changes in gene expression when cells were exposed or not to a cell-cycle checkpoint inhibitor. This dataset was used by Li et al. [@Li:2022aa] as an example of the dangers of relying on tools with high false positive error rates when analyzing clinical or clinically-related transcriptome samples. Indeed, in the benchmarking analysis done by this group, they found that  parameter based methods such as DESeq2 and edgeR that on reported p-value and fold-change cutoffs often led to conclusions in the original dataset that were indistinguishable from permutations of the dataset. In other words, that the analysis of transcriptome datasets from patient-derived samples often exhibited many false-positive identifications.

```{r immuno, echo=F, warning=F, message=F, include=TRUE, fig.dim=c(8,8),  fig.align="center", out.width="0.8\\linewidth",  fig.cap=c('Results of modeling true positive (TP) differences in the PD-1 dataset. For this the data were shuffled and  5% of the transcripts were modeled to have TP differences between groups where the differences were drawn from a Normal distribution with a mean difference of 0 and a sd of 2. Panel A shows the mean FDR of 10 instances for scale-naive ALDEx2 (A 0), and ALDEx2 with \\(\\gamma=0.2\\) (A 0.2) or \\(\\gamma=0.2\\) (A 0.5), and where significant features were identified by  DESeq2 without (D 0), or with (D 0.5) a 0.5-fold change difference. The x-axis shows how the FDR changes for each tool as a function of the estimated modeled difference between groups. Panel B shows the mean sensitivity (TP found /all TP). Panels C and D show volcano and effect plots for the ALDEx2 output with the transcripts identified as significant at each scale setting as colored points, and non-significant transcripts in black.')}

# this contains the code to run the simulation
# and to process the output of the simulation
# the simulation is only run is the output does not
# exist in the analysis folder
# look for thin_sim_data.out.Rda

source("code/immuno.R") 

# this code block is for plotting the data only

par(mfrow=c(2,2))
plot(means$coef[ald.row], means$FDR[ald.row], ylim=c(0,0.6), type='b',lwd=3,
  xlab="modeled minimum difference", ylab="mean FDR", col='blue')
title("A: FDR", adj=0, line= 0.8) 
#points(means.wi$coef[ald.row], means.wi$FDR[ald.row],  type='l', lwd=3,  col='cyan', lty=3)
points(means$coef[ald2.row], means$FDR[ald2.row],  type='b', lwd=3, 
  col='grey')
#points(means.wi$coef[ald2.row], means.wi$FDR[ald2.row],  type='l', lwd=3, col='cyan', lty=3)
points(means$coef[ald5.row], means$FDR[ald5.row],  type='b', lwd=3, 
  col='grey50')
#points(means.wi$coef[ald5.row], means.wi$FDR[ald5.row],  type='l', lwd=3, col='cyan', lty=3)
points(means$coef[des.row], means$FDR[des.row],  type='b', lwd=3, col='red')
points(means$coef[des5.row], means$FDR[des5.row],  type='b', lwd=3,
  col='darkred')
legend(0,0.3, legend=c('A 0', 'A 0.2', 'A 0.5'), pch=19,
  col=c('blue', 'grey', 'grey50'), bty="n", cex=1.2) 
legend(0.25,0.3, legend=c('D 0', 'D 0.5'), pch=19,
  col=c('red', 'darkred'), bty="n", cex=1.2) 
rect(xleft=0, xright=0.55,  ybottom=0.1, ytop=0.3)

plot(means$coef[ald.row], means$SEN[ald.row], ylim=c(0,1), lwd=3, type='b',
  xlab="modeled minimum difference", ylab="mean Sensitivity", col='blue')
title("B: Sensitivity", adj=0, line= 0.8) 
#points(means.wi$coef[ald.row], means.wi$SEN[ald.row],  lwd=3, type='l', col='cyan', lty=3)
points(means$coef[ald2.row], means$SEN[ald2.row],  lwd=3, type='b', col='grey')
#points(means.wi$coef[ald2.row], means.wi$SEN[ald2.row],  lwd=3, type='l', lty=3, col='cyan')
#points(means.wi$coef[ald5.row], means.wi$SEN[ald5.row],  lwd=3, type='l', col='cyan', lty=3)
points(means$coef[ald5.row], means$SEN[ald5.row],  lwd=3, type='b', col='grey50')
points(means$coef[des.row], means$SEN[des.row],  lwd=3, type='b', col='red')
points(means$coef[des5.row], means$SEN[des5.row], lwd=3,  type='b', col='darkred')

addmin <- min(data.out[[10]]$ald0$we.eBH[data.out[[10]]$ald0$we.eBH > 0])/10
# row 2677
aldex.plot(data.out[[10]]$ald0, type='volcano', main="")
title("C: Volcano", adj=0, line= 0.8) 
points(data.out[[10]]$ald0$diff.btw[which(data.out[[10]]$ald2$we.eBH < 0.05)], 
  -log10(data.out[[10]]$ald0$we.eBH[which(data.out[[10]]$ald2$we.eBH < 0.05)]+ addmin), cex=0.6, pch=19, 
  col='orange')
points(data.out[[10]]$ald0$diff.btw[which(data.out[[10]]$ald5$we.eBH < 0.05)], 
  -log10(data.out[[10]]$ald0$we.eBH[which(data.out[[10]]$ald5$we.eBH < 0.05)]+ addmin), col='blue', 
  cex=0.6, pch=19)
points(data.out[[10]]$ald0[2677,"diff.btw"], 
  -log10(data.out[[10]]$ald0[2677,"we.eBH"]+ addmin), col='orange', cex=1.5)

abline(v=c(-0.5,0.5), lty=2, col='darkgrey')
abline(v=c(-1.5,1.5), lty=2, col='darkgrey')

gam0 <- expression(paste(gamma,"=0"))
gam2 <- expression(paste(gamma,"=0.2"))
gam5 <- expression(paste(gamma,"=0.5"))
legend(2, 60, legend=c("NS", gam0, gam2, gam5), pch=19, col=c("black", "red", "orange", "blue"), cex=1.2)


aldex.plot(data.out[[10]]$ald0, type='MW')
title("D: Effect", adj=0, line= 0.8) 
points(data.out[[10]]$ald0$diff.win[which(data.out[[10]]$ald2$we.eBH < 0.05)], 
  (data.out[[10]]$ald0$diff.btw[which(data.out[[10]]$ald2$we.eBH < 0.05)]), cex=0.6, pch=19, 
  col='orange')
points(data.out[[10]]$ald0$diff.win[which(data.out[[10]]$ald5$we.eBH < 0.05)], 
  (data.out[[10]]$ald0$diff.btw[which(data.out[[10]]$ald5$we.eBH < 0.05)]), col='blue', 
  cex=0.6, pch=19)
points(data.out[[10]]$ald0[2677,"diff.win"], 
  (data.out[[10]]$ald0[2677,"diff.btw"]), col='orange', cex=1.5)

abline(v=c(-0.5,0.5), lty=2, col='darkgrey')
abline(v=c(-1.5,1.5), lty=2, col='darkgrey')
                    
```

Figure 3 shows the results of ten permutations of this dataset while simulating 5% of the transcripts to be true positives where the difference from no change was derived from a Normal distribution [@Gerard:2020aa]. This simulation was conducted with the `seqgendiff` R package [@Gerard:2020aa] to both permute the dataset and to add true positive features. We did ten permutations and kept track of the number of true and false positive identifications. Figure 3A shows the actual false discovery rate for ALDEx2 with and without the addition of scale uncertainty and for DESeq2 with or without a fold-change cutoff of \(2^{0.5}\). When the modeled difference was greater than 0, ALDEx2 exhibited a FDR of \(\gamma = 0\): `r round(means$FDR[ald.row[1]],3)`,  \(\gamma = 0.2\): `r round(means$FDR[ald2.row[1]],3)`, and  \(\gamma = 0.5\): `r round(means$FDR[ald5.row[1]],3)`, while DESeq2 with no fold-change cutoff had an FDR of `r round(means$FDR[des.row[1]],2)` and with a 0.5 fold-change cutoff an FDR of `r round(means$FDR[des5.row[1]],2)`. Figure 3 also plots these results as a function of the minimum modelled fold change of the true positive transcripts. First, we can see that this analysis recapitulates the observations of Li et al [@Li:2022aa] in that DESeq2 has very poor false positive control at a nominal FDR of 0.05. The FDR is not controlled any better when a fold-change cutoff is applied, and this agrees with previous work showing that fold-change cutoffs do not materially improve FDR control in high throughput datasets [@Zhang:2009aa;@Ebrahimpoor:2021aa]. Figure 3B shows that DESeq2 has higher sensitivity than does ALDEx2, and not surprisingly this sensitivity increases as the difference between groups increases. For the case where the mean difference is 0 or greater, ALDEx2 exhibited a sensitivity of \(\gamma = 0\): `r round(means$SEN[ald.row[1]],2)`,  \(\gamma = 0.2\): `r round(means$SEN[ald2.row[1]],2)`, and  \(\gamma = 0.5\): `r round(means$SEN[ald5.row[1]],2)`, while DESeq2 with no fold-change cutoff had a sensitivity of `r round(means$SEN[des.row[1]],2)` and with a 0.5 fold-change cutoff a sensitivity of `r round(means$SEN[des5.row[1]],2)`. In this example, scale-naive ALDEx2 has near perfect FDR control and reasonable sensitivity at low modeled difference between groups. However, when the modeled difference between groups becomes large, then the scale-naive version of ALDEx2 begins to exhibit unacceptable rates of false positives and the false positive rate for DESeq2 also increases. Adding in even small amounts of scale uncertainty drops the true FDR rate to 0, but at the expense of sensitivity. The major contributor to the increase in FDR with larger modeled differences is that the tools are identifying as positives those transcripts that are modeled to have differences just below the threshold. We need to recognize that there is no such thing as a statistical free lunch; the analyst can have high sensitivity but low confidence that any individual transcript is truly different, or have lower sensitivity but have very high confidence that the difference is real.  In other words, the sensitivity of a method is directly tied to how much error the investigator is willing to tolerate. 

Examination of the volcano plot [@Cui:2003aa] and effect plot [@gloor:effect] in Figure 3C and D provides some insight into why adding scale uncertainty provides better FDR control than does the approach of using a p-value and a fold-change cutoff. In the volcano plot, adding scale uncertainty differentially excludes transcripts with a combination of marginal p-values and low difference between groups, and that this becomes more pronounced with a larger scale value. This effect is also seen in Figure 2 but is more nuanced. In contrast, the fold-change cutoff does not incorporate the magnitude of the p-value and so transcripts with large differences, but marginal p-values are retained. The effect plot in Figure 3D shows that the transcripts with marginal p values and large differences that are excluded when scale uncertainty is added are those that have  a large dispersion. 

As a concrete example consider the point that is circled in panels C and D with a marginal p-value, with a difference between of nearly 4 and a dispersion of greater than 6. This transcript is no longer significant when \(\gamma = 0.2\), but would require a very large fold-change cutoff to be excluded by the standard approach. In addition,  transcripts with very small dispersion and very small differences are also excluded when scale uncertainty is added. Thus, the addition of scale uncertainty achieves the desired outcome of lowering the FDR for those transcripts that are either marginally differentially abundant, or where the underlying dispersion--and hence the uncertainty in measurement--is very high, or for transcripts that fit both criteria.

Supplementary Figure 2 shows a second permuted  dataset with the addition of modeled differences between groups. Here we used another real dataset with over 200 biological replicates of BRCA1 tumor and control tissue samples from Li et al. [@Li:2022aa]. This Supplementary Figure shows that the FDR control of DESeq2 is somewhat better than in the PD-1 dataset, although still much greater than the the anticipated 5%. Further, a fold-change cutoff reduces the FDR of DESeq2 from about 30% to just over 20% with a power of  over 80%. However, we can see that scale-naive ALDEx2 performs substantially better with a negligible FDR and comparable power. Adding scale uncertainty again improves FDR even for those transcripts modeled to have larger differences and the power is substantially better than in the PD-1 dataset, reaching the same power as DESeq2 or scale-naive ALDEx2 when the modeled difference is large. As before, this is driven by removing from consideration those transcripts with either a small difference between or a marginal p-value, or both.
 
 Supplementary Figures 3  shows that effect of applying \(\gamma = 0.5 \) to this dataset results in reducing the number of positive transcripts from being \(\sim 70\)\% of the whole dataset to less than 10\% of the dataset and that this is largely because of a reduction in significance of those transcripts with low dispersion.  Supplementary Figures 4 shows a sensitivity analysis of the BRCA1 dataset showing that different scale uncertainty amounts alter the number of significant transcripts in a biological replicate experiment similarly to a technical replicate experiment. Supplementary Figures 5 and 6 delve into how adding scale uncertainty affects the variace-abundance relationship in subtle ways, and may help readers to understand the observations seen in Figure 3 and supplementary Figure 2. 

Together the results in this section show that adding scale uncertainty has the desirable effect of altering the transcripts identified as significantly different between groups in a way that exhibits better control of FDR albeit with a corresponding reduction in sensitivity. Those parts that were statistically significantly different *only because of low dispersion* or that *had marginal p-values* or both, are now preferentially excluded from statistical significance. In practice, we suggest that a \texttt{gamma} parameter of between 0.2 and 0.5 is realistic for most experimental designs [@Nixon2024B] regardless if the replication is technical or biological. 


<!-- Interestingly, the yeast transcriptome experiment has an unacknowledged difference in scale between conditions; yeast deficient for snf1 are smaller, grow more slowly and are sensitive to a variety of common agents that cause cell stress [@Yoshikawa:2011aa]. The CLR normalization assumes that the scale of the wild-type is exactly 98.4\% of the snf1 strain, and so is \( \theta = 0.016\). Given the per-generation time difference of about 15\%, this seems implausible. Setting \(\gamma=0.5\) can be interpreted as acknowledging uncertainty in the difference in the underlaying scale between groups. Nixon et al. -@Nixon2024B showed that the 95\% bounds on the scale uncertainty for a given \(\gamma\) value, say 0.5, can be calculated as \(2^{-(2 \gamma + \theta)}, 2^{(2 \gamma + \theta)}\), or \(2^{-(2*0.5 + 0.016)}, 2^{(2*0.5 + 0.016)} = 0.494,2.022\). Thus, by adding scale uncertainty, we are acknowledging that the data are uncertain and that any results are robust to that uncertainty, within the bounds of our model. -->

## Housekeeping genes and functions to guide scale model choices.

Dos Santos et al. [@dosSantos:2024]  used a vaginal metatranscriptome dataset  to compare the gene expression in bacteria collected from healthy (H) and bacterial vaginosis (BV) affected women. This dataset is derived from two publicly available datasets composed of a set of 20 non-pregnant women from London, Ontario Canada [@Macklaim:2018aa] and a subset of 22 non-pregnant women collected from German women who underwent metronidazole treatment for BV [@Denge00262-18]. Batch effects for these two groups were removed with ComBat-seq [@Zhang:2020ab] and the two datasets were merged into one, giving a total of 16 H and 26 BV samples. In the Dos Santos paper, all results from this initial analysis were replicated in a much larger dataset derived from the MOMS-PI study [@Fettweis:2019aa]

In this vaginal environment, both the relative abundance of species between groups and the gene expression level within a species is different [@macklaim:2013]. Additionally, prior research suggests that the total number of bacteria is about 10 times more in the BV than in the H condition [@Zozaya:2010]. Thus, these are extremely challenging datasets in which to determine differential abundance as there are both compositional and scale changes between conditions. The usual method to analyze vaginal metratranscriptome data is to do so on an organism-by-organism basis [@macklaim:2013; @Denge00262-18; @Fettweis:2019aa] because the scale confounding of the environment is less pronounced. One attempt at system-wide analysis returned several housekeeping functions as differentially expressed between groups [@Denge00262-18]; a result likely due to a disconnect between the assumptions of the normalization used and the actual scale of the environment [@Wu2021].

```{r ribo, echo=F} 
# list of ribosomal associated functions in the datset TIL about 
# complete.cases() ribo.v <-c("K02863","K02864","K02867","K02871","K02874","K02876","K02878","K02879"," K02881","K02884","K02886","K02887","K02888","K02890","K02892","K02895","K02897", "K02899","K02902","K02904","K02906","K02907","K02909","K02911","K02913","K02914" ,"K02916","K02926","K02931","K02933","K02935","K02939","K02945","K02946","K02948 ","K02950","K02952","K02954","K02956","K02959","K02961","K02963","K02965"," K02967","K02968","K02970","K02982","K02986","K02988","K02990","K02992","K02994", "K02996")

```

```{r meta, echo=F, warning=F, message=F,comment=F, result=F, fig.cap="Analysis of vaginal transcriptome data aggregated at the Kegg Orthology (KO) functional level. Panel A shows an effect plot for the default analysis where the functions that are elevated in  the  healthy individuals have positive values and functions that are elevated in BV have negative values. Highlighed in the box are KOs that are almost exlusively housekeeping functions; these and are colored cyan. These housekeeping functions should be located on the midline of no difference.  Panel B shows the same data scaled with \\(\\gamma = 0.5\\), which increase the minimum dispersion as before.  Panel C shows the same data scaled with \\(\\gamma = 0.5\\) and a 0.14 fold difference in dispersion applied to the BV samples relative to the H samples. In these plots statistically significant (q-value < 0.01) functions in the informed model are  in red, false positive functions are in blue,  non-significant functions in black and false negative functions are in orange. " }

# all code for ALDEx2 and DESeq is being moved to the code/directory all 
# variables recreated using the makefile from code/meta_aldex.R

l10q <- expression(-log[10](q)) 
l2d <- expression(log[2]~ Difference)

load('analysis/xt.Rda') 
load('analysis/xg.Rda') 
load('analysis/xt.m.Rda') 
load('analysis/xt.lv.Rda') 
load('analysis/xg.clr.Rda')

hk <- rownames(xt.all)[xt.all$diff.win < 2.5 & xt.all$diff.btw > 1 & xt.all$diff.btw < 3] 
hk.off <- xt.all$diff.win < 2.5 & xt.all$diff.btw > 1 & xt.all$diff.btw < 3

fn <- rownames(xt.m.all)[xt.m.all$we.eBH < 0.01 & xt.m.all$diff.btw < 1 & xt.all$we.eBH > 0.01] 
fp <- rownames(xt.all)[xt.all$we.eBH < 0.01 & xt.m.all$we.eBH > 0.01 & !xt.m.all$we.eBH < 0.01]

#par(mfrow=c(1,1)) 
#plot(density(xt.m.e$diff.btw[hk.off])) 
#abline(v=0, col='red')

par(mfrow=c(1,3)) 
aldex.plot(xt.all, cutoff.pval=0.01, xlim=c(0.3,9),xlab="Dispersion", ylab=l2d) 
points(xt.all[fp,'diff.win'],xt.all[fp,'diff.btw'], col='blue', cex=0.4, pch=19)

points(xt.all$diff.win[hk.off], xt.all$diff.btw[hk.off], pch=19, cex=0.4, col=rgb(0,1,1,0.5)) 
points(xt.all[fn,'diff.win'], xt.all[fn,'diff.btw'], col='orange', cex=0.4, pch=19) 
rect(0.5,1,2.5,3, col=rgb(0,0,0,0.1), lty=2, lwd=3) 
title('A: ALDEx2 unscaled', adj=0, line= 0.8) 
legend(0,12, legend=c("HK", "TP", "FP", "FN"), pch=19, cex=0.5, col=c("cyan", "red", "orange", "blue"))

aldex.plot(xg.all, xlim=c(0.3,9), cutoff.pval=0.01, xlab="Dispersion", ylab=l2d) 
points(xg.all[fp,'diff.win'], xg.all[fp,'diff.btw'], col='blue', cex=0.4, pch=19) 
points(xg.all$diff.win[hk.off], xg.all$diff.btw[hk.off], pch=19, cex=0.4, col=rgb(0,1,1,0.5)) 
points(xg.all[fn,'diff.win'], xg.all[fn,'diff.btw'], col='orange', cex=0.4, pch=19) 
title('B: ALDEx2 gamma', adj=0, line= 0.8)


aldex.plot(xt.m.all, xlim=c(0.3,9), cutoff.pval=0.01, xlab="Dispersion", ylab=l2d) 
points(xt.m.all[fp,'diff.win'], xt.m.all[fp,'diff.btw'], col='blue', cex=0.4, pch=19) 
points(xt.m.all$diff.win[hk.off], xt.m.all$diff.btw[hk.off], pch=19, cex=0.4, col=rgb(0,1,1,0.5)) 
points(xt.m.all[fn,'diff.win'], xt.m.all[fn,'diff.btw'], col='orange', cex=0.4, pch=19) 
title('C: ALDEx2 both', adj=0, line= 0.8)


```

In this example, we show how to specify and interpret a user defined or *informed* scale model that can explicitly account for some of these modeling difficulties [@Nixon2024B] even in a difficult to analyze dataset. An informed scale model can control for both the mean difference of scale between groups (e.g., directly incorporate information on the differences in total number of bacteria between the BV and H conditions) as well as the uncertainty of that difference as illustrated in Figure 1:3. To specify a user-defined scale model, we can pass a matrix of scale values instead of an estimate of just the scale uncertainty  to \texttt{aldex.clr()}. This matrix should have the same number of rows as the of Monte-Carlo Dirichlet samples, and the same number of columns as the number of samples. While this matrix can be computed from scratch by the analyst, there is an \texttt{aldex.makeScaleModel()} function that can be used to simplify this step in most cases. This encodes the scale model as \(\Lambda \sim N(log_2 \mu_n, \gamma^{2})\), where \(\mu_n\) represents the scale value for each sample or group and \(gamma\) is the uncertainty as before. The scale estimate can be a measured value (cell count, nucleic acid input, spike-ins, etc) or an estimate.  Nixon et al. [@nixon2024scale;@Nixon2024B] showed that only the ratio of the means are important when providing values for \( \mu_n\); i.e., the ratio between the \( \log_2 \mu_i\) and \(\log_2 \mu_j\) values. See the supplement to  Nixon et al. [@Nixon2024B] for more information.

Figure 4A shows an effect plot of the data where reads are grouped by homologous function regardless of the organism of origin. Each point represents one of 3728 KEGG  functions [@Okuda:2008]. There are many more functions represented in the BV group (bottom) than in the healthy group (top). This is because the \textit{Lactobacilli} that dominate a healthy vaginal microbiome have reduced genome content relative to the anaerobic organisms that dominate in BV, because there is a greater diversity of organisms in BV than in H samples, and because the BV condition has about  an order of magnitude more bacteria than does the H condition.

The naive scale model appears to be reflecting the bacterial load as observed by calculating the mean scale value for each group. Using a negligible scale value; i.e., \(\gamma=1e-3\) exposes the naive scale estimate for samples  in the \texttt{@scaleSamps} slot from the \texttt{aldex.clr} output. The naive scale estimate for the `r load('analysis/xg.clr.Rda')` healthy group is `r round(mean(rowMeans(xg@scaleSamps[c(1:8,37:44),])),2)` and for the BV group is  `r round(mean(rowMeans(xg@scaleSamps[c(9:36),])),2)` for a difference of `r round(mean(rowMeans(xg@scaleSamps[c(1:8,37:44),])) - mean(rowMeans(xg@scaleSamps[c(9:36),])),2)`. This is interpreted as the scale of the H group of samples being `r round(2^(mean(rowMeans(xg@scaleSamps[c(1:8,37:44),])) - mean(rowMeans(xg@scaleSamps[c(9:36),]))),2)` greater than the BV group. 

Applying the default scale model of \(\gamma=0.5\) increases the dispersion slightly but does not move the   housekeeping functions toward the midline. This is as expected; the mean of the default scale model is based on the CLR normalization so no shift in location would be expected over the scale-naive ALDEx2 model. Nevertheless, about 30\% of the housekeeping functions are no longer statistically significantly different. Note that this change is simple to conduct, has no additional computational complexity and requires only a slight modification for the analyst.


There are 101  functions with low dispersion that appear to be shared by both groups (boxed area in Figure 3A, and colored in cyan). Inspection shows that these   largely correspond to core metabolic functions such as transcription, translation, ribosomal functions, glycolysis, replication, chaperones, etc (Supplementary file housekeeping.txt).  The transcripts of many of these are commonly used as invariant reference sequences in wet lab experiments [@Rocha:2020aa] and so are not be expected   to contribute to differences in ecosystem behaviour. Because we expect housekeeping functions to be nearly invariant in their expression and to occur in all organisms, the average location of these should be centred on 0 difference to represent an internal reference set.  However, with the naive scale model, the mean  of these housekeeping functions is  approximately located at 2.3. Thus, we  desire a scale model that approximately centres the housekeeping functions and an appropriate scale in this dataset for functional analysis  will place these functions closer to 0 than does the naive estimate. One way to choose an appropriate value for \(\mu_n\) is to use the \texttt{aldex.clr} function on only the presumed invariant functions setting \(\gamma > 0\), and then accessing the \texttt{@scaleSamps} slot as before. Doing so suggests that the difference in scale should be about 14%.   A second approach would be to identify the functions used as the denominator with the \texttt{denom="lvha"} option [@Wu2021] for the \texttt{aldex.clr} function, and then to use these values as before. This approach suggests a 5% difference in scale, and is potentially less subject to user interpretation.

`r load('analysis/xt.m.Rda')`

For the purposes of this example, if we assume a 14% difference in scale, we can set \(\mu_i = 1\) and \(\mu_j = 1.14\) using the \texttt{makeScaleMatrix} function. This function uses a logNormal distribution to build a scale matrix given a user-specified mean difference between groups and uncertainty level. Applying a per-group relative differential scale of 0.14 moves the housekeeping functions close to the midline of no difference (Figure 3C, assuming 14% mean difference = `r round(mean(xt.m.all[hk.off,"diff.btw"]),2)`, assuming a 5% mean difference = `r round(mean(xt.lv.all[hk.off,"diff.btw"]),2)`), and applying a gamma of 0.5 provides the same dispersion as in panel B of Figure 3. Note that now a significant number of functions are differentially up in BV that were formerly classed as not different without the full scale model (orange), or when only a default scale was applied. Inspection of the functions shows that these are largely missing from the \emph{Lactobacillus} species and so should actually be captured as differentially abundant in the BV group. Supplementary Figure 7 shows that the using either the 5% or the 14%  scale difference  give imperceptibly different results suggesting that an informed scale model does not have to precisely estimate the scale difference to be useful. Nixon et al, @Nixon2024B also found that multiple reasonable estimates for the \(\mu_n\) part of the informed scale model were similarly useful in microbiome data.

Thus, applying an informed scale allows us to distinguish between both false positives (housekeeping functions in cyan, and others in blue) and false negatives (orange functions) even in a very difficult to analyze dataset. We used this scale model to uncover hiter-to-now unknown differences in microbiome functional activity between the Healthy and BV cohorts that were missed in previos analyses and that explain important clinical differences between them [@dosSantos:2024]. The remarkable improvements in biological interpretation afforded by an informed scale model, and the transferrability of it between sample cohorts of the same condition is outlined in dos Santos et al. [@dosSantos:2024]. We suggest that the default scale model  is sufficient when the data are approximately  centred but that an informed model is more appropriate with datasets are not well centred or when the investigator has prior information about the underlying biology.

# Discussion

Scale estimates affect two parts of the analysis. Modeling uncertainty in scale prevents false certainty in the precision of estimation and controls false positive identification. Modeling between-group scale  relaxes the assumption of identity between the sizes of the environments and allows better control of false  negative identification. The scale estimates can be derived from the total number of molecules in the environment or from other estimates  of input size (cell counts, initial concentrations, spike-ins, growth rates, etc).

Biological systems are both predictably variable and stochastic [@Taniguchi:2010aa] and systems biology experiments show that there are transcripts with approximately constant concentrations in the cell and those with large variability under different growth conditions [@Scott:2010].  Current measurement methods that rely on high throughput sequencing fail to capture all of the variation, particularly variation due to scale [@nixon2024scale;@Nixon2024B].  In the absence of external information [@Loven:2012aa;@Vandeputte:2017aa;@yeast-absolute] sequencing depth normalisation methods cannot recapture the scale information [@Loven:2012aa;@Lovell:2015], and can only normalize for the technical variation due to sequencing depth. Here we demonstrated that even approximate estimates of the true system scale and the uncertainty of measuring it can aid in the interpretation of RNA-sequencing experiments.

Nixon et al. [@nixon2024scale] introduced the idea of explicitly modeling the scale of a HTS dataset, and showed how to incorporate these models in the analysis of microbiome and other datasets [@Nixon2024B]. They  demonstrated that many tools commonly used to analyze HTS datasets had substantial Type 1 and Type 2 error rates in line with recent findings by others [@Quinn:2018aa;@Ge:2021aa;@Li:2022aa]. A version of ALDEx2 with the ability to include scale uncertainty was shown to be able to correct for  high Type 1 error rate for that tool, albeit with some loss of sensitivity. Finally, they showed that incorporating an informed scale model incorporating both location and scale uncertainty estimates could both control for Type 1 and Type 2 error rates [@Nixon2024B;@Dos-Santos:2024aa]. 

The process of choosing the parameters are experiment-specific and can be anchored in known information such as cell counts, spike-ins, information from the literature  or similar [@McGovern:2023;@Nixon2024B;@nixon2024scale]. In the metatranscriptome example used in this report [@Dos-Santos:2024aa;] the choice of parameter was driven by the assumption inherent in the biology that core housekeeping functions would serve as an appropriate standard [@Vandesompele:2002aa]. The choice of parameters must be guided by the experimental question  and other approaches in the literature which suggest normalizing the transcript levels to the bacterial metagenomic levels [@Zhang:2021ab] could be used to set the scale parameters, but in this case are  more granular at the individual organism level rather than at the systemic functional level. 

Building and using a scale model thus has substantial benefits relative to the dual cutoff approach that is advocated for many gene expression experiments [@Cui:2003aa;@Schurch:2016aa]. In particular, the dual cutoff approach has long been known to not control for Type 1 errors [@Zhang:2009aa;@Ebrahimpoor:2021aa], and the frequent lack of concordance between tools when benchmarked on transcriptomes [@Bullard:2010;@Soneson:2013;@Schurch:2016aa;@Quinn:2018aa;@Ge:2021aa;@Li:2022aa] and microbiomes [@McMurdie:2014a;@Thorsen:2016aa; @Weiss:2017aa; @hawinkel2017;@Nearing:2022aa;@Yerke:2024aa] suggests poor control of Type 2 errors as well [@Quinn:2018aa;@Li:2022aa]. Thus, incorporating a scale model during the analysis of HTS data promises the best of both worlds. A default scale model can control for Type 1 errors with minimal prior knowledge of the environment and this can be done with essentially no additional computational overhead. Furthermore, this work and previous [@Nixon2024B;@Dos-Santos:2024aa] show that even minimal information about the underlying environment can be used to build a relatively robust informed scale model that controls for both Type 1 and 2 error rates. It is important to note that the approach advocated here is distinct from that suggested by Zhang et al. [@Zhang:2021aa;@Zhang:2021ab] where the DNA amount for a gene is a covariate in the model for transcriptomic differential abundance. In our analysis we grouped all the transcript information to functional level regardless of organism, instead of modeling per-organism gene abundances. In the future we anticipate being able to build more complex models similar to those used by Zhang et al. with the additional information of uncertainty in the underlying gene count.

In the analysis of HTS data it is often observed that larger datasets converge on the majority of parts being significantly different [@Schurch:2016aa;@Li:2022aa;@nixon2024scale]. Li et al. [@Li:2022aa] conducted a permutation-based benchmarking study and found that widely used tools performed worse than simple Wilcoxon rank-sum tests coupled with the TPM normalization in controlling the FDR when sample sizes became large. Li et al. suggested that the presence of outliers were one of the factors driving the extreme FDR in some tests. We found that when the Wilcoxon test was used within the ALDEx2 framework that it had essentially the same outputs as did the t-test. For example, in the PD-1 dataset where \(\gamma = 0\)  the ALDEx2 t-test exhibited a mean FDR of `r round(means$FDR[ald.row[1]],3)*100`% and mean sensitivity of  `r round(means$SEN[ald.row[1]],3)*100`% while the corresponding values from the  ALDEx2 Wilcoxon test  were `r round(means.wi$FDR[ald.row[1]],3)*100`% and `r round(means.wi$SEN[ald.row[1]],3)*100`%. This result again supports that the assumptions of the normalizations are as important or more important than the statistical test.  Brooks et al. [@Brooks:2024aa] suggested that inappropriate choice of benchmarking methods are also a major contributing factor and that better objective standards of truth are needed. In this report we generated semi-synthetic test data used binomial thinning which produces data that more closely mimic the properties of real high throughput sequencing data, and so can more rigorously test different tools [@Gerard:2020aa]. From the perspective of our work the disagreement between tools can be explained by the observation that different analytic approaches  produce  different parameter estimates for either location or scale, or for both, as suggested in Figure 1. Thus, more data produces worse estimates because the additional data simply increases the precision of a flawed estimate [@gustafson2015bayesian;@nixon2024scale].

Scale simulation  is now built into ALDEx2 [@Nixon2024B] and in this report we suggest that  there are two main root causes to common HTS data pathologies. The first contributing factor is the observed very low dispersion estimate for many features that is a by-product of some experimental designs and  normalizations (Figure 2). Supplying additional uncertainty alleviates many FP but in a way that more appropriately controls the FDR as shown in Figure 3. The second contributing factor is unacknowledged asymmetry in many datasets [@Wu2021]; i.e., different gene content or a directional change in the majority of features. In the case of asymmetry, the use of a user-specified scale model can be very useful for otherwise difficult-to-analyze datasets such as meta-transcriptomes and in-vitro selection datasets where the majority of features can change as shown in Figure 4. We showed two ways of estimating the  scale difference between groups and found that any reasonable estimate is an improvement over the naive approach and also over the default scale model. This is in line with the observations by Nixon et al [@Nixon2024B] in a 16S rRNA gene sequencing dataset. While we acknowledge that some prior information is needed that this information is widely available and is already used when performing the gold-standard quantitative PCR test of differential abundance [@Thellin:1999aa;@SEQC/MAQC-III-Consortium:2014aa]. 

Beyond concerns of fidelity and rigor, scale models also enhance the reproducibility and transparency of HTS analyses. The development of HTS and the associated problems of very high dimensional data that was not always statistically well-behaved led to many different proposed solutions including multiple normalizations and moderated test statistics. That these perform poorly in real data is shown by the simulation data in this report and elsewhere  where both DESeq2 (which we used) and edgeR (which uses moderated statistical tests) performed poorly in controlling the FDR [@Li:2022aa]. While these approaches often work in many datasets  they fail to address the underlying problems of information that is missing in the data which is what is supplied by adding uncertainty around the information we have about that data. The addition of scale uncertainty directly addresses the missing information by  testing the model over a range of normalizations [@nixon2024scale]. In doing so, the scale-based approach removes the need for moderated statistics and can replace the consensus approach that has been proposed by some groups [@Nearing:2022aa;@Song:2023aa] with no additional computational overhead. Thus, an advantage of incorporating scale is that analyses can be made much more robust such that actual or potential differences in scale can be tested and accounted for explicitly. While it is beyond the scope of the present article, we note that there are many ways of building scale models that enhance the interpretability of the parameters and assumptions and a detailed description of these points is describe elsewhere [@nixon2024scale].

In summary, we supply a toolkit that makes incorporating scale uncertainty and location information simple to incorporate for transcriptomes or indeed any type of HTS dataset. While the underlying scale of the system is generally inaccessible, the effect of scale uncertainty on the analysis outcomes can be modelled and can help explain some of the underlying biology. Adding scale information to the analysis allows for more robust inference because the features that are sensitive to scale can be identified and their impact on conclusions weighted accordingly. The use of informed  scale models permits difficult to analyze datasets to be examined in a robust and principled manner even when the majority of features are asymmetrically distributed or expressed (or both) in the groups [@dosSantos:2024]. Thus, using and reporting scale uncertainty should become a standard practice in the analysis of HTS datasets.

Acknowledgements: JDS and MPN were supported in part by NIH 1R01GM148972-01. Some aspects of this was supported by an NSERC grant to GBG.

Availability of Code: All code is available at https://github.com/ggloor/scale-sim-bio

Author Contributions: Conceptualization, Methodology, Investigation, Formal Analysis, Software GBG, MPN, JDS; Visualization GBG, MPN; Writing Original Draft GBG; Writing Review and Editing GBG, MPN, JDS; Supervision, Funding JDS.
\singlespacing

# References

