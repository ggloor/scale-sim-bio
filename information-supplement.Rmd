---
title: "In high throughput sequencing all normalizations are wrong but some are often useful"
shorttitle: "Scale explains normalization"
author:
- name: Greg Gloor, Justin Silverman
  affiliation: Dep't of Biochemistry, University of Western Ontario, Penn State
  email: ggloor@uwo.ca
citeproc: TRUE
cite-method: citeproc
bibliography: /Users/ggloor/Library/texmf/bibtex/bib/bibdesk_refs.bib
csl: /Users/ggloor/Documents/0_git/csl_styles/nucleic-acids-research.csl
pdf-engine: latex
  \usepackage{amsmath}
output:
  pdf_document
---


```{r echo=F, warning=F, message=F,comment=F}
# DESeq2
load('analysis/res.Rda')
library(zCompositions)
#ALDEx2
library(ALDEx2)
data(selex)
load(file="analysis/yst.all.Rda")
load(file="analysis/yst.s.all.Rda")
load(file="analysis/yst.1.all.Rda")
# with gamma = 0.5
load('analysis/x.s.all.Rda')

sig.des <- which(res@listData$padj < 0.05)
sig.ald <- which(yst.all$we.eBH < 0.05)
sig.s.ald <- which(yst.s.all$we.eBH < 0.05)

sig.all <- yst.all$we.eBH < 0.05
sig.t <- yst.all$we.eBH < 0.05 & abs(yst.all$diff.btw) > 1.4
sig.s <- yst.s.all$we.eBH < 0.05
sig.1 <- yst.1.all$we.eBH < 0.05

tsc <- read.table("~/Documents/0_git/datasets/transcriptome.tsv", header=T, row.names=1, sep="\t")
```


```{r HvG_function, echo=F}
H <- function(x.vec){
  x.p <- x.vec/sum(x.vec)
  x.p <- x.p[! x.p %in% 0]
  return(-1 * sum(x.p * log2(x.p)))
}
myfun.H <- function(x){ y <- c(x, 1-x); H(y)}
mf.H <- Vectorize(myfun.H)
myfun.2H <- function(x){ y <- c(x, 1-x); 2^H(y)}
mf.2H <- Vectorize(myfun.2H)

G <- function(x.vec){
  x.p <- x.vec/sum(x.vec)
  return(mean(log2(x.p)))
}

myfun.G <- function(x){ y <- c(x, 1-x); G(y)}
myfun.2G <- function(x){ y <- c(x, 1-x); 2^G(y)}
mf.G <- Vectorize(myfun.G)
mf.2G <- Vectorize(myfun.2G)


n.random = 20
n.parts = 500
test='N' # or U, B for Normal or Uniform or Beta

# note mean is only used when 
HvG <- function(n.random, n.parts, test='U', mean=30, sd=1){

  mat.log <- matrix(data=NA, nrow=n.parts, ncol=n.random)
  mat.h <- matrix(data=NA, nrow=n.parts, ncol=n.random)

  # 2000 entries
  val <- seq(2,(n.parts*2), by=2)

  # this will generate data of the 
  # correspondence between H and G in a runif
  # for random number vectors of length
  # 2 to 10000. Does this 5 times
  # N approaches 0 difference as the mean of the N approaches infinity
  for(i in 1:n.parts){
    for(j in 1:n.random){	 
     if(test=='N'){ d <- rnorm(val[i], mean=mean, sd=sd) }
     else if (test == 'U'){ d <- runif(val[i]) * mean }
     else if (test == 'B'){ d <- rbeta(val[i], 0.1, 10) * mean }
     d[d < 0] <- 0
     d <- d/sum(d)
     # mean of log(d), exp = G
     mat.log[i,j] <- G(d)
     # entropy
     mat.h[i,j] <- H(d)
      }
  }

  jnk <- list(mat.log, mat.h)
  names(jnk) <- c("mat.log","mat.h")
  return(jnk)
}

HvG.plot <- function(HvG.out, points=FALSE){

#plot(rep(val,n.random),mat.log- mat.h, pch=19, cex=0.5, col=rgb(0,0,0,0.2),
#  log='x', xlab='length vector', ylab='(-1* G) - H')
plot(HvG.out$mat.log, HvG.out$mat.h, pch=19, cex=0.5, col=rgb(0,0,0,0.2),
   xlab='G', ylab='H')
if(points==TRUE){
  points(HvG.out$mat.log, HvG.out$mat.h, pch=19, cex=0.5, col=rgb(1,0,0,0.2))
}
#abline(lm((HvG.out$mat.log[,2]-HvG.out$mat.h[,2]) ~ val), col='red')
abline(lm((HvG.out$mat.h[,2]~HvG.out$mat.log[,2])), col='red')

#lm((mat.log[,2]-mat.h[,2]) ~ val)
lm((HvG.out$mat.h[,2]~HvG.out$mat.log[,2]))
}
 
```


```{r echo=F, eval=F}
prior <- 0:10/10
matG <- matrix(data=NA, ncol=14, nrow=11)
matH <- matrix(data=NA, ncol=14, nrow=11)
for(i in 1:11){
  matG[i,] <- apply(selex+prior[i], 2, G)
  matH[i,] <- apply(selex+prior[i], 2, H)
}

plot(prior, matH[,1]-matH[,8], ylim=c(6,8), pch=19, col=rgb(0,0,1,0.3))
points(prior, matG[,1]-matG[,8], pch=19, col=rgb(1,0,1,0.3))
abline(lm(matG[2:11,1]-matG[2:11,8] ~ prior[2:11]), col='red')
abline(lm(matH[2:11,1]-matH[2:11,8] ~ prior[2:11]), col='blue')
for(i in 2:7){ 
  points(prior, matH[,i]-matH[,i+7], pch=19, col=rgb(0,0,1,0.3)) 
  abline(lm(matH[2:11,i]-matH[2:11,i+7] ~ prior[2:11]), col='blue', lty=i)
}
for(i in 2:7){ 
  points(prior, matG[,i]-matG[,i+7], pch=19, col=rgb(1,0,0,0.3)) 
  abline(lm(matG[2:11,i]-matG[2:11,i+7] ~ prior[2:11]), col='red', lty=i)
}


```
# Scale in high throughput sequencing

High throughput sequencing generates  data where the total number of reads is not informative about the environment. These data are compositional, that is, the data is constrained to a fixed upper limit and can be represented by their corresponding proportions (or probabilities) without losing any information. These data must be normalized and many of the normalizations in widespread use can be represented as ratios, further the normalized data are almost universally log-transformed prior to analysis. Thus, all of the aforementioned normalizations are  actually log-ratios of some sort. There has been much debate in the literature, and much misunderstanding by the user base as to which normalization is the best because until now the actual assumptions of those normalizations could not be understood inside a common framework. 

Recently, Silverman and colleagues proved that log-ratios could be understood as  implicit assumptions about the actual scale (size) of the environment. Informally, they recognized that an underlying environment \(W\) containing \(D\) parts in \(N\) samples  could be represented by a count matrix decomposed into proportional (p) and total (t) sub-matrices for each sample \(n\)

\begin{equation}
 W_{dn} = W_{dn}^p \cdot  W_n^t
 \label{envt}
\end{equation} 

or the logarithmic form

\begin{equation}
 \log W_{dn} = \log W_{dn}^p  + \log W_n^t,
 \label{logenvt}
\end{equation} 

where \begin{equation} W_n^t=\sum{W_{1 \ldots D, n}} \label{sumenvt}\end{equation}  and \begin{equation}W_{dn}^p = \frac{W_{dn}}{ W_n^t} . \label{propenvt}\end{equation}

The logarithmic form in equation 2 is strikingly similar to the logarithmic formula for the centred log-ratio (CLR) transformation used in compositional data analysis (CoDa). Here the observed data matrix  \(Y\) can be transformed by the CLR
\footnote{The formula for the CLR gives the same answer if the matrix  \(Y\) is the raw count values or if its values have been converted to proportions, or indeed by any other linear transformation. This is the scale-invariance property of CoDa in action.}
  by a taking the log-ratio of each part with the sample geometric mean which we will denote as \(h\):

\begin{equation}
 CLR(Y_{dn}) = \log Y_{dn}  + h_n,
 \label{clr}
\end{equation} 

where  \begin{equation} h_n = - \frac{1}{D} \sum log Y_{1 \dots D, n}. \label{geometric}\end{equation} 

In other words, if \(Y\) is a sample of \(W\), then  \(h_n\) is an implicit estimate of the scale  \( \log W_n^t\). Of course, just because the equations are similar does not guarantee that the values estimated are correct. They went on to show that in the rare instance when the geometric mean was a good estimate of the scale then analyses were trustworthy, but more commonly that the geometric mean was a biased scale estimator of of the data.  Nixon et al made two further important  conclusions: first,  that any estimate that used the logarithm of a ratio approach was also estimating the scale; and second, that the only the relative scale was important. We will discuss the implications of these conclusions in a later section. 


All CoDa based approaches are constrained to real vectors  greater than 0

\begin{equation}
X = [x_1, x_2 \ldots,x_D], x \in \mathbb{R} | x > 0, \sum_X = 1.
\end{equation}

Despite this limitation some advantages of the CoDa framework are that it has true distance metrics and it has a solid geometric interpretation. The usual ways of dealing with 0 values in CoDa approaches are to either impute 0 values, to add a prior to all values, or to restrict the analysis to the parts that include only non-0 values. All these methods restrict the generality of CoDa methods and are rightly pointed out as being weaknesses of the approaches. 

Information theory is another framework in which proportional vectors can be examined. Information theory  developed as a way to examine the amount of information needed to encode, transmit and decode information. Thus the concept of size was build in from the beginning and this has been one of the most useful advances in all of computation and statistics.  The domain of information theory is subtly different from CoDa in that 0 values are permitted:

\begin{equation}
X = [x_1, x_2 \ldots,x_D], x \in \mathbb{R} | x \ge 0, \sum_X = 1.
\end{equation}

 In information theory, the elemental amount of information or surprisal for \(x_i\) is the inverse of the logarithm of the elemental probability [@rezaInfoIntro]. This measure is often called self-information or the surprisal and we will denote this as \(I\):

\begin{equation}
I(x_i) = \log_2(\frac{1}{x_i}) = -\log_2{x_i}.
\end{equation}

Logarithms of probabilities are additive so that for multiple independent probabilities the entropy \(H\) which is the expected information content of the random variable \(x\) is  

\begin{equation}
H= \sum_{X} x_i I(x_i) = -\sum_{X} x_i\log_2(x_i).
\end{equation}

In this formula Shannon defined  \(x_i\log_2(x_i) = 0, x_i=0\) and this effectively removed the 0 constraint when using information theory. Less well known is that a sample average entropy was also defined 

\begin{equation}
h = - \frac{1}{D}\sum log_2(x_i),
\end{equation}

and this is simply the geometric mean. Usefully, \(h\) measure converges on \(H\) as the number of measurements from a "typical subset" of the data approaches infinity; this is the Asymptotic Equipartition Property of information as described in [@mackay2003information;@wilde_2017]. As shown in Chapter 4 of Mackay [@mackay2003information] the typical subset is approximately Gaussian and so if both \(h\) and \(H\) are drawn from this distribution they should be identical in the limit. Another way to think about this is that the two measures converge as the distribution of \(p(x_i\) approaches the uniform \(\frac{1}{N}\). Thus the relationship between \(H\) and \(h\) demonstrates independently that the log of the geometric mean can be interpreted as a size of a probability vector. 

One the challenges of the scale-based approach has been to identify appropriate base scale values for informed scale models. It has been noted before that \(h\) can be a good or a poor initial estimate. Recall that the actual scale value is not important, but that the ratio between scale values in each group is important (or the difference in log(scale) values). Figure 1 shows that one potential confounder is that the value of \(h\) is very sensitive to small changes in initial assumptions when the data are sparse. For example, one method of treating 0 values is to add a small prior to each value in the matrix. Figure 1 shows how \(H\) and \(h\) vary for values of this prior in the range of 0.01 to 1. Note that entropy is essentially not affected by these prior values, but that  \(h\) can produce a very different initial estimate of the scale with different priors. Another method of treating 0s is to replace the 0 values with a pseudocount, this approach  produces a result that is similar to adding a relatively large prior in sparse data and similar to adding a small prior in non-sparse data. A final method is to impute 0 values using a CoDa-based approach. This method 

As shown in Figure 1, one reason for this is that the geometric mean cannot be calculated when one or more of the entries in the probability vector is 0. Thus, Furthermore, the tight relationship between can be exploited 
 
```{r zeroh, echo=F, fig.cap="Plots of H and h with different values added to 0 count parts. The non-sparse plot shows that H and h are linearly related and have a very narrow range over when samples contain no 0 values. The sparse plot shows that H still varies in a small range (although it is about 10-fold larger) but that h now varies has through an approximate 8-fold range when 0 replacement values between 0.1 and 1. The outlier point is a 0 replacement value of 0.01. The difference plot is the mean difference in either h or H between the non-sparse and sparse samples in the dataset. "}

### inpiration can pass function outputs (even nested) as parameters
# ald.ys.h <- aldex(tsc, conditions=y.co, gamma=aldex.makeScaleMatrix(0.5, mu=apply(tsc, 2, function(x)-H(x)), conditions=y.co))
# x.ys.h <- aldex.clr(tsc, conds=y.co, gamma=aldex.makeScaleMatrix(0.5, mu=apply(tsc, 2, function(x)-H(x)), conditions=y.co))

## SELEX
prior <- c(0:10) / 10
p.out <- matrix(data=NA, nrow=14, ncol=6)
prior[1] <- 0.001
for(i in 1:length(prior)){
p.out[i,1] <- -G(selex[,1]+prior[i])
p.out[i,2] <- -G(selex[,8]+prior[i])
p.out[i,3] <- H(selex[,1]+prior[i])
p.out[i,4] <- H(selex[,8]+prior[i])
Gx <- -(apply(selex+prior[i], 2, G))
p.out[i,5] <- mean(Gx[1:7]-Gx[8:14])
Hx <- -(apply(selex+prior[i], 2, H))
p.out[i,6] <- mean(Hx[1:7]-Hx[8:14])
}
selrp1 <- selex 
selrp1[selrp1==0] <- 1
p.out[12,1] <- -G(selrp1[,1])
p.out[12,2] <- -G(selrp1[,8])
p.out[12,3] <- H(selrp1[,1])
p.out[12,4] <- H(selrp1[,8])
Gx <- -(apply(selrp1, 2, G))
p.out[12,5] <- mean(Gx[1:7]-Gx[8:14])
Hx <- -(apply(selrp1, 2, H))
p.out[12,6] <- mean(Hx[1:7]-Hx[8:14])

sel.GBM <- t(cmultRepl(t(selex), label=0, method="GBM"))
p.out[13,1] <- -G(sel.GBM[,1])
p.out[13,2] <- -G(sel.GBM[,8])
p.out[13,3] <- H(sel.GBM[,1])
p.out[13,4] <- H(sel.GBM[,8])
Gx <- -(apply(sel.GBM, 2, G))
p.out[13,5] <- mean(Gx[1:6]-Gx[8:13])
Hx <- -(apply(sel.GBM, 2, H))
p.out[13,6] <- mean(Hx[1:6]-Hx[8:13])

sel.CZM <- t(cmultRepl(t(selex), label=0, method="CZM"))
p.out[14,1] <- -G(sel.CZM[,1])
p.out[14,2] <- -G(sel.CZM[,8])
p.out[14,3] <- H(sel.CZM[,1])
p.out[14,4] <- H(sel.CZM[,8])
Gx <- -(apply(sel.CZM, 2, G))
p.out[14,5] <- mean(Gx[1:6]-Gx[8:13])
Hx <- -(apply(sel.CZM, 2, H))
p.out[14,6] <- mean(Hx[1:6]-Hx[8:13])

cex=c(rep(0.6,11), 1.5,1.5,1.5)
pch=c(rep(0,12), 1,2)
par(mfrow=c(1,3))
plot(p.out[,1], p.out[,3], xlab="h", ylab="H", main="nonsparse", cex=cex, pch=pch)
plot(p.out[,2], p.out[,4], xlab="h", ylab="H", main="sparse", cex=cex, pch=pch)
stripchart(list(p.out[,5], p.out[,6]), method="jitter", vertical=TRUE, main="nonsparse - sparse", group.names=c('h diff', 'H diff') ,cex=cex, pch=pch)

#TSCOME
prior <- c(0:10) / 10
t.out <- matrix(data=NA, nrow=12, ncol=7)
colnames(t.out) <- c("h1", "h2", "H1",H2","mh", 'mH','prior')
prior[1] <- 0.001
for(i in 1:length(prior)){
t.out[i,1] <- -G(tsc[,1]+prior[i])
t.out[i,2] <- -G(tsc[,49]+prior[i])
t.out[i,3] <- H(tsc[,1]+prior[i])
t.out[i,4] <- H(tsc[,49]+prior[i])
Gx <- -(apply(tsc+prior[i], 2, G))
t.out[i,5] <- mean(Gx[1:48]-Gx[49:96])
Hx <- -(apply(tsc+prior[i], 2, H))
t.out[i,6] <- mean(Hx[1:48]-Hx[49:96])
t.out[i,7] <- prior[i]
}
selrp1 <- tsc 
selrp1[selrp1==0] <- 1
t.out[12,1] <- -G(selrp1[,1])
t.out[12,2] <- -G(selrp1[,49])
t.out[12,3] <- H(selrp1[,1])
t.out[12,4] <- H(selrp1[,49])
Gx <- -(apply(selrp1, 2, G))
t.out[12,5] <- mean(Gx[1:48]-Gx[49:96])
Hx <- -(apply(selrp1, 2, H))
t.out[12,6] <- mean(Hx[1:48]-Hx[49:96])
t.out[12,7] <- 1

par(mfrow=c(1,3))
plot(t.out[,1], t.out[,3], xlab="h", ylab="H", main="nonsparse", cex=c(rep(0.6,11), 1.5))
plot(t.out[,2], t.out[,4], xlab="h", ylab="H", main="sparse", cex=c(rep(0.6,11), 1.5))
stripchart(list(t.out[,5], t.out[,6]), method="jitter", vertical=TRUE, main="nonsparse - sparse", group.names=c('h diff', 'H diff') ,cex=c(rep(0.6,11), 1.5))

### SS

url <- "https://raw.githubusercontent.com/ggloor/datasets/main/singleCell.tsv"
ss <- read.table(url, header=T, row.names=1, sep='\t')
ss <- ss[,c(1:100,1502:1601)]

prior <- c(0:10) / 10
ss.out <- matrix(data=NA, nrow=12, ncol=6)
prior[1] <- 0.001
for(i in 1:length(prior)){
ss.out[i,1] <- -G(ss[,1]+prior[i])
ss.out[i,2] <- -G(ss[,101]+prior[i])
ss.out[i,3] <- H(ss[,1]+prior[i])
ss.out[i,4] <- H(ss[,101]+prior[i])
Gx <- -(apply(ss+prior[i], 2, G))
ss.out[i,5] <- mean(Gx[1:100]-Gx[101:200])
Hx <- -(apply(ss+prior[i], 2, H))
ss.out[i,6] <- mean(Hx[1:100]-Hx[101:200])
}
ssrp1 <- ss 
ssrp1[ssrp1==0] <- 1
ss.out[12,1] <- -G(ssrp1[,1])
ss.out[12,2] <- -G(ssrp1[,101])
ss.out[12,3] <- H(ssrp1[,1])
ss.out[12,4] <- H(ssrp1[,101])
Gx <- -(apply(ssrp1, 2, G))
ss.out[12,5] <- mean(Gx[1:100]-Gx[101:200])
Hx <- -(apply(ssrp1, 2, H))
ss.out[12,6] <- mean(Hx[1:100]-Hx[101:200])

par(mfrow=c(1,3))
plot(ss.out[,1], ss.out[,3], xlab="h", ylab="H", main="nonsparse", cex=c(rep(0.6,11), 1.5))
plot(ss.out[,2], ss.out[,4], xlab="h", ylab="H", main="sparse", cex=c(rep(0.6,11), 1.5))
stripchart(list(ss.out[,5], ss.out[,6]), method="jitter", vertical=TRUE, main="nonsparse - sparse", group.names=c('h diff', 'H diff') ,cex=c(rep(0.6,11), 1.5))

```

Many such normalizations have been developed and are in widespread use. These include simple proportions, relative log expression (RLE), trimmed mean of M values (TMM), cumulative sum scaling (CSS), LVHA, IQLR and others. In the case of proportions the assumption being made is that the total sum of \(Y_n\) is the same (or at least directly proportional to) the total sum of \(W_n\). All the other normalizations make the assumption that a subset of the parts in \(Y_n\) are the same (or at least directly proportional to) the same subset of parts in \(W_n\). Thus, while the actual details of how the normalization is conducted vary, the end result is always the same: analyses are conducted with a log of a ratio that can be expressed as a function of the observed data 

\begin{equation}
N(Y_{dn}) = \log Y_{dn}  + f(\cdot n),
\label{function} 
\end{equation} 

where \(\cdot n\) represents the whole or a subset of the parts in sample \(n\).


Returning to equation \eqref{function} we are now in a position to think about  how the concept of scale allows us to understand the assumptions made by different normalizations and to understand how scale normalization supersedes these individual normalizations, including the CLR. 

The simplest normalization is the proportion calculated as in equation \eqref{propenvt}. For proportions the values in the output all sum to 1, and so it  is easy to see that the assumption being made is that the scale of all the \(n\) samples is the same. This holds if we replace 0 values, add priors or remove 0 count parts. In all cases the total remains 1 and the assumption holds. In many cases the assumption that the scale of all samples is nearly constant is likely good enough. For example, it likely holds approximately in data taken from closely controlled experiments where only a small perturbation was made.

However, early on it was noted that simple proportions introduced bias into the analyses and so alternative normalizations were developed. These normalizations were explicitly developed to attempt to scale the output data so that it more closely reflected the environment. They did this by making assumptions that appropriate reference parts, or subsets of parts, could be chosen that would allow a good estimate of \(W_d^n\) from \(Y_d^n\). Multiple such normalizations exist but they differ only in how the subset is chosen. For example the RLE as 

There are several ways of determining the appropriate denominator for a composition. The most general is to determine a matrix of all possible pairwise log-ratios; this moves the data onto a real space that is completely interpretable and symmetric [@Aitchison:1986]. However, for large datasets such as those generated by high-throughput sequencing or mass spectroscopy of biological samples where there are thousands of parts, the number of possible ratios becomes computationally infeasible. Thus, a number of other approaches have been proposed that involve different ways of choosing appropriate denominators for the log-ratio. The main problem now being that the choice of denominator can affect the interpretation of the result. 

The earliest method used by Aitchison was the ALR (additive log ratio), where a presumed invariant part was chosen as the reference for the denominator. The log-ratio was calculated between each part and the reference resulting in a dataset with 1 less entry than the original. The ALR has the advantage of simplicity and ease of interpretation [@aitchison1982;@Greenacre:2021aa]. The primary disadvantage of the ALR is that it is not isometric, that is, the ALR does not by default  recapitulate the geometry of the all-vs-all log-ratio. Greenacre [@Greenacre:2021aa] showed that it was possible to identify a reference for most datasets that was isometric for all practical purposes. It is noteworthy that the ALR has long been used, without naming it such, in fields such as molecular biology and mass spectrometry where it is common to use a single reference as a standard.

Another approach proposed was the CLR (centred log-ratio), where it is presumed that the geometric mean of the parts in each sample is invariant. The CLR is isometric, and captures all of the ratio information of the all-vs-all approach and has the further advantage of being computationally tractable. 

Intermediate denominators have also been proposed. For example, the LVHA (low variance, high abundance) log-ratio 

choices of reference give slightly different H ( CLR, ALR, hybrid
  - choice of denominator affects interpretation
  - Greenacre

Silverman and colleagues introduced concept of scale which still uses a log-ratio. The actual method of choosing the reference is nearly immaterial, instead it is the relationship between the denominators for each group that determines if the log-ratio is useful. The key finding was that the denominator chosen was a proxy for the underlying size or scale of the environment. The original work by Silverman and colleagues proved this relationship, here we provide an intuitive demonstration based on information theory. 

Information theory and CoDa are two ways to deal with compositional data, but information theory is more general. CoDa approaches are applicable to any real vector of numbers 

\begin{equation}
X = [x_1, x_2 \ldots,x_D], x \in \mathbb{R} | x > 0, \sum_X = 1.
\end{equation}

Information Theory approaches are subtly different in that here 0 values are permitted:

\begin{equation}
X = [x_1, x_2 \ldots,x_D], x \in \mathbb{R} | x \ge 0, \sum_X = 1.
\end{equation}

 This s in the l to 

Scale is related to entropy
  - Shannon's entropy measures size of the data
  - define entropy
  - define size or scale
  - H is insensitive to 0
  - G \(\sim\) H
    - can use diff H as base


# GM is related to Information and Shannon's entropy in HTS datasets

## Shannon's entropy has a volume or size

Information content is a fundamental property of all measured systems. Shannon defined the information properties of discrete probability vectors in the field of communications and launched the field of information theory. Information theory concerns itself with  how to encode a dataset optimally to minimize its size when transmitted.

The following example is adapted from Chapter 2 of [@wilde_2017]. If we have information encoded in bits--i.e., either a "0" or "1"--we can imagine a uniform encoding scheme for four symbols  

\begin{equation}
x  = [A,C,G,T]
\end{equation}

where the underlying probability of observing each symbol is 
 
\begin{equation}
Pr_x=[\frac{1}{2},\frac{1}{4}, \frac{1}{8}, \frac{1}{8}]. 
\end{equation}
 
Now we can define two encodings; a uniform encoding 

\begin{equation}
U_x=[00,01, 10, 11] 
\end{equation}

and a non-uniform encoding

\begin{equation}
N_x=[0,10, 110, 111]. 
\end{equation}

In the uniform encoding the number of bits per symbol is always 2 and in the non-uniform encoding it varies from one to three, with the most common symbol encoded by one bit and the rarest symbols encoded by three bits. Lets now examine the amount of information needed to encode a string of characters that has a similar frequency to the true background. The 11 character string is: 

\begin{equation}
aacbdabbaab.
\end{equation}

In the uniform encoding this needs 22 bits because each symbol requires two bits:

\begin{equation}
\sum{U_x} = 5a + 4b + c + d = 10 + 8 + 2 + 2 = 22.
\end{equation}

In the non-uniform encoding this needs 19 bits because the number of bits for each symbol is encoded by a variable number of bits:

\begin{equation}
\sum{N_x}  = 5a + 4b + c + d = 5(1) + 4(2) + 1(3) + 1(3) = 19.
\end{equation}

Going back to our known background frequencies we can calculate the expected number of bits for a given set of symbols and frequencies. For the uniform encoding we get 
\begin{equation}
\frac{1}{2}(2), \frac{1}{4}(2), \frac{1}{8}(2), \frac{1}{8}(2) = \frac{16}{8} = \frac{8}{4} = 2
\end{equation}

which is 2 bits per symbol. While for the variable length encoding we get 

\begin{equation}
p_i = \frac{1}{2}(1), \frac{1}{4}(2), \frac{1}{8}(3), \frac{1}{8}(3) = \frac{14}{8} = \frac{7}{4}
\end{equation}

which is smaller.

We are now able to understand the formula for entropy. For notational simplicity assume we have a single discrete random variable to represent a probability distribution with d elements; i.e. \( X =  \mathbf{p}_{i=(1 \dots d )} \). In information theory, the elemental amount of information or surprisal for \(p_i\) is the inverse of the logarithm of the elemental probability [@rezaInfoIntro]. This measure is often called self-information or the surprisal and I will denote this as \(I\):

\begin{equation}
I(X_i) = \log_2(\frac{1}{X_i}) = -\log_2{X_i}.
\end{equation}

Logarithms of probabilities are additive so that for multiple independent probabilities the expected information content of the random variable \(x\) is  

\begin{equation}
\sum_{X} p(X_i) I(X_i) = -\sum_{X}  p(X_i)\log_2(X_i).
\end{equation}

And this is the formula for entropy \( H \)! For any encoding of information in a probability vector Shannon showed that \(H\) is the expected information content of that probability vector. We can demonstrate this property  if we remember that the base 2 logarithms of the fractions from equation 2  are -1, -2, -3, -3. The expected entropy to base 2 is thus: 

\begin{equation}
H(x) = -\Bigl(\frac{1}{2}(-1) + \frac{1}{4}(-2) + \frac{1}{8}(-3) + \frac{1}{8}(-3) \Bigr) = \frac{7}{4}
\end{equation} 

which is precisely the same result as in equation 9 where we  calculated the average bit length of the infinite length message. Thus, entropy  measures the average information encoded by each symbol in a message. One conclusion of the above discussion is that information has a size and that we can measure that size as the average number of bits needed to encode the information.


Less well known is that a sample average entropy was also defined 

\begin{equation}
h = - \frac{1}{d}\sum log_2(p),
\end{equation}

and that this measure converges on \(H\) as the number of measurements from a "typical subset" of the data approaches infinity; this is the Asymptotic Equipartition Property of information as described in [@mackay2003information;@wilde_2017]. As shown in Chapter 4 of Mackay [@mackay2003information] the typical subset is approximately Gaussian and so if both \(h\) and \(H\) are drawn from this distribution they should be identical in the limit. Another way to think about this is that the two measures converge as the distribution of \(p_i\) approaches \(\frac{1}{n}\).


```{r asymptote, echo=F, fig.cap="Plot showing that \\(H\\) and \\(h\\) converge as the size of the dataset increases.  "}
set.seed(13)
coeff <- vector()
s.coeff <- vector()

mn <- c(5,10,30,60,100,200,400,800,1000,2000)
std <- c(10,7,5,2,1,.8,.6,.5,.3,.2)
for(i in 1:10){
  X <- HvG(50, 500, test='N', mean=mn[i], sd=1)
  jnk <- lm((X$mat.h[,2]~X$mat.log[,2]))
  coeff[i] <- as.numeric(jnk$coefficients[1])
  X <- HvG(50, 500, test='N', mean=1000, sd=std[i])
  jnk <- lm((X$mat.h[,2]~X$mat.log[,2]))
  s.coeff[i] <- as.numeric(jnk$coefficients[1])
}
par(mfrow=c(1,1))
plot(mn, -coeff, log='y', xlab="vector mean", ylab="H vs. G intercept")
```


Compositional data analysis is concerned with the analysis of compositions; datasets that are strictly positive and that have an arbitrary upper limit. These can always be reduced to a proportion (or probability) without loss of information. Compositional approaches have become popular for the analysis of high throughput sequencing data, and other high dimensional data generated by 'omics platforms. In  compositional data analysis (CoDa)  one popular data transformation is the centred log-ratio (CLR) transform, which is:

\begin{equation}
clr_x = log_2(x_i) - mean(log_2(x) =  log_2(x_i) + h_x,
\end{equation}

and we can see immediately that the second term is equivalent to \(h(x)\). Thus, information theory and compositional data analysis intersect through the geometric mean of a probability vector. 

This intersection of information theory and CoDa  can help us understand the scale of a system as defined by Nixon and Silverman which can help in the interpretation of many high throughput sequencing datasets. Since \(H\) can be defined as a volume for discrete distributions it is reasonable to treat \(h\)  treated similarly. 

Thus, the use of the geometric mean as a measure of size is acceptable both from the information theoretic and from a CoDa perspective.

Biological context of H - average amount of information in each entry. CLR is diff between sample avg entropy and each i(x)

We can think about  scale  from an information theoretic point of view as a measure of how much information, or total uncertainty, is encoded in a particular sample [@Shannon.48;@Jaynes:2003]. In the geometric interpretation of information theory used in quantum information theory of information [@Cover:1991;@wilde_2017], entropy can be interpreted as the volume occupied by a probability distribution relative to the maximum total entropy. See chapters 4 of the PhD thesis of Lecamwasam [-@ruvi.blog] for a more complete explanation of this. Furthermore, the  Asymptotic Equipartition Property says that the expected value of h(X) for a large number of random variables approaches H(X) as the number of variables approaches infinity [@Cover:1991;@wilde_2017].


The weighted average entropy of the system  \( H(X) \) is the weighted sum of the elemental information;

\begin{equation}
H(X) = - \sum_{i=1}^{d} p_i \log_2 p_i
\end{equation}

\( H(X) \) corresponds to the mean amount of information in each entry that is needed  to reproduce  \( X \). We can also calculate the unweighted expected amount of information for each observation in the random variable, and this is the mean of the elemental probability. This measure is also called the sample average of the information [@wilde_2017];

\begin{equation}
 h(X) = - \frac{1}{d} \sum_{i=1}^{d} \log_2 p_i
\end{equation}

The linkage between compositional analysis, scale inference and information theory comes when we realize that the logarithm of the geometric mean calculated in base(2) is: 

\begin{equation}
 l2G(X) = \log_2 G(X) = \frac{1}{d} \sum_{i=1}^{d} \log_2 p_i
\end{equation};

We see that \( h(X) = - l2G(x) \). Furthermore, \(l2G\) is used as the basis for the centred log-ratio transform and is the starting point for scale-based inference:

 \[CLR(X) = log2(p_i) - l2G(X) = log2(p_i) + h(X)  \]
 
Thus we see that the geometric mean used in the centred log ratio (CLR), often used for Compositional Data Analysis (CoDa) [@aitchison1982] is  related to entropy or \(H\). Indeed, we can rearrange the CLR formula to show that it can be interpreted as computing the difference between the elemental information and the mean information content: 
  
  \[CLR(X) = -log2(p_i) - (-l2G(X)) = -(-log2(p_i) - h(X))\]

As defined in [@nixon2023scale], the scale is the inverse of \( l2G \), which is \(h(X)\). Thus, one way we can understand scale is that it is measuring the total complexity of the system, and this is expected to increase with absolute size in most cases. Moreover, \(H(X)\) and \(G(X)\) share similar shapes in the continuum between 0-1 for a bivariate distribution as shown below:

```{r} 
par(mfrow=c(1,2))
curve(mf.G, from=1e-2, to =.99,  col='red', lty=2, ylim=c(-3,1),
   xlab="x in [0,1]", ylab="value", main="H and G")
curve(mf.H, from=1e-2, to =.99,  col='black', lty=2, add=T )

legend(.4,-1.5, legend=c('H','G'), col=c('black','red'), pch="-")

vals <- seq(from=.001, to=0.99, by=0.001)

plot(mf.2G(vals),mf.2H(vals), xlim=c(0,0.5), ylim=c(1,2), pch=19, cex=0.1,
  xlab="2^G", ylab="2^H", main="2^G vs 2^H")

```



The difference being that entropy is constructed to have a value of 0 at the margins because Shannon defined \( 0log(0) = 0\) while the geometric mean approaches negative infinity. 

Now let's think about the idea of entropy as a volume which allows us to identify what amount of the available entropy space a given observation fills. The following is taken and modified from [@ruvi.blog], to which you should refer for a more fulsome discussion, and it is covered in Chapter 2 of Wilde[-@wilde_2017] and Chapter 3 of Cover and Thomas [-@Cover:1991].  If we start with a four part system \(X1 = [A,C,G,T]\) where the frequencies are equally and identically distributed, then \(p_A = p_C = p_G = p_T = \frac{1}{4} \). \( H(X1) = -1 * 4 * (\frac{1}{4} * log2(\frac{1}{4}) ) = 2\). This is the maximum entropy possible. We can obtain the "volume" of \(X1\) by exponentiating \(H1\) using the same base as was used to calculate the entropy; \( V1 = 2^{H1} = 4 \). This is the same as the number of letters in the system; so the volume needed to explain the system is 4 units (in this case bits). But what happens in another system, \(X2\) where A occurs with a much higher probability, say 0.7, and the other three are distributed equiprobably amongst the remainder with a probability of 0.1; i.e. \(p_C = p_G = p_T = 0.1 \). In this case \( H2 = -1 * ((\frac{7}{10} * log2(\frac{7}{10}) ) + (3 * (\frac{1}{10} * log2(\frac{1}{10}) ))) = 1.358 \). Here the volume of \(X2 = 2^{H2} = 2.56\); meaning that less than the maximum volume is taken up by the information. Here \(X2\) consumes about 64% of the volume of system \(X1\). Thus, the entropic volume is a measure of the total complexity or the scales of the two systems. 

But what happens if we consider the geometric mean instead of the entropy? In the example above, 
\( l2G(X1) = (4 * log2(0.25))/ 4 \) = -2, and \( l2G(X2) = (log2(\frac{7}{10}) + 3 * log2(\frac{1}{10}))/4 = -2.62 \). Exponentiating gives us values of 0.25 and 0.16 suggesting that the \( l2G \) measure includes some estimate of size. Comparing the size of \( H \) vs \(2^(l2G) \) suggests that both measures contain related information. Thus we can understand that scale is related to the information volume of a system, which in turn is related to the size of the system. 

Empirically, we can see that \(G(\mathbf{Y}^{\parallel}_{n})\) is strongly correlated with Shannon's Entropy \(H(\mathbf{Y}^{\parallel}_{n})\) as expected from the discussion above, and that this difference converges to a constant as the number of entries in the probability vector increases regardless of the distribution, although different distributions converge at different rates. For example, if we plot the relationship between H and G as a function of the length of the probability vector we can see a direct inverse relationship. 

```{r, echo=F, fig.cap="Association between entropy (H) and geometric mean (G) as a function of vector length. Twenty random vectors were constructed for each length between  2 and 1000 in increments of 2 for each of the random distributions in the legend; N = Normal, U = Uniform, B = Beta. The bottom right of each plot represents vectors of length 2, and the top left represents the vector of lenght 500. The maximum value of H increases as the vector length increases, and the maximum value of G decreases in lock-step. Each random distribution has an obviously distinct relationship between the two measures. For the purposes of high throughput sequencing the Beta distribution is most similar to that seen in the majority of instances."}
set.seed=24
par(mfrow=c(1,3))
U <- HvG(50, 500, test='U', mean=100)
U.plot <- HvG.plot(U)
text(-4,0.5,labels='vec len = 2')
text(-7.4,9.8,labels='vec len = 1000')
title(main='random uniform')
abline(0,-1, lty=2)
B <- HvG(50, 500, test='B', mean=100)
B.plot <- HvG.plot(B)
title(main='Extreme skew')
abline(0,-1, lty=2)
N <- HvG(50, 500, test='N', mean=100, sd=1)
N.plot <- HvG.plot(N)
title(main='N, mean=100')
abline(0,-1, lty=2)


# intercept
U.plot$coefficients[1]
B.plot$coefficients[1]
N.plot$coefficients[1]
par(mfrow=c(1,1))

```


When we plot the relationship for any individual probability vector, we  see that there is an direct relationship between the entropy and the log of the geometric mean, but that this relationship strongly depends on the underlying distribution of the probability distribution \( X \). 


```{r, echo=F, fig.cap="Plot of the association between H and G at a vector length of 30. The relationship between H and G is inverse, and the strength of that association depends on the distribution. The N distribution shows a very strong assocation, while the Beta distribution is less well defined. Associations are shown for a vector length of 30."}
par(mfrow=c(1,3))
plot(N$mat.log[15,], N$mat.h[15,], main="Normal", xlab="G", ylab="H")
abline(lm(N$mat.h[15,]~N$mat.log[15,]), lty=2, col='grey', lwd=2)
abline(0,1, col='red', lty=3)
plot(U$mat.log[15,], U$mat.h[15,], main="Uniform", xlab="G", ylab="H")
abline(lm(U$mat.h[15,]~U$mat.log[15,]), lty=2, col='grey', lwd=2)
plot(B$mat.log[15,], B$mat.h[15,], main="Beta", xlab="G", ylab="H")
abline(lm(B$mat.h[15,]~B$mat.log[15,]), lty=2, col='grey', lwd=2)
par(mfrow=c(1,1))
```


In real data, shown in Supplemental Figure 3 and Table 1 the correspondence is not as predictable, likely because the real data is a more complex distribution than any of the idealized distributions.  Thus, these two measures have different behaviours with different distributions of \( p_i \). In the case of a uniform distribution both \( H(\mathbf{Y}^{\parallel}_{n}) \) and \( G(\mathbf{Y}^{\parallel}_{n}) \) are maximal when \( p(x) \) is equally and identically distributed. Thus, we expect that they are positively correlated here. In a Normal or a skewed distribution, we also observe a positive correlation because both are affected in the same direction by outlier values. In very sparse  datasets, the two measures could become uncoupled because \( H(\mathbf{Y}^{\parallel}_{n}) \) could ascribe some uncertainty to the large number of low probability events, while \( G(\mathbf{Y}^{\parallel}_{n}) \) would tend to be very small. Here these two measures could be either uncorrelated or exhibit negative correlation. We can see this distributional behaviour in different datasets.

Intuitively,  systems with different scales will contain different amounts of information and so we would expect \(W^{\perp}_n \sim H_n\). As the scale of a system as defined by Nixon et al. [-@nixon2023scale] is inversely related to \(G\), this means that scale is directly proportional to the information content and entropy of the data. 

Below I show that we can replace \( G \) with \(H \) in the calculations performed by ALDEx2 without loss of utility.

Recall the underlying system is described by a \(D \times N\) matrix of counts \(\mathbf{W}\) decomposed into the proportions for the \(n^{th}\) sample \(\mathbf{W}^{\parallel}_n\) (or the equivalent probability distribution \( \mathbf{p}(w_n) \) ), and its scale \(\mathbf{W}^{\perp}_n\), such that \(\mathbf{W}=\mathbf{W}^{\parallel}\mathbf{W}^{\perp} \). Sequencing  returns counts which are related to the underlying proportion; i.e., \(\mathbf{Y}^{\parallel}_n \sim \mathbf{W}^{\parallel}_n\)
 



```{r info, echo=F,warning=F, message=F,comment=F, fig.cap="Plot of Shannon's entropy (H) vs geometric mean (G) for each sample in different datasets. The groups that each sample belong to are highlighted as filled or open circles. Each group in each dataset has different entropy with the groups in the selex and metatranscriptome datasets being highly distinct."}
# Gm ~ Im
# plotting geometric mean vs Shannon's entropy
# shows differential information by group
library(ALDEx2)
data(selex)

load(url('https://raw.githubusercontent.com/ggloor/datasets/main/ko.both.Rda'))
url <- "https://raw.githubusercontent.com/ggloor/datasets/main/transcriptome.tsv"
yst <- read.table(url, header=T, row.names=1)
url <- "https://raw.githubusercontent.com/ggloor/datasets/main/meta16S.tsv"
rRNA <- read.table(url, header=T, row.names=1, sep='\t')
url <- "https://raw.githubusercontent.com/ggloor/datasets/main/singleCell.tsv"
ss <- read.table(url, header=T, row.names=1, sep='\t')
ss <- ss[,c(1:100,1502:1601)]
# remove the one gene with 0 reads
yst <- yst[rownames(yst) != "YOR072W-B",]
# Gierlinski:2015aa
yst[,c('SNF2.6', 'SNF2.13','SNF2.25','SNF2.35')] <- NULL 
yst[,c('WT.21','WT.22','WT.25','WT.28','WT.34','WT.36')] <- NULL  

HS <- apply(selex+0.5, 2, function(x)  H(x) )
GS <- apply(selex+0.5, 2, function(x) G(x) )

HY <- apply(yst+0.5, 2, function(x) H(x) )
GY <- apply(yst+0.5, 2, function(x) G(x) )

HM <- apply(ko.both+0.5, 2, function(x) H(x) )
GM <- apply(ko.both+0.5, 2, function(x) G(x) )

H16 <- apply(rRNA+0.5, 2, function(x)  H(x) )
G16 <- apply(rRNA+0.5, 2, function(x) G(x) )

Hss <- apply(ss+0.1, 2, function(x) H(x) )
Gss <- apply(ss+0.1, 2, function(x) G(x) )

par(mfrow=c(2,3))
plot(HS, GS, ylab="Geometric mean", xlab="Shannon's entropy",
  pch=c(rep(19,7),rep(1,7)), col=rgb(0,0,0,0.5))
title(main='selex')

plot(HY,GY, 
  ylab="Geometric mean", xlab="Shannon's entropy", pch=c(rep(19,44),rep(1,42)),
  col=rgb(1,0.6,0,0.5))
title(main='transcriptome')

plot(HM, GM, col=rgb(1,0,0,0.5), pch=c(rep(1,8), rep(19,28), rep(1,8)), 
  ylab="Geometric mean", xlab="Shannon's entropy")
title(main='meta-transcriptome')

plot(H16,G16, col=rgb(0,1,1,0.5),
  pch=c(rep(1,198), rep(19,161)), 
  ylab="Geometric mean", xlab="Shannon's entropy")
title(main='16S rRNA')

plot(Hss,Gss, col=rgb(0.5,1,0,0.5),
 pch=c(rep(1,100), rep(19,100)), 
  ylab="Geometric mean", xlab="Shannon's entropy")
title(main="single cell")

# empty plot for legend
plot(selex[,1], selex[,2], xlab=NA, ylab=NA, axes=F,xlim=c(-10,-1), ylim=c(-10,-1))
legend(-10,-1, legend=c('selex', 'transcriptome','metatranscriptome', '16S', 'SS', 'low entropy group', 'high entropy group'),
  col=c('darkgrey', 'orange', 'red', 'cyan', 'greenyellow','grey','grey'), pch=c(19,19,19,19,19,1,19))
  
```

The table below summarizes the mean values for, and the correlation between, \( G \) and \( H \)  (cor)  and the sparsity defined as the proportion of features with less than 1 count per sample (spar) for each association in each group of samples:

| Dataset | group | \(\overline{G}\) | \( \overline{H} \) | cor | spar |
|---------|-------|---|---|-----|-------|
| Selex   | control | -11.2 | 10.2 | 0.99 | 0 |
| $~~~$"  | selected | -17.8 | 2.8 | -0.88 | 0.802 |
| yeast   | snf2 ko  | -14.0 | 10.7 | 0.99 | 0.004 |
| $~~~$"  | WT    | -14.2 | 10.4 | 0.99 | 0.007 |
| Meta    | H     | -18.8 | 8.6 | 0.78 | 0.451 |
| $~~~$"  | BV   | -18.2 | 8.9 | 0.79 | 0.238 |
| 16S     | Pup   | -14.7 | 5.4 | 0.68 | 0.079 |
| $~~~$"  | Cent | -15.2 | 5.4 | 0.53 | 0.251 |
| SS      | A     | -13.0 | 8.2 | 0.83 | 0.978 |
| $~~~$"  | B    | -12.9 | 8.3 | 0.80 | 0.977 |


While not necessary in all datasets (e.g., the transcriptome example discussed in the previous section and see the Supplementary material), it can greatly improve modeling in certain cases, especially when the scale is highly asymmetric between conditions. In order to specify a scale model from scratch, we need to revisit the concept that all normalizations in widespread use are actually ratios with the denominator implied by the normalization. Therefore, we can easily deviate from a certain normalization (e.g., the geometric mean assumption implied by the CLR) by specifying a total model based on the mean difference between conditions. While knowing the mean difference between conditions may seem cumbersome in practice, it is the \emph{relationship} between the group scale values that is important, not their raw values (Nixon et al. 2023). This can be illustrated quite simply by starting with the mean ratio in \(G\) between groups for the yeast transcriptome dataset which are \(6.05\times 10^{-5}\) for snf2 and \(5.08\times 10^{-5}\) for WT; their ratio being 1.17, or a 0.17-fold difference. Using this information, we can recapitulate the differential abundance analysis in Figure 2B and 2C exactly by using setting the mean denominator of group 1 to 1, and group 2 to 1.17 with a gamma of 0.5 as shown in Supplemental Figure 2. This ratio can be adjusted to alter the mean assumption placed on the group scale values. 

We can see that for most datasets the difference between the \(\overline{G}\) in each group is relatively small. Most significantly, the selex dataset has a very large difference of about 100-fold, and both the 16S and the metatranscriptome dataset have about a 1.5 fold difference. These three datasets are candidates for a full scale model correction.

```{r full.scale, echo=F, fig.cap="Plot of the offset of the mean differnece between groups as a function of scale ratio. For this, the default scale of 1:1 was altered in increments of 0.1 keeping the gamma parameter (dispersion) at 0.5. The filled circle shows the outcome when the calculation is done using the geometric mean and the same gamma parameter."}

load('analysis/data.out.Rda' )

plot(data.out[,6],data.out[,1], pch=c(rep(1,19),19), ylim=c(-11, 4),
  xlab='full scale offset', ylab='mean difference between groups')
points(data.out[,6],data.out[,2], pch=c(rep(1,19),19), col='red')
points(data.out[,6],data.out[,3], pch=c(rep(1,19),19), col='orange')
points(data.out[,6],data.out[,4], pch=c(rep(1,19),19), col='cyan')
points(data.out[,6],data.out[,5], pch=c(rep(1,19),19), col='blue')
abline(h=0, col='grey', lty=2)
abline(v=0, col='grey', lty=2)
legend(0.7, 0, legend=c('16S','selex','tscome','metatscome','ss'),
  pch=19, col=c('black','red','orange','cyan','blue'))

```

Nixon et al. [-@nixon2023scale] showed that many operations on HTS datasets relied on both the proportion and scale components of the data. Moreover, all normalizations impose a scale model on the data, but the appropriateness of these models has never been explicitly acknowledged or tested. Thus, the full scale model option allows the investigator to set both the offset between the geometric means of the features and their dispersion and observe how this affects the analysis outcome. 

In the offset plot above we can see the cause and the effect of the full model with a fixed gamma of 0.5 and a base scale of 1 in each group. We see that the mean location of well centred datasets (yeast transcriptome, single-cell transcriptome) are close to 0, but could be centred better with small changes in scale ranging from 0 (single cell) to 1:1.1 for the yeast transcriptome dataset. In contrast, centring the 16S dataset requires about a 1:1.3 fold change. The metatranscriptome dataset would require about a 0.5:1 change in relative scale between groups, but as shown in the main text, centring the housekeeping genes is more apt. 

The in vitro selection dataset is clearly an outlier in both the difference between the average group geometric mean, and in the offset plot.  However, this dataset can  be used to illustrate the power of the full scale model and the relationship between \( G_n \) and scale. In Figure 3 we can see that the default output of ALDEx2 has a centered output. This occurs largely by chance, as the high sparsity of the selected (S) group is balanced almost exactly by the arbitrarily chosen sequencing depth so the non-selected group (NS) appears to have a similar location as the S group. The difference in entropy between the two groups and the differences in geometric mean are very large, with the difference in \(\log_2{\overline{G}(S)}\) and \(log_2{\overline{G}(NS)}\) being about \(2^{6.6}\). Setting the scale of both the S and NS groups to 1 we find that the difference in location is approximately \(2^{7.7}\) in close agreement with the difference the geometric means. For this dataset to be centered we need to have a scale ratio \( \approx \) 1:50 or more. Note that the scale ratio is inverse to the ratio of geometric means as described above.  In fact, in this dataset the relative abundances of the majority of features are nearly invariant, but this is masked by the large absolute changes in a small number of features [@mcmurrough:2014], thus changing the scale of the data. Neither DESeq nor edgeR are able to provide a reasonable analysis of this dataset because the normalizations used assume equivalent scales [@gloorAJS:2016]. 

Figure 3 shows an effect plot of various scale models with this dataset. The full scale model, where the strong assumption that the mean \(G\) is assumed to be 1:1 between the two groups, dramatically skews the output and the large number of relatively invariant features are now identified as significantly different. While not wrong as long as the assumption that the scales are identical is stated, this is not a useful analysis outcome. Modifying the mean scale difference between the NS:S groups to be \( \approx \) 1:50 different moves the centre of the large number of relatively invariant features to the centreline of no difference, and recapitulates the default result obtained using \(G_n\) as the scale estimate where the ratio is \( \sim 100:1 \). Note that we get exactly the same answer (within random sampling error) with a scale of .02 for group NS and a scale of 1 for group S, or using a scale of 1 for group NS and a scale of 50 for group S. This shows that it is the relative difference between scales that is important in this dataset, not the absolute values. From this result we can conclude that, on average, the difference in underlying scale in the system is about \(\approx 50\)-fold, and this is congruent with the circa 100-fold difference in  \(\overline{G}\) between groups; the discrepancy being explained because the default scale model is applied uniformly to all samples, whereas the different values of the within-group geometric means ranging over a \{ > 2.5 \) fold range.  Thus, an advantage of a full scale model is that we have gained both information and understanding about the drivers of asymmetry underlying system.


```{r selex, eval=T, echo=F, warning=F, message=F,comment=F, fig.cap="Effect plots of the selex dataset with various gamma and scale parameters. All scales are calculated with a logNormal distribution to ensure symmetry for the user. "}

# from selex.aldex.mu.R
load(file="analysis/sel.2.all.Rda")
load(file="analysis/sel.5.all.Rda")
load(file="analysis/sel.1.all.Rda")
load(file="analysis/sel.all.Rda")

par(mfrow=c(1,4))
aldex.plot(sel.all, main=('gamma=0.5\nmu=G'))
aldex.plot(sel.1.all, main=('gamma=0.5\nmu=1,1'))
aldex.plot(sel.5.all, main=('gamma=0.5\nmu=1,50'))
aldex.plot(sel.2.all, main=('gamma=0.5\nmu=0.02, 1'))
```

# How scaling affects dispersion

```{r, echo=F}
load(file="analysis/yst.all.Rda")
load(file="analysis/yst.s.all.Rda")
load(file="analysis/yst.1.all.Rda")
```



# Issues with DESeq2 and edgeR

DESeq2 and edgeR are two of the most commonly used tools for differential abundance analysis of bule RNA sequencing datasets. They both operate by finding a scaling factor that makes all the samples commesurate. DESeq2 does this by finding a midpoint feature that can be used as a reference in each sample; this can be different for different samples. The edgeR reference finds the midpoint of the 'typical' sample instead. In both cases the data are then scaled by dividing by a small factor that makes the read counts commesurate. Differential abundance analysis is then performed on the scaled values after taking their logarithm to base 2. In some ways this is similar to the log-ratio approach used by ALDEx2, but is more prone to dataset and sample effects than is the log-ratio method [@GloorAJS2023] 

```{r DESedg, eval=T, echo=F, fig.cap="Shown here are the mean log2 fold change as a density plot, and a Volcano plot showing the location and adjusted p-value for each feature in the metatranscriptomic dataset. The DESeq2 approach does a good job of centring this data, while edgeR is less suitable. The volcano plots show dramatically different outcomes. The DESeq2 algorithm assigns very large fold changes to features that have only moderate change, and further identifies a very large proportion of features as significantly different. In contrast, edgeR exhibits a much smaller number of differentially abundant features. In both volcano plots, the housekeeping genes in the main Figure 3 are shown in orange. We can see that these are asymmetrically distributed in both plots. Additionally the location of the features that DESeq2 identified as having a very large difference are shown in the edgeR volcano plot as blue circles."}

# find housekeeping from ALDEx2

load('analysis/xt.m.Rda')
load('analysis/xt.Rda')
load('analysis/xg.Rda')
hk <- rownames(xt.all)[xt.all$diff.win < 2.5 & xt.all$diff.btw > 1 & xt.all$diff.btw < 3]
hk.off <- xt.all$diff.win < 2.5 & xt.all$diff.btw > 1 & xt.all$diff.btw < 3

load('analysis/meta.DES.res.Rda')
load('analysis/meta.edge.qlf.Rda')

# these are all very rare K0s
zero.var <-  which(meta.DES.res@listData$lfcSE*sqrt(44) == 0)

weird.des <- which(meta.DES.res@listData$log2FoldChange < -20 )

# housekeeping genes centred with DESeq2 - nice

sig.des <- which(meta.DES.res@listData$padj < 0.01)
sig.edge <- which(p.adjust(meta.edg.qlf$PValue) < 0.01)
sig.ald <- which(xt.m.all$we.eBH < 0.01)

par(mfrow=c(2,3))

plot(density(xt.all$diff.btw), xlim=c(-5,5), ylim=c(0,1), main='ALDEx2 FC')
points(density(xt.all$diff.btw[hk.off]), type='l', col='orange')
abline(v=0, col='orange', lty=2)

plot(density(meta.DES.res@listData$log2FoldChange), xlim=c(-5,5), ylim=c(0,0.8), main='DESeq2 FC')
points(density(meta.DES.res@listData$log2FoldChange[hk.off]), type='l', col='orange')
abline(v=0, col='orange', lty=2)

plot(density(meta.edg.qlf$logFC), xlim=c(-5,5), ylim=c(0,0.5), main='edgeR FC')
points(density(meta.edg.qlf$logFC[hk.off]), type='l', col='orange')
abline(v=0, col='red', lty=2)


plot(xt.m.all$diff.btw, -1*log10(xt.m.all$we.eBH), 
  col=rgb(0,0,0,0.1), xlab='log2 Difference', ylab='-1 log10(p.adjust)', main='ALDEx2 volcano')
points(xt.m.all$diff.btw[sig.ald],-1*log10(xt.m.all$we.eBH)[sig.ald] + 1e-8, col='red')
points(xt.m.all$diff.btw[hk.off],-1*log10(xt.m.all$we.eBH)[hk.off] + 1e-8, col='orange', cex=0.6)


plot(meta.DES.res@listData$log2FoldChange, -1*log10(meta.DES.res@listData$padj), 
  col=rgb(0,0,0,0.1), xlab='log2 Difference', ylab='-1 log10(p.adjust)', main='DESeq2 volcano')
points(meta.DES.res@listData$log2FoldChange[sig.des],-1*log10(meta.DES.res@listData$padj[sig.des] + 1e-70), col='red')
points(meta.DES.res@listData$log2FoldChange[hk.off],-1*log10(meta.DES.res@listData$padj[hk.off] + 1e-70), col='orange', cex=0.6)

plot(meta.edg.qlf$logFC, -1*log10(p.adjust(meta.edg.qlf$PValue)), 
  col=rgb(0,0,0,0.1), xlab='LFC', ylab='-1log10(p)', main='edgeR volcano')
points(meta.edg.qlf$logFC[sig.edge], -1*log10(p.adjust(meta.edg.qlf$PValue)[sig.edge]), 
  col='red')
points(meta.edg.qlf$logFC[hk.off], -1*log10(p.adjust(meta.edg.qlf$PValue)[hk.off]), 
  col='orange', cex=0.6)
  points(meta.edg.qlf$logFC[weird.des], -1*log10(p.adjust(meta.edg.qlf$PValue)[weird.des]), 
  col='blue', cex=0.6)

```


Examining the plots, edge R clearly not centred with the median housekeeping functions offset by `r median(meta.edg.qlf$logFC[hk.off==T])` and minimum FDR value  is `r min(p.adjust(meta.edg.qlf$PValue)[hk.off==T])`. Additionally, as shown in Figure 4there is little range in the p-values relative to those seen for DESeq2, and the values are more in line with the range seen with ALDEx2.

DESeq2 is better centred with median housekeeping functions offset by `r median(meta.DES.res@listData$log2FoldChange[hk.off==T])` but the minimum FDR value  is `r min(meta.DES.res@listData$padj[hk.off==T])`. In addition there are a large number of functions with 0 variance, these are very low count functions with very high sparsity in the dataset. These are not differential in the DESeq2 analysis. However, there are a number of functions in the DESeq2 analysis that are very differentially abundant, with a log2 fold change of < -20. Examination of the raw counts shows that these are uniformly 0 or 1 in the H dataset, and present at high counts in some, but not all of the BV dataset. That these stand out is odd, since inspection of the count table shows that there are many other functions that are missing from the H dataset, and have higher and more uniform counts in the BV dataset. Thus, the importance of the outlier functions seen in the DESeq2 volcano plot should be viewed with suspicion and may be an artefact of the normalization used. When overplotted on the edgeR volcano plot (blue), it is clear that these functions have non-signficant p-values.

```{r eval=T, echo=F, fig.cap="Shown here are two volcano plots that plot the mean log2 fold change plotted vs the log of the FDR for the yeast transcriptome dataset and for the metatranscriptome dataset. Data were generated in ALDEx2 with \\(\\gamma = [0, 0.5] \\) in black or in red."}

par(mfrow=c(1,2))
plot(yst.all$diff.btw, -log10(yst.all$we.eBH +1e-70), pch=19, cex=0.5, col=rgb(0,0,0,0.3), 
  ylab='-log10(FDR)', xlab="log2(diff)", main='yeast')
points(yst.s.all$diff.btw, -log10(yst.s.all$we.eBH + 1e-20), pch=19, cex=0.5, col=rgb(1,0,0,0.3))
legend(4,60, legend=c("gamma=0", "gamma=0.5"), pch=19, cex=0.5, col=c("black", "red"))

plot(xt.all$diff.btw, -log10(xt.all$we.eBH), pch=19, cex=0.5, col=rgb(0,0,0,0.3),
  ylab='-log10(FDR)', xlab="log2(diff)", main='meta')
points(xg.all$diff.btw, -log10(xg.all$we.eBH), pch=19, cex=0.5, col=rgb(1,0,0,0.3))

```

Adding a small amount of scale, \(\gamma=0.5\), has a major effect on the volcano plot for the yeast transcriptome dataset, but mimimal effect on the vaginal metatranscriptome dataset.  In both datasets the FDR values are raised, but no effect is observed on the difference between values. However, the concordance between the FDR values and the difference between become very tight for the yeast transcriptome dataset, but the concordance is not visibly unchanged for the other dataset. This is likely because both the difference between and the dispersion were both very small in the former and substantially larger in the latter.


# Checking the scale assumptions of the RLE and TMM normalizations

We can show that the normalizations built into the edgeR Bioconductor package (RLE, TMM, TMMwsp, upperquantile) are scale assumptions by using the normalization factor as an input to `aldex.makeScaleMatrix()` and then measure the mean location of the data as above. The ideal behavior of a normalization is that the mean location of the data should be close to 0 or unchanged. This behavior will ensure that Type 1 and Type 2 errors due to scale assumptions are minimized. 

The results, shown in the tables below compare these normalizations to the no scale assumption (iso) and the geometric mean normalization (GM) assuming that the normalizations are either on a log scale or a linear scale. That these affect scale can be observed by their effect on the mean location of the data. For the most part assuming no scale differences between groups centres the date less well than does the geometric mean. In most cases the other normalizations perform poorer than assuming no scale at all for the rRNA dataset and about as well as the no scale assumption in the metatranscriptome datasets and the single-cell transcriptome dataset. All normalizations perform poorly in the selex dataset. Overall, these results may not be surprising because the TMM and RLE (and related normalizations) were developed specifically to scale and normalize transcriptome datasets [@Robinson:2010a;@Anders:2010], but have become widely used in the analysis of other data modalities; e.g. [@McMurdie:2014a]. 

```{r getNorm, echo=F}
# output from code/scale_norms.R

load('analysis/normalizations.Rda')
knitr::kable(norm.data.out, 'simple', caption='Log scale models')
```

```{r getNormNL, echo=F}
# output from code/scale_norms.R

load('analysis/normalizations.nl.Rda')
knitr::kable(norm.nl.data.out, 'simple', caption='Linear scale models')
```

Thus far we have observed that adding scale uncertainty has a negligible effect on either the differences between groups or the relative abundance measure with a correlation of `r round(cor(yst.all$diff.btw, yst.1.all$diff.btw), 3)` and `r round(cor(yst.all$rab.all, yst.1.all$rab.all), 3)`. The effect is entirely restricted to the dispersion estimats as shown below.  Panel A shows the change in dispersion relative to the rAbundance of the features. Here we can see that the minimum dispersion is increased as gamma increases. The horizontal lines show the median value for the features with a rAbundance between -0.5 and 0.5. Panel B shows that this increase is non-linear, being more pronounced among those features that had minimal dispersion when gamma=0. Panel C shows that increasing gamma rotates the dispersion estimates at high relative abundances, such that housekeeping genes no longer have larger mean dispersions (rAB > 5) than do regulated genes (rAB >0, < 5). Overplotted are the lowess lines of fit through the densities of transcripts that are judged as significantly different using either scaled or unscaled data, and with or without thresholding. Statistical significance was determined with a FDR < 0.05.

Thus, adding scale uncertainty has two effects both on dispersion. The first is to increase the dispersion estimate for each part and this has its primary effect to reduce the  p-values and standardized effect sizes returned by ALDEx2. The second is to reduce the range of dispersions, and rotate them such that the parts with the lowest dispersions have the greatest increase. This increase in dispersion comes about because the underlying distributions are widened with the inclusion of scale uncertainty.

```{r disp, eval=T, echo=F, warning=F, message=F,comment=F, fig.cap="Adding scale uncertainty changes the dispersion distribution. Panel A shows a plot of the expected value for relative abundance vs the expected value for the pooled dispersion as output by aldex.effect. The dashed  horizontal lines show the median value for the features with a rAbundance between -0.5 and 0.5. Panel B plots the unscaled vs scaled dispersion, note the non-linear relationship. Panel C plots relative abundance vs the  difference between dispersion with gamma set to 0 and to 1 to highlight the rotation that is evident in Panel A. The colored lines indicate the lowess line of fit through the centre of mass of the plot. Line colors represent different populations of points. The grey line is the total population, the red line is the population of significant transcripts with no scale, the orange line is the population of significant transcripts with a difference threshold of about 2.5-fold change, the blue line is the population of significant transcripts with gamma =0.5, and the cyan line is the significant population with gamma = 1.  "}

cuts <- yst.all$rab.all > -0.5 & yst.all$rab.all < 5
par(mfrow=c(1,3))
mn.mid <- median(yst.all$diff.win[cuts])
mn.mid.s <- median(yst.s.all$diff.win[cuts])
mn.mid.1 <- median(yst.1.all$diff.win[cuts])

# we change dispersion a lot for the bulk of the features
plot(yst.all$rab.all, yst.all$diff.win, pch=19, cex=0.5, col=rgb(0,0,0,0.3),
  xlab='rAbundance', ylab='dispersion')
points(yst.s.all$rab.all, yst.s.all$diff.win, pch=19, cex=0.5, col=rgb(1,0,0,0.3))
points(yst.1.all$rab.all, yst.1.all$diff.win, pch=19, cex=0.5, col=rgb(0,0,1,0.3))
abline(h=mn.mid, lty=2)
abline(h=mn.mid.s, lty=2, col='red')
abline(h=mn.mid.1, lty=2, col='blue')
title(main='A', line=-1.2, adj=0.5)
legend(0,4, legend=c("0", "0.5", "1"), 
  pch=19, col=c("black", "red", "blue"))

plot(yst.all$diff.win, yst.1.all$diff.win, pch=19, cex=0.5, col=rgb(0,0,1,0.3), ylim=c(0,4.5), 
  xlab='g=0 dispersion', ylab='g=0.5 | g=1 dispersion')
points(yst.all$diff.win, yst.s.all$diff.win, pch=19, cex=0.5, col=rgb(1,0,0,0.3))
abline(0,1, lty=2)
title(main='B', line=-1.2, adj=0.5)
legend(0,4, legend=c( "0.5", "1"), 
  pch=19, col=c("red", "blue"))

sig.all <- yst.all$we.eBH < 0.05
sig.t <- yst.all$we.eBH < 0.05 & abs(yst.all$diff.btw) > 1.4
sig.s <- yst.s.all$we.eBH < 0.05
sig.1 <- yst.1.all$we.eBH < 0.05

diff.df <- data.frame(yst.all$rab.all, 
  yst.1.all$diff.win- yst.all$diff.win)
sig.df <- data.frame(yst.all$rab.all[sig.all],
   yst.1.all$diff.win[sig.all]- yst.all$diff.win[sig.all])
sig.t.df <- data.frame(yst.all$rab.al[sig.t], 
  yst.1.all$diff.win[sig.t] - yst.all$diff.win[sig.t])
sig.s.df <- data.frame(yst.all$rab.all[sig.s], 
  yst.1.all$diff.win[sig.s] - yst.all$diff.win[sig.s])
sig.1.df <- data.frame(yst.all$rab.all[sig.1], 
  yst.1.all$diff.win[sig.1]- yst.all$diff.win[sig.1])

plot(yst.all$rab.all, yst.1.all$diff.win- yst.all$diff.win, xlab='rAbundance', ylab="disp:1 - disp:0", pch=19, cex=0.5)
title(main='C', line=-1.2, adj=0.5)
lines(lowess(diff.df, f=0.1), col='grey60', lwd=3)
lines(lowess(sig.df, f=0.1), col='red', lwd=3)
lines(lowess(sig.t.df, f=0.5), col='orange', lwd=3)
lines(lowess(sig.s.df, f=0.5), col='royalblue', lwd=3)
lines(lowess(sig.1.df, f=0.7), col='cyan', lwd=3)

legend(-10,1.3, legend=c( "diff", "sig diff", "t + sig","0.5 sig", "1 sig"), 
  pch="-", col=c("grey", "red", "orange", "royalblue", "cyan"))

```