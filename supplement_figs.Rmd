---
title: "Supplement: Beyond compositionally in high throughput sequencing; estimating the importance of scale in data analysis with ALDEx2
"
shorttitle: "Scale ALDEx2 Supp"
author:
- name: Greg Gloor, Michelle Pistner Nixon, Justin Silverman
  affiliation: Dep't of Biochemistry, University of Western Ontario, Penn State
  email: ggloor@uwo.ca
bibliography: /Users/ggloor/Library/texmf/bibtex/bib/bibdesk_refs.bib
output:
  BiocStyle::pdf_document: default
package: ALDEx2 1.33.1
abstract: |
    Introduction to scale simulation and FDR correction with ALDEx2.
date: 21 August 2023
link-citations: true
---

```{r HvG_function, echo=F}
H <- function(x.vec){
  x.p <- x.vec/sum(x.vec)
  return(-1 * sum(x.p * log2(x.p)))
}
myfun.H <- function(x){ y <- c(x, 1-x); H(y)}
mf.H <- Vectorize(myfun.H)

G <- function(x.vec){
  x.p <- x.vec/sum(x.vec)
  return(mean(log2(x.p)))
}

myfun.G <- function(x){ y <- c(x, 1-x); G(y)}
mf.G <- Vectorize(myfun.G)


n.random = 20
n.parts = 500
test='N' # or U, B for Normal or Uniform or Beta

# note mean is only used when 
HvG <- function(n.random, n.parts, test='U', mean=30){

  mat.log <- matrix(data=NA, nrow=n.parts, ncol=n.random)
  mat.h <- matrix(data=NA, nrow=n.parts, ncol=n.random)

  # 2000 entries
  val <- seq(2,(n.parts*2), by=2)

  # this will generate data of the 
  # correspondence between H and G in a runif
  # for random number vectors of length
  # 2 to 10000. Does this 5 times
  # N approaches 0 difference as the mean of the N approaches infinity
  for(i in 1:n.parts){
    for(j in 1:n.random){	 
     if(test=='N'){ d <- rnorm(val[i], mean=mean) }
     else if (test == 'U'){ d <- runif(val[i]) * mean }
     else if (test == 'B'){ d <- rbeta(val[i], 0.1, 10) * mean }
     d[d < 0] <- 0
     d <- d/sum(d)
     # mean of log(d), exp = G
     mat.log[i,j] <- G(d)
     # entropy
     mat.h[i,j] <- H(d)
    }
}

jnk <- list(mat.log, mat.h)
names(jnk) <- c("mat.log","mat.h")
return(jnk)
}

HvG.plot <- function(HvG.out){

#plot(rep(val,n.random),mat.log- mat.h, pch=19, cex=0.5, col=rgb(0,0,0,0.2),
#  log='x', xlab='length vector', ylab='(-1* G) - H')
plot(HvG.out$mat.log, HvG.out$mat.h, pch=19, cex=0.5, col=rgb(0,0,0,0.2),
   xlab='G', ylab='H')

#abline(lm((HvG.out$mat.log[,2]-HvG.out$mat.h[,2]) ~ val), col='red')
abline(lm((HvG.out$mat.h[,2]~HvG.out$mat.log[,2])), col='red')

#lm((mat.log[,2]-mat.h[,2]) ~ val)
lm((HvG.out$mat.h[,2]~HvG.out$mat.log[,2]))
}
 
```

# GM is related to Information and Shannon's entropy in HTS datasets

## Shannon's entropy has a volume or size

Information  is a fundamental property of all measured systems. For discrete probability vectors, Shannon defined their information properties and launched the field of information theory for communications. Famously, for any probability vector, Shannon set the total entropy as the inverse of the sum of the probability weighted logarithm of the probabilities. Less well known is that an unweighted average entropy was also defined, and that this turns out to be the inverse of the logarithm of the geometric mean of the probability vector. Thus, information theory and compositional data analysis intersect through the geometric mean of a probability vector. Here I show that the information theoretic interpretation can help us understand the scale of a system as defined by Nixon and Silverman, and that this interpretation can help in the interpretation of highly asymmetric datasets. I will show the Asymptotic Equipartition Property of information can be defined as a volume for discrete distributions, and how that volume relates to the scale of a system. Finally, I will show how Shannon's entropy can substitute for the geometric mean in common CoDa operations and how that alters the interpretation of the results.

We can think about  scale  from an information theoretic point of view as a measure of how much information, or total uncertainty, is encoded in a particular sample [@Shannon.48;@Jaynes:2003]. In the geometric interpretation of information theory used in quantum information theory, formally described as  the  Asymptotic Equipartition Property of information [@Cover:1991;@wilde_2017], entropy can be interpreted as the volume occupied by a probability distribution. See chapters 4 of the PhD thesis of Lecamwasam [-@ruvi.blog] for a nice explanation of this. 

For notational simplicity assume we have a single discrete random variable to represent a probability distribution with d elements; i.e. \( X =  \mathbf{p}_{i=(1 \dots d )} \). In information theory, the elemental amount of information or surprisal for \(p_i\) is the inverse of the logarithm of the elemental probability, \( -log_2(p_i)\)[@rezaInfoIntro]. This measure is often called self-information.

The total entropy of the system  \( H(X) \) is the weighted sum of the elemental information;

\[
H(X) = - \sum_{i=1}^{d} p_i \log_2 p_i
\]

\( H(X) \) corresponds to the amount of information needed in order to reproduce  \( X \). We can also calculate the expected amount of information for each observation in the random variable, and this is the mean of the elemental probability. This measure is also called the sample average of the information [@wilde_2017];

\[
 h(X) = - \frac{1}{d} \sum_{i=1}^{d} \log_2 p_i
\]

The linkage between compositional analysis, scale inference and information theory comes when we realize that the logarithm of the geometric mean calculated in base(2) is: 

\[
 l2G(X) = \log_2 G(X) = \frac{1}{d} \sum_{i=1}^{d} \log_2 p_i
\];

We see that \( h(X) = - l2G(x) \). Furthermore, \(l2G\) is used as the basis for the centred log-ratio transform and is the starting point for scale-based inference:

 \[CLR(X) = log2(p_i) - l2G(X) = log2(p_i) + h(X)  \]
 
Thus we see that the geometric mean used in the centred log ratio (CLR), often used for Compositional Data Analysis (CoDa) [@aitchison1982] is directly related to entropy or \(H\). Indeed, we can rearrange the CLR formula to show that it can be interpreted as computing the difference between the elemental information and the mean information content: 
  
  \[CLR(X) = -log2(p_i) - (-l2G(X)) = -(-log2(p_i) - h(X))\]

As defined in [@nixon2023scale], the scale is the inverse of \( l2G \), which is \(h(X)\). Thus, one way we can understand scale is that it is measuring the total complexity of the system, and this is expected to increase with absolute size in most cases. Moreover, \(H(X)\) and \(G(X)\) share similar shapes in the continuum between 0-1 for a bivariate distribution as shown below:

```{r} 
par(mfrow=c(1,1))
curve(mf.G, from=1e-2, to =.99,  col='red', lty=2, ylim=c(-3,1),
   xlab="x in [0,1]", ylab="value")
curve(mf.H, from=1e-2, to =.99,  col='black', lty=2, add=T )
legend(.4,-1.5, legend=c('H','G'), col=c('black','red'), pch="-")
```

The difference being that entropy is constructed to have a value of 0 at the margins because Shannon defined \( 0log(0) = 0\) while the geometric mean approaches negative infinity. 

Now let's think about the idea of entropy as a volume which allows us to identify what amount of the available entropy space a given observation fills. The following is taken and modified from [@ruvi.blog], to which you should refer for a more fulsome discussion, and it is covered in Chapter 2 of Wilde[-@wilde_2017] and Chapter 3 of Cover and Thomas [-@Cover:1991].  If we start with a four part system \(X1 = [A,C,G,T]\) where the frequencies are equally and identically distributed, then \(p_A = p_C = p_G = p_T = \frac{1}{4} \). \( H(X1) = -1 * 4 * (\frac{1}{4} * log2(\frac{1}{4}) ) = 2\). This is the maximum entropy possible. We can obtain the "volume" of \(X1\) by exponentiating \(H1\) using the same base as was used to calculate the entropy; \( V1 = 2^{H1} = 4 \). This is the same as the number of letters in the system; so the volume needed to explain the system is 4 units (in this case bits). But what happens in another system, \(X2\) where A occurs with a much higher probability, say 0.7, and the other three are distributed equiprobably amongst the remainder with a probability of 0.1; i.e. \(p_C = p_G = p_T = 0.1 \). In this case \( H2 = -1 * ((\frac{7}{10} * log2(\frac{7}{10}) ) + (3 * (\frac{1}{10} * log2(\frac{1}{10}) ))) = 1.358 \). Here the volume of \(X2 = 2^{H2} = 2.56\); meaning that less than the maximum volume is taken up by the information. Here \(X2\) consumes about 64% of the volume of system \(X1\). Thus, the volume is a measure of the total complexity or the scales of the two systems. In this way we can understand that scale is related to the information volume of a system. 

Empirically, we can see that \(G(\mathbf{Y}^{\parallel}_{n})\) is strongly correlated with Shannon's Entropy \(H(\mathbf{Y}^{\parallel}_{n})\) as expected from the discussion above, and that this difference converges to a constant as the number of entries in the probability vector increases regardless of the distribution, although different distributions converge at different rates. For example, if we plot the relationship between H and G as a function of the length of the probability vector we can see a direct inverse relationship. 

```{r, echo=F, fig.cap="Association between entropy (H) and geometric mean (G) as a function of vector length. Twenty random vectors were constructed for each length between  2 and 1000 in increments of 2 for each of the random distributions in the legend; N = Normal, U = Uniform, B = Beta. The bottom right of each plot represents vectors of length 2, and the top left represents the vector of lenght 500. The maximum value of H increases as the vector length increases, and the maximum value of G decreases in lock-step. Each random distribution has an obviously distinct relationship between the two measures. For the purposes of high throughput sequencing the Beta distribution is most similar to that seen in the majority of instances."}
set.seed=24
par(mfrow=c(1,3))
U <- HvG(20, 500, test='U', mean=1)
U.plot <- HvG.plot(U)
text(-4,0.5,labels='vec len = 2')
text(-7.4,9.8,labels='vec len = 1000')
title(main='random uniform')
B <- HvG(20, 500, test='B', mean=1)
B.plot <- HvG.plot(B)
title(main='Extreme skew')
N <- HvG(20, 500, test='N', mean=30)
N.plot <- HvG.plot(N)
title(main='N, mean=300')
# intercept
U.plot$coefficients[1]
B.plot$coefficients[1]
N.plot$coefficients[1]
par(mfrow=c(1,1))

```

When we plot the relationship for any individual probability vector, we  see that there is an direct relationship between the entropy and the log of the geometric mean, but that this relationship strongly depends on the underlying distribution of the probability distribution \( X \). 



```{r, echo=F, fig.cap="Plot of the association between H and G at a vector length of 30. The relationship between H and G is inverse, and the strength of that association depends on the distribution. The N distribution shows a very strong assocation, while the Beta distribution is less well defined. Associations are shown for a vector length of 30."}
par(mfrow=c(1,3))
plot(N$mat.log[15,], N$mat.h[15,], main="Normal", xlab="G", ylab="H")
abline(lm(N$mat.h[15,]~N$mat.log[15,]), lty=2, col='grey', lwd=2)
plot(U$mat.log[15,], U$mat.h[15,], main="Uniform", xlab="G", ylab="H")
abline(lm(U$mat.h[15,]~U$mat.log[15,]), lty=2, col='grey', lwd=2)
plot(B$mat.log[15,], B$mat.h[15,], main="Beta", xlab="G", ylab="H")
abline(lm(B$mat.h[15,]~B$mat.log[15,]), lty=2, col='grey', lwd=2)
par(mfrow=c(1,1))
```


In real data, shown in Supplemental Figure 3 and Table 1 the correspondence is not as predictable, likely because the real data is a more complex distribution than any of the idealized distributions.  Thus, these two measures have different behaviours with different distributions of \( p_i \). In the case of a uniform distribution both \( H(\mathbf{Y}^{\parallel}_{n}) \) and \( G(\mathbf{Y}^{\parallel}_{n}) \) are maximal when \( p(x) \) is equally and identically distributed. Thus, we expect that they are positively correlated here. In a Normal or a skewed distribution, we also observe a positive correlation because both are affected in the same direction by outlier values. In very sparse  datasets, the two measures could become uncoupled because \( H(\mathbf{Y}^{\parallel}_{n}) \) could ascribe some uncertainty to the large number of low probability events, while \( G(\mathbf{Y}^{\parallel}_{n}) \) would tend to be very small. Here these two measures could be either uncorrelated or exhibit negative correlation. We can see this distributional behaviour in different datasets.

Intuitively,  systems with different scales will contain different amounts of information and so we would expect \(W^{\perp}_n \sim H_n\). As the scale of a system as defined by Nixon et al. [-@nixon2023scale] is inversely related to \(G\), this means that scale is directly proportional to the information content and entropy of the data. 

Below I show that we can replace \( G \) with \(H \) in the calculations performed by ALDEx2 without loss of utility.

Recall the underlying system is described by a \(D \times N\) matrix of counts \(\mathbf{W}\) decomposed into the proportions for the \(n^{th}\) sample \(\mathbf{W}^{\parallel}_n\) (or the equivalent probability distribution \( \mathbf{p}(w_n) \) ), and its scale \(\mathbf{W}^{\perp}_n\), such that \(\mathbf{W}=\mathbf{W}^{\parallel}\mathbf{W}^{\perp} \). Sequencing  returns counts which are related to the underlying proportion; i.e., \(\mathbf{Y}^{\parallel}_n \sim \mathbf{W}^{\parallel}_n\)
 



```{r info, echo=F,warning=F, message=F,comment=F, fig.cap="Plot of Shannon's entropy (H) vs geometric mean (G) for each sample in different datasets. The groups that each sample belong to are highlighted as filled or open circles. Each group in each dataset has different entropy with the groups in the selex and metatranscriptome datasets being highly distinct."}
# Gm ~ Im
# plotting geometric mean vs Shannon's entropy
# shows differential information by group
devtools::load_all('~/Documents/0_git/ALDEx_bioc')
data(selex)

load(url('https://raw.githubusercontent.com/ggloor/datasets/main/ko.both.Rda'))
url <- "https://raw.githubusercontent.com/ggloor/datasets/main/transcriptome.tsv"
yst <- read.table(url, header=T, row.names=1)
url <- "https://raw.githubusercontent.com/ggloor/datasets/main/meta16S.tsv"
rRNA <- read.table(url, header=T, row.names=1, sep='\t')
url <- "https://raw.githubusercontent.com/ggloor/datasets/main/singleCell.tsv"
ss <- read.table(url, header=T, row.names=1, sep='\t')
ss <- ss[,c(1:100,1502:1601)]
# remove the one gene with 0 reads
yst <- yst[rownames(yst) != "YOR072W-B",]
# Gierlinski:2015aa
yst[,c('SNF2.6', 'SNF2.13','SNF2.25','SNF2.35')] <- NULL 
yst[,c('WT.21','WT.22','WT.25','WT.28','WT.34','WT.36')] <- NULL  

HS <- apply(selex+0.5, 2, function(x)  H(x) )
GS <- apply(selex+0.5, 2, function(x) G(x) )

HY <- apply(yst+0.5, 2, function(x) H(x) )
GY <- apply(yst+0.5, 2, function(x) G(x) )

HM <- apply(ko.both+0.5, 2, function(x) H(x) )
GM <- apply(ko.both+0.5, 2, function(x) G(x) )

H16 <- apply(rRNA+0.5, 2, function(x)  H(x) )
G16 <- apply(rRNA+0.5, 2, function(x) G(x) )

Hss <- apply(ss+0.1, 2, function(x) H(x) )
Gss <- apply(ss+0.1, 2, function(x) G(x) )

par(mfrow=c(2,3))
plot(HS, GS, ylab="Geometric mean", xlab="Shannon's entropy",
  pch=c(rep(19,7),rep(1,7)), col=rgb(0,0,0,0.5))
title(main='selex')

plot(HY,GY, 
  ylab="Geometric mean", xlab="Shannon's entropy", pch=c(rep(19,44),rep(1,42)),
  col=rgb(1,0.6,0,0.5))
title(main='transcriptome')

plot(HM, GM, col=rgb(1,0,0,0.5), pch=c(rep(1,8), rep(19,28), rep(1,8)), 
  ylab="Geometric mean", xlab="Shannon's entropy")
title(main='meta-transcriptome')

plot(H16,G16, col=rgb(0,1,1,0.5),
  pch=c(rep(1,198), rep(19,161)), 
  ylab="Geometric mean", xlab="Shannon's entropy")
title(main='16S rRNA')

plot(Hss,Gss, col=rgb(0.5,1,0,0.5),
 pch=c(rep(1,100), rep(19,100)), 
  ylab="Geometric mean", xlab="Shannon's entropy")
title(main="single cell")

# empty plot for legend
plot(selex[,1], selex[,2], xlab=NA, ylab=NA, axes=F,xlim=c(-10,-1), ylim=c(-10,-1))
legend(-10,-1, legend=c('selex', 'transcriptome','metatranscriptome', '16S', 'SS', 'low entropy group', 'high entropy group'),
  col=c('darkgrey', 'orange', 'red', 'cyan', 'greenyellow','grey','grey'), pch=c(19,19,19,19,19,1,19))
  
```

The table below summarizes the mean values for, and the correlation between, \( G \) and \( H \)  (cor)  and the sparsity defined as the proportion of features with less than 1 count per sample (spar) for each association in each group of samples:

| Dataset | group | \(\overline{G}\) | \( \overline{H} \) | cor | spar |
|---------|-------|---|---|-----|-------|
| Selex   | control | -11.2 | 10.2 | 0.99 | 0 |
| $~~~$"  | selected | -17.8 | 2.8 | -0.88 | 0.802 |
| yeast   | snf2 ko  | -14.0 | 10.7 | 0.99 | 0.004 |
| $~~~$"  | WT    | -14.2 | 10.4 | 0.99 | 0.007 |
| Meta    | H     | -18.8 | 8.6 | 0.78 | 0.451 |
| $~~~$"  | BV   | -18.2 | 8.9 | 0.79 | 0.238 |
| 16S     | Pup   | -14.7 | 5.4 | 0.68 | 0.079 |
| $~~~$"  | Cent | -15.2 | 5.4 | 0.53 | 0.251 |
| SS      | A     | -13.0 | 8.2 | 0.83 | 0.978 |
| $~~~$"  | B    | -12.9 | 8.3 | 0.80 | 0.977 |

We can see that for most datasets the difference between the \(\overline{G}\) in each group is relatively small. Most significantly, the selex dataset has a very large difference of about 100-fold, and both the 16S and the metatranscriptome dataset have about a 1.5 fold difference. These three datasets are candidates for a full scale model correction.

```{r full.scale, echo=F, fig.cap="Plot of the offset of the mean differnece between groups as a function of scale ratio. For this, the default scale of 1:1 was altered in increments of 0.1 keeping the gamma parameter (dispersion) at 0.5. The filled circle shows the outcome when the calculation is done using the geometric mean and the same gamma parameter."}

load('analysis/data.out.Rda' )

plot(data.out[,6],data.out[,1], pch=c(rep(1,19),19), ylim=c(-11, 4),
  xlab='full scale offset', ylab='mean difference between groups')
points(data.out[,6],data.out[,2], pch=c(rep(1,19),19), col='red')
points(data.out[,6],data.out[,3], pch=c(rep(1,19),19), col='orange')
points(data.out[,6],data.out[,4], pch=c(rep(1,19),19), col='cyan')
points(data.out[,6],data.out[,5], pch=c(rep(1,19),19), col='blue')
abline(h=0, col='grey', lty=2)
abline(v=0, col='grey', lty=2)
legend(0.7, 0, legend=c('16S','selex','tscome','metatscome','ss'),
  pch=19, col=c('black','red','orange','cyan','blue'))

```

Nixon et al. [-@nixon2023scale] showed that many operations on HTS datasets relied on both the proportion and scale components of the data. Moreover, all normalizations impose a scale model on the data, but the appropriateness of these models has never been explicitly acknowledged or tested. Thus, the full scale model option allows the investigator to set both the offset between the geometric means of the features and their dispersion and observe how this affects the analysis outcome. 

In the offset plot above we can see the cause and the effect of the full model with a fixed gamma of 0.5 and a base scale of 1 in each group. We see that the mean location of well centred datasets (yeast transcriptome, single-cell transcriptome) are close to 0, but could be centred better with small changes in scale ranging from 0 (single cell) to 1:1.1 for the yeast transcriptome dataset. In contrast, centring the 16S dataset requires about a 1:1.3 fold change. The metatranscriptome dataset would require about a 0.5:1 change in relative scale between groups, but as shown in the main text, centring the housekeeping genes is more apt. 

The in vitro selection dataset is clearly an outlier in both the difference between the average group geometric mean, and in the offset plot.  However, this dataset can  be used to illustrate the power of the full scale model and the relationship between \( G_n \) and scale. In Figure 3 we can see that the default output of ALDEx2 has a centered output. This occurs largely by chance, as the high sparsity of the selected (S) group is balanced almost exactly by the arbitrarily chosen sequencing depth so the non-selected group (NS) appears to have a similar location as the S group. The difference in entropy between the two groups and the differences in geometric mean are very large, with the difference in \(\log_2{\overline{G}(S)}\) and \(log_2{\overline{G}(NS)}\) being about \(2^{6.6}\). Setting the scale of both the S and NS groups to 1 we find that the difference in location is approximately \(2^{7.7}\) in close agreement with the difference the geometric means. For this dataset to be centered we need to have a scale ratio \( \approx \) 1:50 or more. Note that the scale ratio is inverse to the ratio of geometric means as described above.  In fact, in this dataset the relative abundances of the majority of features are nearly invariant, but this is masked by the large absolute changes in a small number of features [@mcmurrough:2014], thus changing the scale of the data. Neither DESeq nor edgeR are able to provide a reasonable analysis of this dataset because the normalizations used assume equivalent scales [@gloorAJS:2016]. 

Figure 3 shows an effect plot of various scale models with this dataset. The full scale model, where the strong assumption that the mean \(G\) is assumed to be 1:1 between the two groups, dramatically skews the output and the large number of relatively invariant features are now identified as significantly different. While not wrong as long as the assumption that the scales are identical is stated, this is not a useful analysis outcome. Modifying the mean scale difference between the NS:S groups to be \( \approx \) 1:50 different moves the centre of the large number of relatively invariant features to the centreline of no difference, and recapitulates the default result obtained using \(G_n\) as the scale estimate where the ratio is \( \sim 100:1 \). Note that we get exactly the same answer (within random sampling error) with a scale of .02 for group NS and a scale of 1 for group S, or using a scale of 1 for group NS and a scale of 50 for group S. This shows that it is the relative difference between scales that is important in this dataset, not the absolute values. From this result we can conclude that, on average, the difference in underlying scale in the system is about \(\approx 50\)-fold, and this is congruent with the circa 100-fold difference in  \(\overline{G}\) between groups; the discrepancy being explained because the default scale model is applied uniformly to all samples, whereas the different values of the within-group geometric means ranging over a \{ > 2.5 \) fold range.  Thus, an advantage of a full scale model is that we have gained both information and understanding about the drivers of asymmetry underlying system.


```{r selex, eval=T, echo=F, warning=F, message=F,comment=F, fig.cap="Effect plots of the selex dataset with various gamma and scale parameters. All scales are calculated with a logNormal distribution to ensure symmetry for the user. "}

conds <- c(rep('NS',7), rep('S',7))

x.all <- aldex(selex, conditions=conds, CI=T, gamma=0.5)

sel.mu <- aldex.makeScaleMatrix(gamma=0.5, mu=c(10,10), conds)
x.1.all <- aldex(selex, conditions=conds, CI=T, gamma=sel.mu)

# setting ratio as 1:50 explicitly
#sel.mu <- aldex.makeScaleMatrix(gamma=0.5, mu=c(1,50), conds, log=FALSE)
scale <- c(rep(1,7), rep(50,7)) 
sel.mu <- aldex.makeScaleMatrix(gamma=0.5, mu=scale, conds, log=FALSE)
x.5.all <- aldex(selex, conditions=conds, CI=T, gamma=sel.mu)

# setting ratio in log space
# 2^-5.64 : 2^0  = 0.02 : 1
# this also works, but reversed because of factor()  
# sel.mu <- aldex.makeScaleMatrix(gamma=0.5, mu=c(0,-5.64), conds, log=TRUE)
scale <- c(rep(-5.6,7), rep(0,7))
sel.mu <- aldex.makeScaleMatrix(gamma=0.5, mu=scale, conds, log=TRUE)

x.2.all <- aldex(selex,conditions=conds, CI=T, gamma=sel.mu)

par(mfrow=c(1,4))
aldex.plot(x.all, main=('gamma=0.5\nmu=G'))
aldex.plot(x.1.all, main=('gamma=0.5\nmu=1,1'))
aldex.plot(x.5.all, main=('gamma=0.5\nmu=1,50'))
aldex.plot(x.2.all, main=('gamma=0.5\nmu=0.02, 1'))
```

# How scaling affects dispersion

Thus far we have observed that adding scale uncertainty increases the minimum dispersion value with little effect on the difference between values as in panel A. Panel B shows the change in dispersion relative to the rAbundance of the features. Here we can see that the minimum dispersion is increased as gamma increases. The horizontal lines show the median value for the features with a rAbundance between -0.5 and 0.5. Panel C shows that this increase is non-linear, being more pronounced among those features that had minimal dispersion when gamma=0. Panel D shows that increasing gamma actually shrinks the dispersion estimates towards the mean dispersion, while increasing the total dispersion as shown in panel C.

Thus, adding scale uncertainty has two effects both on dispersion. The first is to increase the dispersion estimate for each part and this has its primary effect to reduce the  p-values and standardized effect sizes returned by ALDEx2. The second is to reduce the range of dispersions such that the parts with the lowest dispersions have the greatest increase. This increase in dispersion comes about because the underlying distributions are widened because of the additional uncertainty added by the inclusion of scale uncertainty.

```{r disp, eval=T, echo=F, warning=F, message=F,comment=F, fig.cap="jnk"}
yst.conds <- c(rep('S',44),rep('W',42))
x <- aldex.clr(yst, yst.conds, gamma=NULL)
xs <- aldex.clr(yst, yst.conds, gamma=0.5)
x1 <- aldex.clr(yst, yst.conds, gamma=1)

x.e <- aldex.effect(x)
xs.e <- aldex.effect(xs)
x1.e <- aldex.effect(x1)

par(mfrow=c(2,2))
# we imperceptibly change difference
plot(x.e$diff.btw, x1.e$diff.btw, xlab='diff g=0', ylab='diff g=1')
abline(0,1, col='grey', lwd=2, lty=2)
title(main='A', line=-1.2, adj=0.5)
#=-1
cuts <- x.e$rab.all > -0.5 & x.e$rab.all < 0.5
# the residual of the 
mn.mid <- median(x.e$diff.win[cuts])
mn.mid.s <- median(xs.e$diff.win[cuts])
mn.mid.1 <- median(x1.e$diff.win[cuts])

# we change dispersion a lot for the bulk of the features
plot(x.e$rab.all, x.e$diff.win, pch=19, cex=0.5, col=rgb(0,0,0,0.3),
  xlab='rAbundance', ylab='dispersion')
points(xs.e$rab.all, xs.e$diff.win, pch=19, cex=0.5, col=rgb(1,0,0,0.3))
points(x1.e$rab.all, x1.e$diff.win, pch=19, cex=0.5, col=rgb(0,0,1,0.3))
abline(h=mn.mid, lty=2)
abline(h=mn.mid.s, lty=2, col='red')
abline(h=mn.mid.1, lty=2, col='blue')
title(main='B', line=-1.2, adj=0.5)

plot(x.e$diff.win, x1.e$diff.win, pch=19, cex=0.5, col=rgb(0,0,0,0.3), ylim=c(0,4.5), 
  xlab='g=0 dispersion', ylab='g=0.5 | g=1 dispersion')
points(x.e$diff.win, xs.e$diff.win, pch=19, cex=0.5, col=rgb(1,.4,.1,0.3))
abline(0,1, lty=2)
title(main='C', line=-1.2, adj=0.5)

strip.data <- list(x.e$diff.win[cuts] - mn.mid, xs.e$diff.win[cuts] - mn.mid.s, x1.e$diff.win[cuts] - mn.mid.1)

stripchart( strip.data, col=c(rgb(0,0,0,0.3), rgb(1,0,0,0.3), rgb(0,0,1,0.3)), vertical=T, method='jitter', pch=19, group.names=c('g=0', 'g=0.5', 'g=1'), ylab='residual dispersion')
title(main='D', line=-1.2, adj=0.5)

```


# Issues with DESeq2 and edgeR

DESeq2 and edgeR are two of the most commonly used tools for differential abundance analysis of bule RNA sequencing datasets. They both operate by finding a scaling factor that makes all the samples commesurate. DESeq2 does this by finding a midpoint feature that can be used as a reference in each sample; this can be different for different samples. The edgeR reference finds the midpoint of the 'typical' sample instead. In both cases the data are then scaled by dividing by a small factor that makes the read counts commesurate. Differential abundance analysis is then performed on the scaled values after taking their logarithm to base 2. In some ways this is similar to the log-ratio approach used by ALDEx2, but is more prone to dataset and sample effects than is the log-ratio method [@GloorAJS2023] 

```{r DESedg, eval=T, echo=F, fig.cap="Shown here are the mean log2 fold change as a density plot, and a Volcano plot showing the location and adjusted p-value for each feature in the metatranscriptomic dataset. The DESeq2 approach does a good job of centring this data, while edgeR is less suitable. The volcano plots show dramatically different outcomes. The DESeq2 algorithm assigns very large fold changes to features that have only moderate change, and further identifies a very large proportion of features as significantly different. In contrast, edgeR exhibits a much smaller number of differentially abundant features. In both volcano plots, the housekeeping genes in the main Figure 3 are shown in orange. We can see that these are asymmetrically distributed in both plots. Additionally the location of the features taht DESeq2 identified as having a very large difference are shown in the edgeR volcano plot as blue circles."}

# find housekeeping from ALDEx2

load('analysis/xt.m.Rda')
load('analysis/xt.Rda')
hk.off <- xt.all$diff.win < 5 & xt.all$diff.btw > 0 & xt.all$diff.btw < 3

load('analysis/meta.DES.res.Rda')
load('analysis/meta.edge.qlf.Rda')

# these are all very rare K0s
zero.var <-  which(meta.DES.res@listData$lfcSE*sqrt(44) == 0)

weird.des <- which(meta.DES.res@listData$log2FoldChange < -20 )

# housekeeping genes centred with DESeq2 - nice

sig.des <- which(meta.DES.res@listData$padj < 0.01)
sig.edge <- which(p.adjust(meta.edg.qlf$PValue) < 0.01)
sig.ald <- which(xt.m.all$we.eBH < 0.01)

par(mfrow=c(2,3))

plot(density(xt.all$diff.btw), xlim=c(-5,5), ylim=c(0,1), main='ALDEx2 FC')
points(density(xt.all$diff.btw[hk.off]), type='l', col='orange')
abline(v=0, col='orange', lty=2)

plot(density(meta.DES.res@listData$log2FoldChange), xlim=c(-5,5), ylim=c(0,0.8), main='DESeq2 FC')
points(density(meta.DES.res@listData$log2FoldChange[hk.off]), type='l', col='orange')
abline(v=0, col='orange', lty=2)

plot(density(meta.edg.qlf$logFC), xlim=c(-5,5), ylim=c(0,0.5), main='edgeR FC')
points(density(meta.edg.qlf$logFC[hk.off]), type='l', col='orange')
abline(v=0, col='red', lty=2)


plot(xt.m.all$diff.btw, -1*log10(xt.m.all$we.eBH), 
  col=rgb(0,0,0,0.1), xlab='log2 Difference', ylab='-1 log10(p.adjust)', main='ALDEx2 volcano')
points(xt.m.all$diff.btw[sig.ald],-1*log10(xt.m.all$we.eBH)[sig.ald] + 1e-8, col='red')
points(xt.m.all$diff.btw[hk.off],-1*log10(xt.m.all$we.eBH)[hk.off] + 1e-8, col='orange', cex=0.6)


plot(meta.DES.res@listData$log2FoldChange, -1*log10(meta.DES.res@listData$padj), 
  col=rgb(0,0,0,0.1), xlab='log2 Difference', ylab='-1 log10(p.adjust)', main='DESeq2 volcano')
points(meta.DES.res@listData$log2FoldChange[sig.des],-1*log10(meta.DES.res@listData$padj[sig.des] + 1e-70), col='red')
points(meta.DES.res@listData$log2FoldChange[hk.off],-1*log10(meta.DES.res@listData$padj[hk.off] + 1e-70), col='orange', cex=0.6)

plot(meta.edg.qlf$logFC, -1*log10(p.adjust(meta.edg.qlf$PValue)), 
  col=rgb(0,0,0,0.1), xlab='LFC', ylab='-1log10(p)', main='edgeR volcano')
points(meta.edg.qlf$logFC[sig.edge], -1*log10(p.adjust(meta.edg.qlf$PValue)[sig.edge]), 
  col='red')
points(meta.edg.qlf$logFC[hk.off], -1*log10(p.adjust(meta.edg.qlf$PValue)[hk.off]), 
  col='orange', cex=0.6)
  points(meta.edg.qlf$logFC[weird.des], -1*log10(p.adjust(meta.edg.qlf$PValue)[weird.des]), 
  col='blue', cex=0.6)

```


Examining the plots, edge R clearly not centred with the median housekeeping functions offset by `r median(meta.edg.qlf$logFC[hk.off==T])` and minimum FDR value  is `r min(p.adjust(meta.edg.qlf$PValue)[hk.off==T])`. Additionally, as shown in Figure 4there is little range in the p-values relative to those seen for DESeq2, and the values are more in line with the range seen with ALDEx2.

DESeq2 is better centred with median housekeeping functions offset by `r median(meta.DES.res@listData$log2FoldChange[hk.off==T])` but the minimum FDR value  is `r min(meta.DES.res@listData$padj[hk.off==T])`. In addition there are a large number of functions with 0 variance, these are very low count functions with very high sparsity in the dataset. These are not differential in the DESeq2 analysis. However, there are a number of functions in the DESeq2 analysis that are very differentially abundant, with a log2 fold change of < -20. Examination of the raw counts shows that these are uniformly 0 or 1 in the H dataset, and present at high counts in some, but not all of the BV dataset. That these stand out is odd, since inspection of the count table shows that there are many other functions that are missing from the H dataset, and have higher and more uniform counts in the BV dataset. Thus, the importance of the outlier functions seen in the DESeq2 volcano plot should be viewed with suspicion and may be an artefact of the normalization used. When overplotted on the edgeR volcano plot (blue), it is clear that these functions have non-signficant p-values.

# Checking the scale assumptions of the RLE and TMM normalizations

We can show that the normalizations built into the edgeR Bioconductor package (RLE, TMM, TMMwsp, upperquantile) are scale assumptions by using the normalization factor as an input to `aldex.makeScaleMatrix()` and then measure the mean location of the data as above. The ideal behavior of a normalization is that the mean location of the data should be close to 0 or unchanged. This behavior will ensure that Type 1 and Type 2 errors due to scale assumptions are minimized. 

The results, shown in the tables below compare these normalizations to the no scale assumption (iso) and the geometric mean normalization (GM) assuming that the normalizations are either on a log scale or a linear scale. That these affect scale can be observed by their effect on the mean location of the data. For the most part assuming no scale differences between groups centres the date less well than does the geometric mean. In most cases the other normalizations perform poorer than assuming no scale at all for the rRNA dataset and about as well as the no scale assumption in the metatranscriptome datasets and the single-cell transcriptome dataset. All normalizations perform poorly in the selex dataset. Overall, these results may not be surprising because the TMM and RLE (and related normalizations) were developed specifically to scale and normalize transcriptome datasets [@Robinson:2010a;@Anders:2010], but have become widely used in the analysis of other data modalities; e.g. [@McMurdie:2014a]. 

```{r getNorm, echo=F}
# output from code/scale_norms.R

load('analysis/normalizations.Rda')
knitr::kable(norm.data.out, 'simple', caption='Log scale models')
```

```{r getNormNL, echo=F}
# output from code/scale_norms.R

load('analysis/normalizations.nl.Rda')
knitr::kable(norm.nl.data.out, 'simple', caption='Linear scale models')
```

# References
